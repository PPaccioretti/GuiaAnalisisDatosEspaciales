[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Guía para el análisis de datos espaciales. Aplicaciones en agricultura",
    "section": "",
    "text": "Prólogo\nEn las últimas décadas se ha impulsado el desarrollo y la utilización de nuevas tecnologías que permiten capturar datos espaciales, i.e. datos de una variable regionalizada o asociados a una localización en el espacio.\nLa infraestructura de datos espaciales es cada vez mayor en tamaño y calidad, especialmente la asociada a la generación de datos que provienen de sensores ya sea remotos o proximales. Los volúmenes de datos espaciales no sólo son vastos y variados, sino que también, en la mayoría de los escenarios, son accesibles. Estos datos generan nuevas oportunidades para la investigación en agricultura.\nLa variabilidad en los procesos aleatorios que generan datos espaciales se modela con diversas herramientas de la Estadística Espacial y se representa gráficamente en mapas de variabilidad espacial donde puede observarse cómo cambian los valores de una o más variables aleatorias según su posición en el espacio.\nAún cuando se estudian dominios espaciales continuos con alta densidad de datos, usualmente no existen observaciones de la variable de interés para todos las localizaciones o sitios del espacio analizado; así se hace necesario obtener predicciones espaciales, i.e. predecir el valor de la variable en sitios sin datos. Con grillas de predicción densa, es posible obtener mapas de contorno casi continuos espacialmente.\nCon varias variables para cada sitio, una de ellas interpretada como resultante de un proceso y otras como explicativas o potenciales predictores, es posible obtener predicciones espaciales a partir de modelos que consideran la correlación espacial de los datos. Los modelos pueden estimarse tanto en un marco teórico frecuentista (Cressie y Wikle 2015; Schabenberger y Gotway 2005) como desde el marco teórico bayesiano (Correa Morales, Causil, y Javier 2018). También, desde la Ciencia de Datos con base computacional, se encuentran disponibles algoritmos de aprendizaje automático que incorporan la espacialidad en el análisis de datos (Li et al. 2011).\nEn esta guía se ilustra el manejo y procesamiento de datos espaciales con distintos métodos estadísticos y su aplicación en agricultura. El texto está organizado en tres partes; la primera contiene bases conceptuales para el análisis de datos georreferenciados provenientes de procesos espaciales continuos. La segunda, la implementación de protocolos de análisis completos sobre datos distribuidos a escala fina en el espacio, con códigos de programa listos para ejecutar en el software estadístico R (Team 2019) y en el software InfoStat (Di Rienzo et al. 2019). La tercera parte del texto ilustra la implementación del manejo y análisis de datos distribuidos a escala regional con códigos en R. La versión digital de este libro puede obtenerse desde www.agro.unc.edu.ar/~estadisticaaplicada donde también se encuentran los códigos de programación y los datos usados en este texto.\n\n   Esta obra está bajo una Licencia  Creative Commons Atribución – No Comercial – Sin Obra Derivada 4.0 Internacional.\n\n\n\n\n\n\n\nCorrea Morales, Juan Carlos, Barrera Causil, y Carlos Javier. 2018. Introducción a la estadística bayesiana: notas de clase. Instituto Tecnológico Metropolitano.\n\n\nCressie, Noel, y Christopher K Wikle. 2015. Statistics for spatio-temporal data. John Wiley & Sons.\n\n\nDi Rienzo, J A, F Casanoves, M G Balzarini, L Gonzalez, M Tablada, y C W Robledo. 2019. «InfoStat».\n\n\nLi, Jin, Andrew D Heap, Anna Potter, y James J Daniell. 2011. «Application of machine learning methods to spatial interpolation of environmental variables». Environmental Modelling & Software 26 (12): 1647-59.\n\n\nSchabenberger, Oliver, y Carol A Gotway. 2005. Statistical methods for spatial data analysis. CRC press.\n\n\nTeam, R Core. 2019. «R: A Language and Environment for Statistical Computing»."
  },
  {
    "objectID": "Parte1_cap_a_MjoDtosEsp.html#transformación-y-conversión-de-coordenadas",
    "href": "Parte1_cap_a_MjoDtosEsp.html#transformación-y-conversión-de-coordenadas",
    "title": "\n1  Manejo de datos espaciales\n",
    "section": "\n1.1 Transformación y conversión de coordenadas",
    "text": "1.1 Transformación y conversión de coordenadas\nPara localizar el sitio (coordenadas) con el cual se asocia un dato espacial, se necesita un sistema de referencia. Existen dos tipos de coordenadas, cartesianas y geográficas. Las coordenadas cartesianas se miden desde el centro de la tierra, mientras que las geográficas desde una superficie de referencia o datum. Para Sudamérica el datum comúnmente utilizado es WGS84 (World Geodetic System 84). Éste es el datum estándar por defecto para coordenadas en los dispositivos GPS comerciales. Para combinar capas de información o para realizar otros procesamientos de datos espaciales es necesario conocer el datum y frecuentemente transformar o convertir las coordenadas. Transformar implica pasar de un sistema de referencia a otro (cambiar el datum), mientras que cuando se convierten coordenadas no se cambia de datum.\nPor una cuestión de practicidad, es usual proyectar el sistema de coordenadas geográficas (expresados en grados, minutos y segundos) a un sistema de coordenadas cartesianas, como por ejemplo el sistema de proyección UTM (Universal Transverse Mercator). Esta operación permite que las distancias entre los sitios desde donde se leen los datos se expresen como distancias absolutas (metros) en vez de distancias relativas (grados). Por ello, un paso inicial en el análisis de datos espaciales es convertir las coordenadas geográficas en coordenadas cartesianas (UTM). La mayoría del software SIG tiene la capacidad para realizar la transformación o conversión de coordenadas."
  },
  {
    "objectID": "Parte1_cap_a_MjoDtosEsp.html#manipulación-de-múltiples-capas-de-datos",
    "href": "Parte1_cap_a_MjoDtosEsp.html#manipulación-de-múltiples-capas-de-datos",
    "title": "\n1  Manejo de datos espaciales\n",
    "section": "\n1.2 Manipulación de múltiples capas de datos",
    "text": "1.2 Manipulación de múltiples capas de datos\nCuando se recolectan datos de más de una variable georreferencia (múltiples capas de datos especializados) es poco probable que se registre la misma ubicación para cada variable o tiempo de medición. Por ejemplo, rara vez las mediciones de propiedades del suelo y los índices derivados de imágenes satelitales de cultivos no son obtenidas exactamente para la misma localización y frecuentemente existen capas de datos en distintas escalas. Esta variabilidad en las coordenadas espaciales dificulta la fusión de datos para realizar análisis estadísticos multivariados, i.e. análisis que contemplen simultáneamente las distintas capas de datos.\nSe necesita organizar los datos en una grilla común a todas las capas, de manera que cada celda de la grilla cuente con la información de su ubicación espacial y cada una de las variables medidas. Existen diversas alternativas metodológicas para crear este tipo de grillas. Una de ellas consiste en generar una grilla regular de una determinada dimensión la cual se interseca con cada una de las variables medidas. Luego los valores de cada capa son asignados al nodo de la celda más cercana al punto medido. Cuando se tiene más de un dato de una variable para el mismo nodo, se suele calcular una medida de posición como la media o mediana de los datos e inclusive en algunos casos puede ser de interés tomar una medida de variabilidad como el desvió estándar o coeficiente de variación de los datos que comparten la celda. Otra alternativa metodológica es generar la grilla regular y utilizar la información recolectada para realizar una interpolación espacial en sitios no medidos y así obtener una predicción espacial de la variable de interés en cada celda de la grilla. Este proceso se realiza para cada una de las variables medidas empleando la misma grilla. Diversos métodos de interpolación pueden ser usados, uno frecuente es la interpolación kriging.\nEl espaciado de la grilla debe reflejar el nivel de detalle requerido y la capacidad de procesamiento computacional. Por ejemplo, en aplicaciones de agricultura de precisión (escala de lote) puede utilizase una grilla de celdas cuadradas de 5 m × 5 m que se aproxima a la mitad del ancho operativo básico de muchas maquinarias. Esto genera unos 400 puntos de grilla por hectárea. Con lotes grandes puede ser conveniente utilizar una cuadricula de 10 m × 10 m para superar problemas computacionales y al mismo tiempo mantener una resolución de mapa adecuada para la visualización y análisis de los patrones espaciales.\nLa normalización de los datos es otra práctica comúnmente usada en el manejo de múltiples capas de datos. Con esta técnica se busca ajustar los valores de variables no conmensurables, incluso medidos en diferentes escalas a una escala común. La normalización puede realizarse en base a la media de cada capa o variable y expresar la unidad como un porcentaje (%) de la media. La normalización también suele realizarse utilizando el máximo de la capa como referente o calculando la diferencia de la variable respecto al valor mínimo y dividiendo por el rango. Finalmente, cabe citar a la estandarización (sustracción de la media y división por el desvío estándar) como una transformación usual para expresar variables un conjunto de variables no conmensurables en un conjunto de variables normal estándar.\nUn paso importante en el análisis exploratorio de datos geoestadísticos es explorar la distribución de la variable. Para ello, puede realizarse una estadística descriptiva que incluye la elaboración de gráficos de distribución de frecuencias y medidas resumen (media, mediana y coeficiente de asimetría) de la variable en análisis. Cuando el método de análisis supone distribución normal de los datos, estas medidas exploratorias pueden ayudar a verificar el cumplimiento de los supuestos. Se considera que una distribución de frecuencias es simétrica y está próxima a la de una variable normal cuando la media y la mediana son prácticamente iguales y el coeficiente de asimetría es inferior a 1. La distribución de la variable también provee información para la depuración de datos raros."
  },
  {
    "objectID": "Parte1_cap_a_MjoDtosEsp.html#depuración-de-datos",
    "href": "Parte1_cap_a_MjoDtosEsp.html#depuración-de-datos",
    "title": "\n1  Manejo de datos espaciales\n",
    "section": "\n1.3 Depuración de datos",
    "text": "1.3 Depuración de datos\nLos outliers, datos raros o atípicos, son observaciones con valores que se encuentran fuera del patrón general o distribución del conjunto de datos. La eliminación de los outliers es fundamental en el análisis de datos espaciales ya que las varianzas espaciales son muy sensibles a la presencia de datos raros. Los outliers deben eliminarse cuando el conjunto de datos no se limita dentro del rango de variación esperable con valores máximos y mínimos derivados de conocimiento previo sobre la distribución de la variable. También pueden eliminarse desde un criterio estadístico, cuando luego de calcular la media y la desviación estándar (SD), se identifican los valores que se encuentran fuera de la media \\(\\pm\\) 3 SD. Según conocimiento teórico, el 89% de los datos de una variable debieran encontrarse entre la media \\(\\pm\\) 3 SD cualquiera sea la distribución de la variable. Es recomendable, antes de la eliminación de los outliers, graficarlos utilizando coordenadas espaciales para visualizar su localización. De esta manera será posible identificar si los datos seleccionados para ser eliminados se relacionan con algún patrón sistemático o se corresponden a errores aleatorios.\nAl eliminar los outliers globales se eliminan los extremos del conjunto de datos, pero no los extremos locales (outliers espaciales). Los outliers espaciales, conocidos también como inliers, son datos que difieren significativamente de su vecindario, pero se sitúan dentro del rango general de variación del conjunto de datos. Existen estadísticos para identificar inliers, tal es el caso del índice autocorrelación espacial local de Moran (LM) (Anselin 1995). Dado un grupo de datos que pertenecen a diferentes vecindarios, el LM es aplicado a cada dato individualmente y da idea del grado de similitud o diferencia entre el valor de una observación respecto al valor de sus vecinos. La fórmula del índice de autocorrelación espacial local de Moran es la siguiente:\n\\[{LM}_i=\\frac{n\\ (Z(s_i)-\\bar{z})}{(n-1)s^2}\\sum_{j=1}^{n}\\left[w_{ij}\\left(Z(s_j)-\\bar{z}\\right)\\right]\\]\ndonde \\(Z(s_i)\\) es el valor de la variable z en la posición \\(i\\); \\(\\bar{z}\\) y \\(s^2\\) son la media y varianza muestral de \\(z\\), respectivamente; \\(Z(s_j)\\) es el valor de la variable \\(z\\) en todos los otros sitios (donde \\(j\\neq i\\)); \\(w_{ij}\\) es el peso espacial entre las ubicaciones \\(i\\) y \\(j\\).\nPara el cálculo del Índice de Moran se debe identificar el vecindario de cada dato, es decir el dominio donde existen datos que pueden ser interpretados como vecinos espaciales y que serán usados como referencia para decidir si el dato correspondiente es o no diferente a sus vecinos. Los vecindarios se definen a través de redes de conexión las que si bien pueden ser de distintos tipos pueden expresarse en el formato de una matriz de ponderación espacial \\(W\\). Cuando \\(W\\) es binaria, i.e. compuesta por ceros y unos, se indica con 1 si la posición \\(j\\) se considera vecina a la posición \\(i\\). Otra posibilidad para construir la matriz de ponderaciones espaciales es usando uná función de la distancia \\(d\\) (usualmente distancia Euclídea) entre los sitios \\(i\\), \\(j\\) como elemento de \\(W\\). Una función de amplio uso es la inversa de la distancia, es decir: \\(w_{ij}=1/d_{ij}\\). Así, valores muy cercanos en el espacio tendrán mayor ponderación. Existen diferentes opciones para definir el tamaño y la forma de los vecindarios de un dato espacial.\nEl índice de Moran local esta estandarizado por lo que su nivel de significación puede ser evaluado en base a una distribución normal estándar. Los valores positivos del LM se corresponden con agrupamiento espacial de valores similares ya sean altos o bajos (autocorrelación positiva), mientras que un valor de LM negativo indica un agrupamiento de valores diferentes, por ejemplo, un sitio con valor bajo de la variable se encuentra rodeado de vecinos con valores altos (autocorrelación negativa).\nPara determinar la significancia estadística de LM, se calcula el valor-p asociado a la prueba de hipótesis que establece que la correlación de la información de un sitio con la de sus vecinos es nula. El valor-p para un índice determinado debe ser lo suficientemente pequeño para considerar el valor en cuestión como un outlier espacial o inlier (rechazar la hipótesis nula). Dado que se realiza una prueba de hipótesis para cada uno de los puntos espaciales, se recomienda el ajuste de los valores-p por el criterio de Bonferroni.\nPara visualizar el índice LM se puede representar en un diagrama de dispersión la similitud de cada valor observado respecto a las observaciones vecinas. Usualmente en el eje horizontal se expresan los valores de las observaciones mientras que en el vertical se representa el retardo espacial de la variable. Adicionalmente, se puede ajustar y añadir a este diagrama modelos de regresión lineal y estadísticos de influencia para identificar sitios con datos raros.\n\n\n\n\n\n\nAnselin, Luc. 1995. «Local indicators of spatial association—LISA». Geographical analysis 27 (2): 93-115."
  },
  {
    "objectID": "Parte1_cap_b_CarcVarEsp.html#semivariogramas",
    "href": "Parte1_cap_b_CarcVarEsp.html#semivariogramas",
    "title": "\n2  Caracterización de variabilidad espacial\n",
    "section": "\n2.1 Semivariogramas",
    "text": "2.1 Semivariogramas\nLa dependencia espacial o autocorrelación espacial, puede modelarse mediante un semivariograma. Esta función permite analizar la estructura y la naturaleza de la dependencia espacial en un conjunto de observaciones geo-referenciadas. El proceso espacial puede ser representado por el siguiente modelo estadístico:\n\\[Z\\left(s\\right)=\\mu+\\varepsilon\\left(s\\right)\\]\ndonde \\(\\mu\\) es la media del proceso y \\(\\varepsilon\\left(s\\right)\\) es un término de error aleatorio con media cero y covarianza \\(C(h)\\), donde \\(h\\) es el lag o separación en el espacio entre dos sitios particulares. Un campo aleatorio \\({Z(s):\\ s\\in\\ D\\subset R^d}\\) es estrictamente estacionario si la distribución espacial es invariante bajo traslación de las coordenadas a través de todo el dominio (estacionaridad en sentido fuerte). La estacionaridad de segundo orden, o estacionaridad en sentido débil, se produce cuando \\(E\\left[Z\\left(s\\right)\\right]=\\mu\\left(s\\right)\\) y \\(Cov\\big[Z(s),Z(s+h)\\big]=C(h)\\). Es decir, en un campo aleatorio estacionario de segundo orden, la media es constante y la covarianza entre observaciones sobre diferentes posiciones, es función de la separación espacial entre los sitios en las que son tomadas, \\(C(h)\\) es la función de covarianza del proceso espacial. La estacionaridad de primero orden implica la estacionaridad de segundo orden, pero la inversa no es cierta.\nDado que \\(C(h)\\) no depende del valor de las coordenadas y \\(Cov\\big[Z(s),Z(s+0)\\big]= Var \\big[Z(s)\\big]=C\\), en procesos estacionarios de segundo orden, la variabilidad es la misma en todas partes, i.e. \\(Var [Z(s)]=\\sigma^2\\) no es una función de la ubicación espacial. En síntesis, un proceso espacial estacionario de segundo orden tiene media y varianza constantes y la función de covarianza no depende en absoluto de las coordenadas. A \\(C\\left(h\\right)\\) también se la conoce como función de autocovarianza y depende de la escala en la cual \\(Z\\) fue medida. Resulta más conveniente y fácil de interpretar si se la hace adimensional convirtiéndola en autocorrelación \\(\\rho\\left(h\\right)=\\frac{C(h)}{C}\\). La función \\(\\rho\\left(h\\right)\\) se denomina correlograma del proceso espacial.\nAún si \\(Z(h)\\) no es estacionaria de segundo orden, el incremento \\(Z(s)-Z(s+h)\\) puede serlo. Un proceso que tiene esta característica se dice que tiene estacionaridad intrínseca. Esto se produce si \\(E\\big[(Z(s)\\big]=\\mu\\) y \\(\\frac{1}{2}Var\\big[Z(s)-Z(s+h)\\big]=\\gamma(h)\\).\nLa función \\(\\gamma(h)\\) es llamada semivariograma del proceso espacial. La clase de procesos intrínsecamente estacionario es más grande que la clase de procesos estacionarios de segundo orden Notar que un proceso espacial que presenta estacionaridad intrínseca no es necesariamente estacionario de segundo orden. En condiciones de estacionaridad de segundo orden la función de covarianza es el semivariograma.\nUn proceso que parece estacionario en una escala podría no serlo a otra escala (i.e. presentar una tendencia o un componente sistemático). En el modelo, \\(\\mu\\) será remplazado por \\(\\mu(s)\\), i.e. término de tendencia determinístico para el sitio \\(s\\). El semivariograma, en estos casos se calcula sobre los residuos del modelo. El semivariograma, puede interpretarse como función de la varianza de la diferencia entre las observaciones. Si el semivariograma es sólo una función de la distancia entre observaciones, entonces es conocido como semivariograma isotrópico, i.e. no depende de la dirección. El semivariograma y covariograma son parámetros del proceso espacial y juegan un rol crítico en los métodos geoestadísticos de análisis de datos espaciales.\nUn primer paso para caracterizar la variación espacial en un dominio continuo es construir un semivariograma experimental o empírico. Una fórmula usual para computar semivariogramas, es conocida como estimador de los momentos de Matheron\n\\[\\hat{\\gamma}(h)=\\frac{1}{2 m (h)}\\sum_{i=1}^{m(h)} \\Big\\{Z(s_i)-Z(s_i+h) \\Big\\}^2\\]\ndonde \\(m(h)\\) es el número de pares de puntos separados por la particular distancia \\(h\\). El otro estimador ampliamente usado es el estimador de Cressie- Hawkins o estimador robusto cuya fórmula se expresa como\n\\[2 \\widetilde{\\gamma}(h)= \\frac{\\Big[ \\frac{1}{m(h)} \\sum_{i=1}^{m(h)} \\Big| Z(s_i) - Z(s_i + h) \\Big| ^\\frac{1}{2}  \\Big] ^4}{0,457 + \\frac{0,494}{m(h)} + \\frac{0,045}{m^2(h)}}\\]\nEste estimador puede ser menos sesgado que \\(\\hat{\\gamma}(h)\\) cuando la varianza residual es relativamente pequeña siendo también menos sensible a la presencia de valores externos. El estimador muestra típicamente menor variación en distancias pequeñas y también resulta en valores generalmente más pequeños que el estimador de los momentos de Matheron. Computando cualquiera de los dos estimadores, para las distancias \\(h\\), obtenemos un conjunto ordenado de semivarianzas. Tales semivarianzas graficadas en función h constituye el semivariograma empírico o experimental.\nLos parámetros de un semivariograma son: la varianza nugget o simplemente nugget \\((C_0)\\), la varianza estructural o sill parcial \\((C)\\) y el rango \\((R)\\). La asíntota es llamada la meseta del semivariograma o \\(C\\) y el lag o distancia \\(h^\\ast\\) en el cual la meseta es alcanzada se denomina \\(R\\) o rango. Observaciones \\(Z(s_i)\\) y \\(Z(s_j)\\) para las cuales \\(|| Z(s_i) - Z(s_j)|| \\geq h^\\ast\\) son espacialmente independientes. Si el semivariograma alcanza la meseta asintóticamente, se define el rango práctico \\((R_P)\\) como la distancia en el cual la semivarianza alcanza el 95% de la varianza umbral o total.\n\n\n\n\na) Semivariograma empírico. b) Semivariograma teórico, modelo esférico. Se representan los tres parámetros que lo definen: rango, sill y efecto pepita o nugget.\n\n\n\n\nEn la práctica el semivariograma empírico \\(\\hat{\\gamma}(h)\\) puede no pasar a través del origen. La ordenada al origen del semivariograma representa a \\(C_{0}\\), por lo tanto \\(C_0=\\lim_{h\\rightarrow0}{g(h)}\\neq0\\). Este parámetro representa la suma de errores aleatorios o no estructurados espacialmente, así como errores asociados con la variabilidad espacial a escalas más finas que la usada para realizar las mediciones. Un alto valor de \\(C_0\\) indica que la mayoría de la variación espacial no es explicada por el semivariograma. La varianza umbral o sill se obtiene sumando las varianzas antes mencionadas (\\(C_0+C\\)) y es la varianza de observaciones independientes, es decir observaciones que fueron tomadas a mayor distancia que \\(R\\).\nUn semivariograma se define como anisotrópico si cambia en alguna forma respecto a la dirección que se considere. Si el semivariograma no solo depende de la longitud del vector h sino también de la dirección del vector entonces el semivariograma es anisotrópico. En los casos isotrópicos, los contornos de isocorrelación son esféricos, mientras que en el caso que haya anisotropía los contornos de isocorrelación son elípticos. Se reconocen dos tipos de anisotropía: anisotropía geométrica y anisotropía zonal. Anisotropía geométrica ocurre cuando el rango del semivariograma cambia en las distintas direcciones, pero no la varianza sill, por lo tanto, la correlación es más fuerte en una dirección que en otra. Anisotropía zonal existe cuando la varianza estructural del semivariograma cambia con la dirección. Anisotropía geométrica significa que la correlación es más fuerte en una dirección que en otra.\nUna forma en que la anisotropía geométrica puede ser identificada es graficando un semivariograma experimental direccional. Diferencias en el semivariograma muestral usando diferentes ángulos al computarlo, es indicador de anisotropía. La anisotropía geométrica puede ser modelada cambiando el modelo de semivariograma por un proceso isotrópico transformando las coordenadas. Los modelos teóricos de semivariograma más usados en predicción espacial están basados son isotrópicos, por lo que es necesario una corrección en casos de anisotropía para poder utilizar la metodología clásica de predicción en geoestadística. El radio de anisotropía, es decir, el cociente entre los rangos de la dirección de máximo y mínima variación es usada para mesurar anisotropía. Algunos autores consideran que existe anisotropía significativa si el radio de anisotropía es mayor a 2,5.\n\n\n\n\na) Modelo isotrópico. b) Modelo anisotrópico, con ángulo de anisotropía de 45º y un radio de anisotropía de 0,5.\n\n\n\n\nEn los procesos espaciales continuos, caracterizados por semivariogramas suelen obtenerse medidas del grado de estructuración espacial. Una de éstas es la varianza estructural relativa (RSV):\n\\[RSV=\\Bigg(\\frac{C}{C+C_0}\\Bigg)\\times100\\%\\]\nUn valor alto de RSV indica que las predicciones geoestadísticas serán más eficientes que aquellas obtenidas con métodos de predicción que ignoran la información espacial. Un valor alto de RSV también indica una continuidad mayor del proceso espacial. Zimback (2001) establece que el grado de dependencia espacial puede ser clasificado como: \\(RSV \\leq 25\\%\\) bajo, \\(RSV\\) entre \\(25\\%\\) y \\(75\\%\\) medio y \\(RSV \\geq 75\\%\\) alto. También se puede calcular el cociente \\(\\frac{C_0}{C_0+C}\\) y en función de éste hablar de estructura espacial fuerte cuando el cociente es: \\(\\leq 25\\%\\), intermedia si el mismo se encuentra entre 25% y 75% y débil si el mismo es mayor al 75%.\n\n2.1.1 Ajuste de semivariogramas\nEl semivariograma empírico \\(\\hat{\\gamma}(h)\\), es un estimador insesgado de \\(\\gamma(h)\\), pero provee solo estimaciones para un conjunto finito de distancias. Para obtener estimaciones de \\(\\gamma(h)\\), para cualquier lag, al semivariograma empírico se le ajusta un modelo teórico. El análisis geoestadístico sigue entonces estos dos pasos: 1) obtención del semivariograma empírico y 2) ajuste de un modelo teórico de semivariograma al semivariograma empírico.\nLas funciones que sirven como modelos teóricos de semivariograma deben ser condicionalmente definidas positivas. Existen varios modelos teóricos para funciones semivariogramas, entre los que se encuentran el modelo nugget, el lineal, el esférico, el gaussiano y el exponencial (Figura @ref(fig:figSemivariogramas)). El semivariograma de un proceso de ruido blanco (modelo nugget), donde los valores \\(Z\\left(s\\right)\\) se comportan como muestras aleatorias, todas con igual media y varianza sin correlación entre ellas. Este modelo suele ajustar el semivariograma empírico cuando la menor distancia de muestreo en los datos es mayor que el rango del proceso espacial.\n\n\n\n\nFunciones de semivariograma para el modelo exponencial, esférico y gaussiano. \\(C_0\\)=2, \\(C\\)=10 y \\(R\\)=200\n\n\n\n\nEl modelo esférico es uno de los más populares entre los modelos de semivariograma. Tiene dos características principales: un comportamiento lineal cerca del origen y el hecho de que a la distancia \\(R\\) el semivariograma encuentra la meseta y después de esta se mantiene llano. El modelo exponencial se aproxima a la meseta del semivariograma asintóticamente cuando \\(\\parallel h \\parallel\\to\\infty\\). En la parametrización mostrada en la Figura @ref(fig:figSemivariogramas), el parámetro \\(R\\) es el rango práctico del semivariograma. Frecuentemente el modelo puede encontrarse en una parametrización donde el exponente es \\(-\\parallel h \\parallel / R\\). Entonces el \\(R_p\\) corresponde a \\(3R\\). Para el mismo rango y meseta de un modelo esférico, el modelo exponencial alcanza el rango más rápidamente, es decir, a menor distancia que el modelo esférico. El modelo gaussiano exhibe un comportamiento cuadrático cerca del origen y produce una correlación de corto rango que son las más altas que para cualquier modelo estacionario de segundo grado con el mismo rango práctico. Además, es el más continuo cerca del origen de los considerados aquí. En la parametrización el rango práctico es \\(\\sqrt{3R}\\).\nEs importante notar que, si se realiza un análisis basado en semivariogramas y se pretende comparar los parámetros de los semivariogramas obtenidos bajo distintas condiciones, la utilización de modelos teóricos diferentes resulta poco útil. Hay que tener en cuenta que, por ejemplo, los rangos del modelo esférico y el exponencial no son directamente comparables. El modelo esférico es el único que tiene un umbral verdadero, ya que tanto el modelo exponencial como el gaussiano alcanzan el umbral de forma asintótica, o lo que es lo mismo, no lo alcanzan nunca y el modelo lineal no tiene umbral. En consecuencia, los rangos no son directamente equivalentes entre modelos. En este caso, es más conveniente elegir un único modelo para realizar comparaciones de procesos espaciales.\nLos modelos de semivariograma son no lineales a excepción del modelo nugget. Por ello, para la estimación de parámetros estas funciones se usan métodos basados en aproximaciones numéricas. El método de ajuste por mínimos cuadrados ponderados (WLS) es común en la práctica. Para ello, se elige una función y valores iniciales de los parámetros basados en la observación del semivariogramas empírico. El tamaño del conjunto de datos a partir del cual el modelo de semivariograma es ajustado depende del número de lags que se elija. Los valores de las clases de lag en las cuál el número de pares no es mayor a 30 debieran ser removidos si se ajusta el semivariograma por mínimos cuadrados. Journel y Huijbregts 1978 recomiendan solo usar lags menores a la mitad del máximo lag en el conjunto de datos. La distribución de los puntos en el espacio determinará para qué lags esto es posible."
  },
  {
    "objectID": "Parte1_cap_b_CarcVarEsp.html#correlación-espacial-bivariada",
    "href": "Parte1_cap_b_CarcVarEsp.html#correlación-espacial-bivariada",
    "title": "\n2  Caracterización de variabilidad espacial\n",
    "section": "\n2.2 Correlación espacial bivariada",
    "text": "2.2 Correlación espacial bivariada\n\n2.2.1 Coeficiente de correlación\nEl coeficiente de correlación lineal de Pearson (\\(r\\)) es una medida de la magnitud de la correlación lineal entre dos variables. Para calcularlo se supone que se tiene una muestra aleatoria de unidades de análisis donde se han registrado simultáneamente dos variables. El intervalo de confianza para \\(r\\) y el valor \\(p\\) usados para decidir si la correlación poblacional entre ambas variables es cero o distinta de cero, dependen del tamaño de la muestra \\(n\\). El tamaño de la muestra es el número de unidades de análisis independientes.\nCuando las variables en estudio exhiben autocorrelación espacial, las observaciones de cada una de éstas estarán correlacionadas dentro de un determinado vecindario, es decir, no serán independientes entre sí. Luego, en el caso de datos espaciales, se viola la suposición de observaciones independientes para la prueba de significancia \\(r\\). Una propuesta para contemplar las correlaciones generadas por patrones espaciales es calcular el coeficiente de correlación haciendo un ajuste para determinar el número de observaciones independientes (tamaño de muestra efectivo) para acompañar la inferencia necesaria.\nEl coeficiente de correlación modificado (Clifford, Richardson, y Hemon 1989; Dutilleul et al. 1993), permite evaluar correlación entre dos variables espacializadas en el mismo dominio espacial. La prueba se basa en la modificación de las varianzas y los grados de libertad de la prueba \\(t\\) estándar usada para evaluar significancia del coeficiente de correlación de Pearson y requiere de la estimación del tamaño efectivo de la muestra.\nConsiderando \\(A \\subset D\\) un grupo de \\(n\\) sitios \\(A={s_1, s_2,…,s_n}\\), se supone que \\(Z=Z(s_1), Z(s_2),…, Z(s_n)\\) y \\(Y= Y(s_1), Y(s_2),…,Y(s_n)\\) con media constante y matriz de varianzas y covarianzas \\(\\Sigma_Z\\) y \\(\\Sigma_Y\\). Se divide \\(D\\) en los estratos \\(D_0, D_1, D_2,…\\). Entonces \\(Cov(Z(s_i),Z(s_j))= C_Z(k)\\) y \\(Cov\\big(Y(s_i),Y(s_j)\\big)= C_Y(k)\\), con \\(s_i, s_j \\in D_k\\), para \\(k= 0,1,…\\) (Clifford, Richardson, y Hemon 1989) sugieren como estimador de \\(\\hat{C}_Y(h)\\)\n\\[ \\hat{C}_Y(h) = \\frac{\\sum_{s_i,s_j \\in A_k}{\\big( Y(s_i) - \\overline{Y} \\big) \\big( Y(s_j) - \\overline{Y} \\big)}} {n_k} \\]\ndonde \\(n_k\\) es la cardinalidad de \\(D_k\\) y y similaridad para \\(C_Z(k)\\). Luego, Clifford, Richardson, y Hemon (1989) sugirió utilizar \\(n^{-2}\\sum_h{n_h\\hat{C}_Z(h) \\hat{C}_Y(h)}\\) Como un estimador de la varianza condicional de \\(s_{ZY}=n^{-1} \\sum_D{ \\big(Z(s)-\\overline{Z} \\big) \\big(Y(s)-\\overline{Y} \\big) }\\). Como resultado se obtiene la prueba \\(t\\) modificada basada en el estadístico \\(W\\)\n\\[W=n \\; s_{ZY} \\Big( \\sum_h{n_h \\hat{C}_Z(h) \\hat{C}_Y(h)} \\Big)^{-2}\\]\nEl cual a partir de una serie de aproximaciones a la varianza del coeficiente de correlación de Pearson (\\(\\sigma_r^2\\)) entre los procesos \\(Z(s)\\) e \\(Y(s)\\) se puede escribir de la siguiente manera \\(W=(\\hat{M}-1)^{1/2}r\\), \\(\\hat{M} = 1 + {\\hat{\\sigma}}_r^{-2}\\) y \\(\\hat{\\sigma}_r^2 = \\frac{\\sum_h{n_h \\hat{C}_Z(h) \\hat{C}_Y(h)}} {n^2 s_Z^2 s_Y^2}\\).\nSe define \\(W\\) como una prueba t modificada con \\(\\hat{M}-2\\) grados de libertad, donde \\(\\hat{M}\\) es el tamaño de muestra efectivo asumiendo bajo hipótesis nula la no correlación entre \\(Z(s)\\) e \\(Y(s)\\). Cuando se presenta una estructura de correlación espacial positiva, generalmente \\(\\hat{M} < n\\), si existe estructura de autocorrelación negativa se espera que \\(\\hat{M} > n\\).\n\n2.2.2 Coeficiente de co-dispersión\nOtra forma usada en estadística espacial para explorar patrones de correlaciones o covariaciones entre dos variables espacializadas, es el coeficiente de co-dispersión, que cuantifica la correlación entre dos procesos espaciales para un lag espacial particular sobre un espacio bidimensional. Para dos procesos espaciales intrínsecamente estacionarios \\({Z(s):s\\in D\\subset R^2}\\) y \\({Y(s):s\\in D\\subset R^2}\\) definidos en una parte de la región \\(D\\subset R^2\\), el coeficiente de co-dispersión es definido como:\n\\[\\rho_{ZY}(h)= \\frac{E \\Big[ \\big(Z(s+h)-Z(s) \\big) \\big(Y(s+h)-Y(s) \\big) \\Big]}{\\sqrt{E \\big[Z(s+h)-Z(s) \\big]^2 E \\big[Y(s+h)-Y(s) \\big]^2}}\\]\nLa estructura de \\(\\rho_{ZY}\\) es computacionalmente similar al coeficiente de correlación de Pearson. Al igual que ese coeficiente, \\(\\rho_{ZY}(h)\\), donde los límites superior e inferior definen una asociación espacial negativa o positiva perfecta, respectivamente. Sin embardo, a diferencia del coeficiente de correlación de Pearson, \\(\\rho_{ZY}\\) depende del lag \\(h\\), que enfatiza la idea de que la correlación espacial es un valor asociado con una distancia en el plano. El cálculo de la correlación se realiza para diferentes distancias y direcciones en el espacio. Cuando el coeficiente de co-dispersión se calcula para muchas direcciones, es útil mostrar esos valores en un solo gráfico. Vallejos et al. (2015) proponen el mapa de co-dispersión para resumir en un plano los valores de los coeficientes de co-dispersión obtenidos para distintos lag espaciales (direcciones y distancias). El gráfico resume la información sobre la correlación entre dos procesos espaciales en forma radial sobre un plano que circunscribe los coeficientes en una semiesfera de radio no mayor al rango del proceso espacial @ref(fig:figGrafCoDisp). En general las correlaciones espaciales que se observan desde un gráfico de co-dispersión permanecen ocultas en el análisis exploratorio usual y pueden ser distintas a las correlaciones lineales de Pearson no restringidas espacialmente. El gráfico de co-dispersión no debe ser confundido con un mapeo de la co-dispersión de las variables en el espacio de interés. No captura similitudes relacionadas con los patrones o formas que están presentes en los procesos espaciales, sino que captura la dependencia espacial entre los procesos para una distancia h. Los ejes del gráfico de co-dispersión hacen referencia a los lag y direcciones y no a las coordenadas de los sitios muestrales originales.\n\n\n\n\nGráfico de co-dispersión mostrando la correlación espacial entre dos variables para varios lag espaciales."
  },
  {
    "objectID": "Parte1_cap_b_CarcVarEsp.html#interpolación-kriging",
    "href": "Parte1_cap_b_CarcVarEsp.html#interpolación-kriging",
    "title": "\n2  Caracterización de variabilidad espacial\n",
    "section": "\n2.3 Interpolación Kriging",
    "text": "2.3 Interpolación Kriging\nLa predicción espacial, es decir la predicción de valores de la variable en sitios del campo espacial donde no existen observaciones, usualmente se hace por el método kriging basándose en el semivariograma ajustado. Kriging proporciona el mejor estimador lineal insesgado del valor esperado para el sitio y un error de estimación conocido como varianza kriging. Esta varianza depende del modelo de semivariograma ajustado y de la ubicación en el espacio de los datos observados ya que son los datos observados en distintos sitios los que proveen información para aproximar el valor en el sitio sin dato. Las interpolaciones basadas en semivariograma, se denominan geoestadísticas y tienen ciertas ventajas respecto a interpolaciones determinísticas, como las obtenidas por el método IDW que se basa en las distancias geométricas entre los sitios con datos y el sitio a interpolar. Las observaciones que participan en la predicción se ponderan de forma distinta según la distancia estadística a la que se encuentran.\nLos parámetros del semivariograma son los que gobiernan la asignación de los pesos o ponderaciones de las observaciones vecinas al sitio al cual se le asignará la predicción. El parámetro nugget es determinante en la asignación de pesos. si la varianza del error es muy alta, todas las observaciones tenderán a tener el mismo peso en la interpolación. Por el contrario, si la varianza del error es baja, los coeficientes de ponderación serán distintos. Si el rango aumenta, cada punto tendrá mayor peso en la interpolación de otras observaciones. Entre los métodos de interpolación geoestadísticos que utilizan todos los datos simultáneamente se destacan los métodos de kriging ordinario, simple y universal. En el kriging ordinario la media de la variable es estimada localmente. En caso de conocer la media poblacional de la variable, hecho que raramente ocurre, se utiliza el kriging simple. En el kriging universal se estima también la influencia de una tendencia espacial de los datos. La predicción asignada a los puntos incógnita puede realizarse de manera puntual (kriging puntual) o definiendo bloques (kriging en bloques) (Schabenberger y Gotway 2005; Webster y Oliver 2007).\n\n2.3.1 Kriging ordinario\nEl kriging ordinario supone que la variación es aleatoria, que existe dependencia espacial y que el proceso espacial subyacente es intrínsecamente estacionario con media constante y varianza que depende solo de la separación en distancia entre los sitios y no de su posición. La predicción kriging resulta de una combinación lineal de los datos observados. Supongamos que los valores de \\(Z\\), han sido muestreados en los puntos \\(s_1{,s}_2,\\ldots,s_n\\), para generar N datos \\(z(s_i),\\ i=1,\\ 2,\\ldots, N\\). Para el caso del kriging ordinario puntual se predice \\(Z\\) en cualquier nuevo punto \\(s_0\\) mediante:\n\\[\\hat{Z}(s_0)=\\sum_{i=1}^{N}{w_iz(s_i)}\\]\ndonde \\(w\\) son los pesos asignados a cada observación. Para asegurar que la estimación del valor esperado para el sitio sea insesgada y de mínima varianza, los pesos son dado de manera que:\n\\[\\sum_{i = 1}^{N}{w_i=1}\\]\n\\[E\\big[\\hat{Z}(s_0)-z(s_0)\\big]=0\\]\n\\[\\begin{align*}\nVar\\big[\\hat{Z}(s_0) \\big] & = E\\big[\\hat{Z}(s_0) - z(s_0)^2 \\big] \\\\&= 2\\sum_{i=1}^{N}{w_i\\gamma(s_i-s_0)-\\sum_{i=1}^{N}\\sum_{j=1}^{N}{w_iw_j\\gamma(s_i-s_j)}}\n\\end{align*}\\]\ndonde la cantidad \\(\\gamma(s_i - s_0)\\) es la semivarianza de \\(Z\\) entre el punto de muestreo \\(i\\) y el punto objetivo \\(x_0\\) y \\(\\gamma(s_i - s_j)\\) es la semivariancia entre los puntos de muestreo \\(i\\) y \\(j\\). Las semivarianzas se derivan del modelo teórico de semivariograma, debido a que no hay existen valores de semivarianzas entre los sitios con datos y los sitios objetivos donde no existen valores observados. Si un sitio objetivo también es un punto de muestreo, el kriging puntual devuelve el valor observado en ese sitio y la varianza de estimación es cero. El kriging puntual es un interpolador exacto en este sentido. El siguiente paso en kriging es encontrar los pesos que minimizan la varianza de la predicción sujeto a la restricción de que la suman de los pesos se igual a 1.\n\\[\\sum_{i=1}^{N}w_i\\gamma(s_i-s_0)+\\psi(s_0)=\\gamma(s_j-s_0) \\forall j\\]\nLa cantidad \\(\\psi (s_0)\\) es el multiplicador de Lagrange introducido para lograr la minimización. La solución de las ecuaciones de kriging proporciona los pesos para las ponderaciones y la varianza de predicción se obtiene de la siguiente forma:\n\\[\\sigma^2(s_0)=\\sum_{i=1}^{N}{w_i\\gamma(}s_i-s_0)+\\ \\psi(s_0)\\]\n\n2.3.2 Kriging en bloques\nEl kriging en bloques consiste en estimar directamente el valor promedio de la variable sobre un soporte mayor que el soporte de los datos (bloque). Intuitivamente, la idea es calcular mediante kriging puntual los valores en todos los puntos de una superficie o bloque y usar la media de las predicciones como estimador del valor esperado para el sitio. La estimación para cualquier bloque sigue siendo un promedio ponderado de los datos, \\(z\\ (s_1),\\ z\\ (s_2),\\ ...,\\ z\\ (s_N)\\):\n\\[\\hat{Z}(B)=\\sum_{i=1}^{N}{w_iz(s_i)}\\]\nLos factores de ponderación se obtienen nuevamente para minimizar la varianza del error y para obtener un estimador insesgado de la media. La grilla de predicción sobre la que se construye el mapa de variabilidad espacial presenta una dimensión menor que la de los bloques, asegurándose la obtención de un mapa más suavizado respecto al obtenido con kriging puntual. El kriging en bloques ha mostrado ser efectivo a la hora de reducir errores que pueden trasladarse en los mapas como consecuencia de inexactitudes de datos puntuales.\n\n2.3.3 Kriging local\nHemos dicho que los pesos de las observaciones en la predicción geoestadística son funciones de las semivarianzas entre las observaciones en sitios en el vecindario, \\(\\gamma(s_i-s_j)\\), y entre cada punto de muestreo y el punto a predecir, \\(\\gamma(s_i-s_0)\\). En general solo los puntos cercanos al punto a predecir tienen un peso significativo. Cuando la relación nugget:sill es pequeña el interpolador kriging es visto como un predictor local, donde para la predicción de \\(Z(s_0)\\) participarán sólo datos de puntos dentro de la proximidad de \\(s_0\\) (kriging neighborhood o kriging local). El kriging local esencialmente asigna pesos \\(w(s_0)=0\\) para todos los puntos \\(s_i\\) fuera de la zona en la que se quiere predecir. Por otra parte, esto permite que podemos aceptar el supuesto de estacionariedad local (o cuasi estacionariedad), es decir se puede restringir el supuesto de estacionariedad de la media a los vecindarios del kriging. Lo que sucede en distancias mayores a las del vecindario del sitio no será de importancia para la predicción en el sitio. La predicción y varianza kriging dependen principalmente de la parte del semivariograma cercana al origen, por ello es de importancia modelar bien el semivariograma en estos lugares, i.e. dar más peso a las semivarianzas experimentales cercanas al origen. No hay reglas para definir el vecindario para implementar el kriging local, aunque cuando el nugget es relativamente bajo se pude definir un radio de vecindad cercano al rango o rango práctico del modelo de semivariograma ajustado. Cuando el efecto nugget es importante el radio de vecindad debería ser mayor al rango ya que es probable que puntos más distantes tengan aún peso significativo. Otra alternativa para definir el vecindario se basa en términos de un número mínimo y máximo de datos cercanos al punto a predecir. Algunos autores recomiendan utilizar un mínimo de 7 vecinos y un máximo de 20.\n\n2.3.4 Kriging universal\nLa suposición de estacionariedad intrínseca no se cumple cuando existen tendencias geográficas pronunciada de naturaleza sistemática y no aleatoria. La tendencia puede ser regional, es decir, una variación sistemática en toda la región de interés o local de un punto a otro dentro de la región estudiada. La existencia de tendencias puede ser explorada graficando los datos de la variable analizada en función a la variable que se supone genera la tendencia espacial. La tendencia también se manifiesta en los semivariogramas experimentales con un incremento de la semivarianza con la distancia que no tiene límites. Si hay tendencia, entonces \\(\\mu\\) ya no es constante, sino que depende de s. Además, el semivariograma experimental de los datos ya no estima el semivariograma de los errores aleatorios, \\(\\varepsilon(s)\\). Se necesita estimar el semivariograma de \\(\\varepsilon(s) = Z (s) - \\mu(s)\\). Cuando este variograma es el input del kriging, el proceso de interpolación se conoce como “kriging universal” y la predicción se obtiene como:\n\\[\\hat{Z}(s_0)=\\sum_{i=1}^{N}{w_i \\; f_kz(s_i)}\\]\ndonde \\(f_k\\) es función de las coordenadas espaciales.\n\n2.3.5 Validación cruzada\nLa predicción implica asignar nuevos valores de las variables respuesta a contextos o escenarios que no corresponden al conjunto de escenarios medidos, es decir no se trata de aquellos sitios que utilizaron para realizar la predicción espacial. Entre las alternativas para estimar la exactitud de la predicción existen las técnicas de validación cruzada o técnicas de partición del conjunto de datos en datos de calibración y datos de validación (Efron y Hastie 2016). Es necesario identificar un grupo de observaciones sobre las que se ajustará el modelo o el método que permite predecir, usualmente llamado datos de calibración, y otro grupo que se usará para validar, llamado datos de validación. El modelo (semivariograma teórico) se ajusta sobre el conjunto de datos de entrenamiento y posteriormente se usa para la predicción de interés, con observaciones del subconjunto de validación. Seguidamente, los valores observados del conjunto de validación se comparar con los valores predichos por el modelo. Usualmente el proceso se repite cruzando el rol de los subconjuntos de datos, es decir el que era de validación pasa a ser de calibración y viceversa.\nSin embargo, otras estrategias pueden ser usadas para la selección de los datos de entrenamiento y validación. Una es particionar en forma aleatoria los datos en ambos conjuntos. Otro tipo de validación cruzada es dejando uno fuera (Leave-One-Out) donde se utiliza una sola observación para conformar el subconjunto de validación y se deja al resto como subconjunto de entrenamiento. El modelo se ajusta utilizando las \\(n – 1\\) observaciones de entrenamiento y se obtiene una predicción para la observación excluida. Este proceso se repite \\(n\\) veces. Otro tipo de validación cruzada es \\(k-fold\\), donde las observaciones se dividen aleatoriamente en \\(k\\) grupos de aproximadamente igual tamaño. Uno de los \\(k\\) grupos se emplea como subconjunto de validación, mientras que el resto de los grupos se emplean para entrenar el modelo. El proceso de validación cruzada es repetido durante k iteraciones, con cada uno de los subconjuntos de datos de prueba. Un valor común de \\(k\\) que puede dar buenos resultados en cuanto al equilibrio sesgo-varianza para estimar el error de predicción es \\(k=10\\). Si el modelo tuvo un buen desempeño, los residuos de la validación cruzada serán pequeños, su media será cercana a cero y no presentarán estructura.\nEn la evaluación de modelos geoestadísticos, los valores predichos de kriging \\(\\hat{Z}(s_i)\\) se comparan con los observados \\(z(s_i)\\), y se calcula una medida resumen que caracteriza el resultado de la comparación. Algunas de estas medidas resumen son:\nError medio\n\\[ME=\\frac{1}{N}\\sum_{i=1}^{N}\\big\\{z(s_i)-\\hat{Z}(s_i)\\big\\}\\]\ndonde \\(N\\) es el número de observaciones, \\(z(s_i)\\) es el valor verdadero en \\(s_i\\) y \\(\\hat{Z}(s_i)\\) es el valor predicho en ese punto.\nError cuadrático medio (Mean Square Error, MSE):\n\\[MSE=\\frac{1}{N}\\sum_{i=1}^{N}{\\big\\{z(s_i)-\\hat{Z}(s_i)\\big\\}^2}\\]\nRaíz del error cuadrático medio:\n\\[RMSE=\\frac{1}{N}\\sqrt{\\sum_{i=1}^{N}\\big\\{z(s_i)-\\hat{Z}(s_i)\\big\\}^2}\\]\nMedia del cociente del error cuadrático (Mean Squared Deviation Ratio, MSDR):\n\\[MSDR=\\frac{1}{N} \\sum_{i=1}^{N}\\frac{ \\big\\{ z(s_i) - \\hat{Z}(s_i) \\big\\}^2} {\\hat{\\sigma}^2(s_i)}\\]\ndonde \\(\\hat{\\sigma}^2(s_i)\\) es la varianza de la predicción kriging en el punto \\(s_i\\).\nPara el caso de datos espaciales, no solo es necesario disponer de una medida de error de predicción global, sino que también hay que evaluar del error de la predicción en cada sitio específico, i.e. dimensionar el error puntual de la predicción espacial.\n\n\n\n\n\n\nBabai, László. 1979. «Monte-Carlo algorithms in graph isomorphism testing». Université tde Montréal Technical Report, DMS, n.º 79–10.\n\n\nClifford, Peter, Sylvia Richardson, y Denis Hemon. 1989. «Assessing the Significance of the Correlation between Two Spatial Processes». Biometrics 45 (1): 123-34. https://doi.org/10.2307/2532039.\n\n\nDutilleul, Pierre, Peter Clifford, Sylvia Richardson, y Denis Hemon. 1993. «Modifying the t Test for Assessing the Correlation Between Two Spatial Processes». Biometrics 49 (1): 305. https://doi.org/10.2307/2532625.\n\n\nEfron, Bradley, y Trevor Hastie. 2016. Computer age statistical inference: Algorithms, evidence, and data science. Computer Age Statistical Inference: Algorithms, Evidence, and Data Science. https://doi.org/10.1017/CBO9781316576533.\n\n\nSchabenberger, Oliver, y Carol A Gotway. 2005. Statistical methods for spatial data analysis. CRC press.\n\n\nVallejos, Ronny, Adriana Mallea, Myriam Herrera, y Silvia Ojeda. 2015. «A multivariate geostatistical approach for landscape classification from remotely sensed image data». Stochastic Environmental Research and Risk Assessment 29 (2): 369-78. https://doi.org/10.1007/s00477-014-0884-5.\n\n\nWebster, Richard, y Margaret A Oliver. 2007. Geostatistics for environmental scientists. Vadose Zone Journal. Vol. 1. 2. John Wiley & Sons. https://doi.org/10.2136/vzj2002.0321.\n\n\nZimback, C R L. 2001. «Análise espacial de atributos químicos de solos para fins de mapeamento da fertilidade do solo. 2001. 114 f»."
  },
  {
    "objectID": "Parte1_cap_c_CarcVarEspMultiCapa.html#análisis-de-componentes-principales",
    "href": "Parte1_cap_c_CarcVarEspMultiCapa.html#análisis-de-componentes-principales",
    "title": "\n3  Caracterización de variabilidad espacial con múltiples capas de datos\n",
    "section": "\n3.1 Análisis de componentes principales",
    "text": "3.1 Análisis de componentes principales\nDiferentes objetivos pueden surgir cuando analizamos un conjunto de datos que además de ser espaciales o georreferenciados es multivariado (i.e. múltiples capas de información sobre el mismo dominio espacial o varias variables por sitio). Por un lado, se puede querer resumir la variabilidad de los sitios usando unas pocas variables sintéticas que representen bien la variabilidad en las variables originales. Por otro, se puede querer resumir patrones espaciales usando unas pocas variables sintéticas que combinan las múltiples capas de información considerando la correlación espacial subyacente. Una solución al primer problema es usar el Análisis de Componentes Principales (PCA (Pearson 1901)). Mientras que el segundo objetivo puede ser abordado mediante el Análisis de Componentes Principales Espaciales propuesto por (Dray, Saïd, y Débias 2008), también conocido como MULTISPATI-PCA. Éste se basa en el PCA, pero incorpora la restricción dada por los datos espaciales mediante el cálculo del índice de Moran antes de obtener las variables sintéticas o componentes principales (PC). Los datos multivariados son generalmente registrados en una matriz \\(X\\) con \\(n\\) filas (observaciones) y \\(p\\) columnas (variables). El PCA permite identificar las variables que explican la mayor parte de la variabilidad total contenida en los datos, explorar las correlaciones entre variables y reducir la dimensión del análisis al combinar las variables originales en nuevas variables sintéticas. El PCA opera sobre la matriz de covarianza de las variables originales o de las variables estandarizadas con el fin de encontrar una base ortogonal de tal manera que el primer eje del nuevo espacio considera la dirección de mayor variación de los datos originales. La descomposición espectral de la matriz de covarianzas proporciona un conjunto de autovectores y sus correspondientes autovalores. Los autovectores contienen los coeficientes de ponderación para construir variables sintéticas como combinaciones lineales de las variables originales. Los coeficientes de cada variable en estas combinaciones lineales indican la importancia relativa de las variables para explicar la variabilidad entre las observaciones. Las combinaciones lineales obtenidas con PCA se llaman componentes principales (PC), son ortogonales y en conjunto explican la variabilidad de los datos originales. Existen tantas PC posibles de formar como columnas en la matriz \\(X\\). La primera componente (PC1) explica la mayor parte de la variación en el conjunto de datos y la segunda (PC2), la mayor parte de la variabilidad remanente o no explicada por la PC1, y así sucesivamente.\nLos resultados del PCA se pueden visualizar en un gráfico denominado Biplot (Karl Ruben Gabriel 1971) el cual permite representar en un plano óptimo para el estudio de variabilidad, las diferencias entre sitios, la correlación entre variables y las variables que mejor explican las principales componentes de variabilidad. La incorporación de la información geográfica o la característica espacial de los datos suele realizarse a posteriori del PCA mediante la asignación de los valores de las componentes a cada uno de los sitios georreferenciados o ajustando semivariogramas a las PC.\nEl objetivo de MULTISPATI-PCA, otra forma de trabajar con datos espaciales, es encontrar variables sintéticas independientes que optimicen el producto de la varianza total y la autocorrelación espacial. Para delimitar los vecindarios, MULTISPATI-PCA utiliza una matriz de pesos espaciales determinando cuáles y cuántas observaciones cercanas a cada sitio deben ser consideradas para el cálculo del índice de autocorrelación espacial. Este análisis permite estudiar las relaciones entre las variables considerando su estructura espacial. Para la implementación del análisis es necesario primero definir cómo la información espacial será incorporada. En MULTISAPTI - PCA, la detección de la estructura espacial se realiza a través del índice de Moran. Es necesaria la construcción de una red de conexión (también llamada gráficos de vecinos) la cual usa un criterio objetivo para definir que entidades son vecinas y cuáles no. Existen diferentes opciones o alternativas metodológicas para definir los vecindarios que dependen de los diferentes tipos de arreglos espaciales presente en los datos. Para muestreos irregulares los vecindarios suelen definirse a partir de la red de conexión propuesta por Gabriel (K. Ruben Gabriel y Sokal 1969), mediante la triangulación de Delaunay (Lee y Schachter 1980). Otro método es el de los vecinos más cercanos (Cover y Hart 1967) o el basado en la especificación de una distancia como radio del vecindario de cada sitio.\nUna vez que la red de conexión es definida, la información espacial es almacenada en una matriz de conexión binaria \\(C\\) (en la cual \\(c_{ij}=1\\) si las unidades espaciales \\(i\\) y \\(j\\) son vecinas o \\(c_{ij}=0\\) en caso contrario), la cual es simétrica y tiene tantas filas y columnas como sitios. Esta matriz de conectividad \\(C\\) en general es escalada para obtener la matriz de pesos espaciales (representación matemática de la disposición geográfica de los sitios en el dominio espacial). Los pesos espaciales reflejan a priori la ausencia \\((w_{ij}=0)\\), presencia \\((w_{ij}=1)\\) o intensidad \\((w_{ij}>0)\\) de la relación espacial entre los sitios. Una vez que los pesos espaciales han sido definidos, el índice de autocorrelación de Moran es computado.\nEl método MULTISPATI-PCA opera sobre la matriz \\(\\widetilde{X}=WX\\) que está compuesta por los promedios ponderados de los valores de los vecinos de cada sitio según indique la matriz de conexión espacial, esta matriz es llamada matriz lagged. Las dos matrices \\(X\\) y \\(\\widetilde{X}\\) tienen las mismas cantidades de columnas (variables) y de filas (sitios). El análisis MULTISPATI-PCA consiste en analizar la correlación entre este par de matrices (\\(\\widetilde{X}\\) y \\(X\\)) mediante un análisis de co-inercia (Dray, Chessel, y Thioulouse 2003). MULTISPATI-PCA maximiza el producto escalar entre una combinación lineal de las variables originales y una combinación lineal de variables lagged. La ventaja de MULTISPATI-PCA respecto al PCA es que las componentes principales espaciales del MULTISPATI-PCA (sPC) contemplan la autocorrelación espacial entre los sitios, maximizándola en las primeras componentes. Por lo tanto, las primeras sPC del MULTISPATI-PCA muestran fuertes estructuras espaciales o altos índices de autocorrelación y no sólo mayores varianzas como en el PCA clásico. El método MULTISPATI-PCA constituye una herramienta multivariada útil no sólo para mapear la variabilidad conjunta de múltiples capas de datos dentro del dominio espacial estudiado sino también para la delimitación de zonas o áreas homogéneas en sentido multivariado cuando las componentes espaciales se usan como inputs de algoritmos de clasificación."
  },
  {
    "objectID": "Parte1_cap_c_CarcVarEspMultiCapa.html#análisis-de-conglomerados",
    "href": "Parte1_cap_c_CarcVarEspMultiCapa.html#análisis-de-conglomerados",
    "title": "\n3  Caracterización de variabilidad espacial con múltiples capas de datos\n",
    "section": "\n3.2 Análisis de conglomerados",
    "text": "3.2 Análisis de conglomerados\nLos métodos multivariados, utilizados para la clasificación de sitios de un dominio espacial, suelen basarse en algoritmos de agrupamiento no supervisados como los algoritmos de conglomerados jerárquicos o en algoritmos de conglomerados no jerárquico como k-means o fuzzy k means. Contrariamente al algoritmo k-means u otros métodos determinísticos de agrupamiento en los que cada observación sólo puede pertenecer a un único clúster, los métodos de clasificación basados en la teoría difusa (como fuzzy k-means), permiten que cada observación pueda asignarse a más de un clúster, con diferentes grados de pertenencia para cada clúster. Aplicado a datos espaciales puede generar alta fragmentación ya que el algoritmo de agrupación no tiene en cuenta la información espacial asociada a cada observación. Frogbrook y Oliver (2007) y Milne et al. (2012) propusieron introducir la restricción espacial mediante la incorporación de nuevas variables asociadas a parámetros del variograma co-regionalizado o del variograma de la componente principal de las variables originales. Córdoba et al. (2012) propusieron implementar fuzzy k means usando las componentes principales espaciales como variables de entrada para la clasificación con datos espaciales, logrando así disminuir la fragmentación e incrementar la contigüidad de los conglomerados espaciales. Otra alternativa, para delimitar conglomerados espaciales es aplicar filtros espaciales a la clasificación resultante de un método de clasificación estándar (Galarza et al. 2013; Ping y Dobermann 2003).\nEn el método fuzzy k-means además de la matriz de datos \\(X\\) se genera la matriz de pertenencia difusa \\(U\\), que contiene los valores o asignaciones parciales de cada una de las n observaciones en cada uno de los k clusters o conglomerados, con la restricción que se debe cumplir para cualquier \\(i = 1,\\ldots,n\\) y para cualquier \\(j = 1,\\ldots,k\\):\n\\[u_{ij}\\in[0,1]\\ \\forall_i,_j\\]\n\\[\\sum_{j=1}^{k}{u_{ij}=1,\\ \\forall_j}\\]\nLa partición difusa óptima de los datos es la que minimiza la función objetivo \\(j_m\\) igual a la suma ponderada de las distancias cuadráticas entre las observaciones y los centroides de cada clúster que conforman la matriz \\(V\\):\n\\[j_m(U,V)=\\sum_{i=1}^{n}\\sum_{j=1}^{k}(u_{ij})^m(d_{ij})^2\\]\ndonde \\(m\\) es el coeficiente de ponderación difuso (\\(1\\le\\ m\\ < \\infty)\\) cuya función es controlar el grado de solapamiento que se establece entre los clusters y \\({{(d}_{ij})}^2\\) es el cuadrado de la distancia en el espacio de los atributos entre el punto \\(i\\) y la clase centroide \\(j\\). Distintas métricas de distancia pueden ser usadas. La distancia Euclídea se utiliza cuando las variables son independientes y de igual varianza. En caso contrario la distancia de Mahalanobis es usada. El algoritmo difuso fuzzy k-means utiliza un proceso iterativo que hace óptima la partición difusa de los datos \\(X\\). La estructura del algoritmo (Bezdek et al. 1981) es la misma para cualquier conjunto de variables de entrada. Cuando el algoritmo ha asignado pesos o probabilidades de pertenencia a cada grupo para cada observación, se computaban una serie de índices para validar los distintos arreglos de conglomerados.\nPara evaluar la clasificación conseguida con un determinado número de grupos, existen diferentes índices como el coeficiente de partición (o fuzziness performance index-FPI, (Bezdek et al. 1981)), el índice de entropía de la clasificación o normalized classification entropy (NCE, (Bezdek et al. 1981)), el índice de Xie-Beni (Xie y Beni 1991) y el de Fukuyama-Sugeno (Fukuyama y Sugeno 1989), entre otros.\nEl coeficiente de partición (CP) mide el grado de solapamiento (grado de fuzziness) entre los grupos formados. Se considera que mientras menos difusa es la partición, mejor es la clasificación. Por tanto, se prefiere la estructura con un número de conglomerados para la cual el coeficiente de partición es mayor. El máximo equivale a una clasificación en la que cada observación pertenece a un único clúster. El mínimo se da cuando cada observación pertenece, con la misma probabilidad, a cada clúster (mayor incertidumbre).\nOtro índice que se puede usar para decidir con cuantos conglomerados quedarse es el conocido como entropía de la partición (EP) que cuantifica el grado de desorganización de la clasificación. Para este índice los valores próximos a 0 son indicativos de una mejor clasificación, es decir, con mayor grado de organización o menos difusos. El índice de Xie-Beni (XB) evalúa el cociente entre las distancias intracluster e intercluster. Se prefieren particiones donde la distancia intra-cluster es mínima y la distancia inter-cluster máxima. El índice XB es considerado como una medida de compacidad. Un valor bajo de XB, representa una clasificación con grupos compactos y separables. Por consiguiente, la mejor partición se obtiene mediante la minimización de XB. El índice Fukuyama-Sugeno (FS) es función de la separación entre los centroides de los grupos y la media de todos los centroides. El mínimo de FS corresponde a una partición con clases compactas y separables. Es importante considerar que, para un conjunto de datos, los índices no son necesariamente consistentes entre sí sugiriendo diferentes números de clúster como partición óptima. Una propuesta es promediar el valor de estos índices normalizados por el máximo usando para CP su reciproco, \\(CP^*=1/CP\\) , para que el valor mínimo en todos los índices represente la estructura óptima.\n\n\n\n\n\n\nBezdek, James C, Chris Coray, Robert Gunderson, y James Watson. 1981. «Detection and characterization of cluster substructure i. linear structure: Fuzzy c-lines». SIAM Journal on Applied Mathematics 40 (2): 339-57.\n\n\nCórdoba, Mariano, Cecilia Bruno, José Luis J. L. Costa, y Mónica Balzarini. 2012. «Principal component analysis with georeferenced data. An application in precision agriculture». Rev. FCA UNCUYO 44 (1): 27-39.\n\n\nCover, Thomas, y Peter Hart. 1967. «Nearest neighbor pattern classification». IEEE transactions on information theory 13 (1): 21-27.\n\n\nDray, Stéphane, Daniel Chessel, y Jean Thioulouse. 2003. «Co‐inertia analysis and the linking of ecological data tables». Ecology 84 (11): 3078-89.\n\n\nDray, Stéphane, Sonia Saïd, y Françis Débias. 2008. «Spatial ordination of vegetation data using a generalization of Wartenberg’s multivariate spatial correlation». Journal of vegetation science 19 (1): 45-56.\n\n\nFrogbrook, Z L, y M A Oliver. 2007. «Identifying management zones in agricultural fields using spatially constrained classification of soil and ancillary data». Soil Use and Management 23 (1): 40-51. https://doi.org/10.1111/j.1475-2743.2006.00065.x.\n\n\nFukuyama, Yoshiki, y M. Sugeno. 1989. «A new method of choosing the number of clusters for the fuzzy c-mean method». En Proc. 5th Fuzzy Syst. Symp., 1989, 247-50.\n\n\nGabriel, K Ruben, y Robert R Sokal. 1969. «A new statistical approach to geographic variation analysis». Systematic zoology 18 (3): 259-78.\n\n\nGabriel, Karl Ruben. 1971. «The biplot graphic display of matrices with application to principal component analysis». Biometrika 58 (3): 453-67.\n\n\nGalarza, Romina, M Nicolás Mastaglia, Enrique M Albornoz, y César Martınez. 2013. «Identificación automática de zonas de manejo en lotes productivos agrıcolas». En V Congreso Argentino de Agroinformática (CAI) e 42da. JAIIO.\n\n\nLee, Der-Tsai, y Bruce J Schachter. 1980. «Two algorithms for constructing a Delaunay triangulation». International Journal of Computer & Information Sciences 9 (3): 219-42.\n\n\nMilne, A E, R Webster, D Ginsburg, y D Kindred. 2012. «Spatial multivariate classification of an arable field into compact management zones based on past crop yields». Computers and Electronics in Agriculture 80: 17-30. https://doi.org/10.1016/j.compag.2011.10.007.\n\n\nPearson, Karl. 1901. «LIII. On lines and planes of closest fit to systems of points in space». The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science 2 (11): 559-72. https://doi.org/10.1080/14786440109462720.\n\n\nPing, J L, y A Dobermann. 2003. «Creating Spatially Contiguous Yield Classes for Site-Specific Management». Agronomy Journal 95 (5): 1121. https://doi.org/10.2134/agronj2003.1121.\n\n\nXie, Xuanli Lisa, y Gerardo Beni. 1991. «A validity measure for fuzzy clustering». IEEE Transactions on Pattern Analysis & Machine Intelligence 13 (8): 841-47. https://doi.org/10.1109/34.85677."
  },
  {
    "objectID": "Parte1_cap_d_PredMultipleCapa.html#regresión-con-errores-correlacionados-espacialmente-vía-reml",
    "href": "Parte1_cap_d_PredMultipleCapa.html#regresión-con-errores-correlacionados-espacialmente-vía-reml",
    "title": "\n4  Predicción con múltiples capas de datos\n",
    "section": "\n4.1 Regresión con errores correlacionados espacialmente vía REML",
    "text": "4.1 Regresión con errores correlacionados espacialmente vía REML\nSe asume una relación lineal determinística entre la variable respuesta y las covariables espacializadas que se modela a través de coeficientes de regresión, y el proceso espacial subyacente en los datos se modela sobre los términos de error del modelo. Éstos, en lugar de considerarse independientes como en aproximaciones estadísticas clásicas, se suponen espacialmente correlacionados, i.e. con correlaciones entre pares de términos de error expresadas como función de la distancia que separa los sitios desde los que se obtuvieron las observaciones. La varianza residual del modelo mide la variabilidad no estructurada espacialmente y caracteriza la componente estocástica del modelo. Se ajusta un modelo directamente a los datos, y no a las semivarianzas como en la geoestadística clásica. La estructura de covariación espacial se define en función de la distancia entre las observaciones al igual que en la geoestadística, pero se estima simultáneamente con los parámetros del modelo de regresión, i.e. los coeficientes de regresión que interpretan como pendientes o cambios en la respuesta por unidad de cambio de la covariable de sitio.\nEl modelo de regresión lineal múltiple con errores correlacionados espacialmente para una variable \\(Y\\) asume la siguiente distribución para la i-ésima observación:\n\\[Y_i\\sim N \\big(E(Y),V(Y) \\big)\\]\n\\[Y_i=\\beta_0+\\sum_{j=1}^{p}{x_{ij}\\beta_j}+\\varepsilon_i\\]\n\\[\\varepsilon(s_i)\\sim N(0,\\sigma^2)\\]\n\\[Cov(\\varepsilon_i,\\varepsilon_j)=\\left\\{\\ \\begin{array}{lcc}\n\\sigma_s^2+\\sigma_e^2\\ &\\ si\\ &\\ i=j\n\\\\\\sigma_e^2\\ f(d_{ij})\\ &\\ si\\ &\\ i\\ \\neq\\ j\n\\end{array}\n\\right.\\]\ndonde \\(\\beta_0\\) es el intercepto; \\(\\beta_j\\) es el vector de efectos fijos para las variables regresoras \\(x_j\\); \\(x_{ij}\\) la valuación de la covariable \\(x_j\\) en el sitio \\(i\\) y \\(\\varepsilon_i\\) es el término de error que se asume distribuido Normal con media 0 y varianza \\(\\sigma^2\\). \\(Cov(\\varepsilon_i,\\varepsilon_j)\\) es la covarianza de los errores de los sitios \\(i\\) y \\(j\\) determinada a partir de una función de covarianza espacial dependiente de la distancia \\(d_{ij}\\) entre observaciones.\nLa estimación REML es la elegida ya que ha demostrado que reduce el sesgo en las estimaciones de los parámetros de covarianza (Morrell 1998). Si el proceso de ajuste se realiza en etapas, se recomienda postular un modelo saturado o con máximo número de covariables en primera instancia y sobre los residuos de este modelo, vía REML, estimar las varianzas y covarianzas en los datos. Seleccionado el modelo para la matriz de covarianza residual, se intentará reducir la componente determinística y en este momento, cuando se evalúan los coeficientes de regresión con un modelo de varianza-covarianza adecuado, se pude utilizar estimaciones máximum likelihood clásica (ML). El método de REML, ajusta los grados de libertad de los efectos fijos (estructura de medias) antes de estimar los componentes de varianza y por ello es preferido a ML al momento de identificar la matriz de covarianza residual más apropiada para modelar los términos de error. La correlación espacial en los errores podría explicarse con una función de correlación espacial exponencial, gaussiana, esférica o lineal, entre otras. Ellas expresan como la correlación entre dos observaciones decae con la interdistancia (usualmente Euclídea) entre los sitios desde los cuales se obtienen y consecuentemente la selección de un modelo de correlación espacial es equivalente a la selección de un modelo teórico para el ajuste de un semivariograma experimental de la geoestadística clásica. Los modelos de correlación espacial pueden contener o no el efecto nugget, i.e. una estimación de la varianza a una escala más fina que la de la grilla entre observaciones. Finalmente, el modelo para la matriz de covarianza residual puede ser homocedástico (i.e. con varianza residual única) o heterocedástico (i.e. con varianza residual diferente para distintos subgrupos de datos). Comúnmente se evalúan varios modelos alternativos para la matriz de covarianza residual y se selecciona aquel para cual el ajuste del modelo tenga un menor valor para los criterios de información penalizada como AIC y BIC."
  },
  {
    "objectID": "Parte1_cap_d_PredMultipleCapa.html#regresión-con-efectos-aleatorios-de-sitio-vía-inla",
    "href": "Parte1_cap_d_PredMultipleCapa.html#regresión-con-efectos-aleatorios-de-sitio-vía-inla",
    "title": "\n4  Predicción con múltiples capas de datos\n",
    "section": "\n4.2 Regresión con efectos aleatorios de sitio vía INLA",
    "text": "4.2 Regresión con efectos aleatorios de sitio vía INLA\nLos modelos lineales de covarianza residual pueden no ser eficientes para modelar procesos espaciales continuos que involucran matriz de covarianzas grandes y densas, por ejemplo, el modelado de correlaciones espaciales en un conjunto de datos con más de 10000 observaciones es computacionalmente intensivo y podría no ser logrado en computadores de escritorio actuales. Nuevas implementaciones de la regresión lineal múltiple para datos espaciales, desarrolladas en el marco teórico bayesiano, facilitan esta estimación\nEn estadística bayesiana se considera que los parámetros del modelo son variables aleatorias y se calculan distribuciones de probabilidad para los parámetros de las cuales se deriva medidas de incertidumbre (Correa Morales, Causil, y Javier 2018). La información previa sobre los parámetros debe resumirse en distribuciones de probabilidad denominadas distribuciones a priori, a partir de las cuales se estima la distribución de probabilidad a posteriori dadas las observaciones. Estimaciones puntuales de los parámetros de interés se pueden obtener calculando medidas resumen de la distribución a posteriori, como la media o el modo, y se informan juntos a intervalos de credibilidad calculados desde percentiles de la distribución a posteriori. La credibilidad se interpreta como la probabilidad de que el valor estimado para el parámetro pertenezca al intervalo reportado, dado los datos observados.\nLos métodos de simulación por cadenas de Markov Monte Carlo (MCMC), han permitido resolver modelos complejos sin la necesidad de imponer estructuras que lo simplifiquen. Éstos han sido usados para la estimación de modelos con datos espaciales. Sin embargo, el método MCMC también conlleva desafíos computacionales. Rue, Martino, y Chopin (2009) propusieron una alternativa para obtener la distribución a posteriori en contextos de datos espaciales a partir de aproximaciones basadas en el algoritmo INLA que bajo el supuesto de que la variación espacial subyacente se describe como un campo aleatorio gaussiano Markoviano simplifica las estimaciones de covarianzas espaciales en procesos espaciales continuos con una gran cantidad de observaciones. Sobre la base de las aproximaciones por INLA y la implementación de la alternativa en el lenguaje de programación R (R-INLA) se han popularizado las aplicaciones de la regresión bayesiana a datos espacial y espaciotemporales (Cameletti et al. 2013). En este contexto es posible obtener la matriz de precisión de los efectos aleatorio de sitio utilizando aproximaciones por ecuaciones diferenciales parciales estocásticas (spde) (Lindgren y Rue 2015). Bajo este enfoque se construye una malla de predicción con unidades de celda triangulares que cubre todo el dominio espacial para el cual se requiere la predicción, cada vértice de los triángulos representa un nodo sobre el que se predice la variable respuesta por interpolación (Blangiardo y Cameletti 2015). Es posible trabajar con dominios espaciales de límites y bordes complejos (Bakka et al. 2018) y asignar medidas de incertidumbre de cada predicción puntual ya que lo que se obtiene del modelo bayesiano es la distribución a posteriori de los valores predichos para cada sitio más que un único valor de predicción.\nBajo este enfoque el modelo de regresión bayesiana para una variable \\(Y\\) asume la siguiente distribución para la i-ésima observación:\n\\[Y_i\\sim N(\\eta_i,\\sigma_e^2)\\]\n\\[\\eta_i = \\beta_0 + \\sum_{j-1}^{p} {x_{ij}\\beta_j}+ \\xi(s_i)\\]\ndonde \\(\\beta_0\\) es el intercepto; \\(\\beta_j\\) es el vector de efectos fijos de las variables explicativas \\(x_j\\); \\(x_{ij}\\) la valuación de la covariable \\(x_j\\) en el sitio \\(i\\) y \\(\\xi(s_i)\\) el efecto aleatorio de sitio que se asume una realización de un proceso gausiano markoviano latente \\(\\xi(s_i) \\sim MVN(0,\\Sigma)\\), siendo \\(\\Sigma\\) la matriz de varianza y covarianza de los efectos de sitio definidos por la función de covariación espacial de Matérn (Matérn, 1986). En R-INLA la estimación de la inversa de \\(\\Sigma\\) (matriz de precisión) se resuelve eficientemente usando spde.\nPara estimar el modelo, es necesario obtener una representación de la estructura de dependencia desde una estructura de vecindario para datos continuos (malla). La malla se obtiene mediante triangulación de Delaunay restringida comenzando sobre la estructura base correspondiente a los vértices iniciales de las observaciones, luego se agregan o eliminan vértices adicionales para satisfacer las restricciones de la triangulación que se encuentran definidas por los siguientes parámetros: 1) offset define hasta qué punto se debe extender la malla hacia lo interno (es decir, dentro del área donde se pretende predecir) y hacia el exterior (es decir, fuera del área donde se pretende predecir); 2) cutoff define la distancia mínima entre vértices permitida; 3) maximumedge que refiere a la longitud máxima del borde de cada triángulo; 4) minimumangle o ángulo mínimo interior de cada triángulo. Construida la malla es necesario seleccionar un modelo de correlación espacial. En R-INLA con SPDE esta función se define parametrizando la función de correlación espacial de Matérn definiendo su parámetro alpha (variando entre 0 y 2). El valor por defecto de alpha es 2 y corresponde a una función de correlación espacial del tipo exponencial. La matriz de covarianza de los efectos aleatorios es una matriz rala dado el proceso espacial que se supone, y sus valores son aproximados por suavizado vía spde."
  },
  {
    "objectID": "Parte1_cap_d_PredMultipleCapa.html#regresión-vía-modelos-basados-en-árbol",
    "href": "Parte1_cap_d_PredMultipleCapa.html#regresión-vía-modelos-basados-en-árbol",
    "title": "\n4  Predicción con múltiples capas de datos\n",
    "section": "\n4.3 Regresión vía modelos basados en árbol",
    "text": "4.3 Regresión vía modelos basados en árbol\nEl término aprendizaje de máquina o aprendizaje automático corresponde a una rama de la inteligencia artificial que hace referencia a algoritmos o procedimientos de cálculo basados en intenso proceso computacional que “aprenden” de los datos intentando minimizar la intervención humana. Algunos se basan en particiones binarias recurrentes de los datos y evaluaciones de éstas hasta identificar la mejor para explicar variabilidad en la variable respuesta. Los árboles de clasificación y regresión (algoritmos CART) (Breiman 2001) se utilizan con fines predictivos y son particularmente útiles para interpretar relaciones (no necesariamente lineales) en contextos de regresión múltiple con variables explicativas correlacionadas. Estos algoritmos pueden ser empoderados mediante métodos de remuestreo que se usan para obtener muestras aleatorias a partir de los datos observados o muestra original, derivar modelos para cada muestra y ensamblar los resultados para optimizar la predicción. Otros algoritmos de aprendizaje automático basados en árboles son la regresiones por bosques aleatorios (RF por el término en inglés Random Forest) y los árboles de regresión generalizados (GBR por el término en inglés Generalized Boosting Regression) (Efron y Hastie 2016). Los algoritmos basados en árboles engloban así a un conjunto de técnicas supervisadas no paramétricas (i.e. sin supuestos distribucionales) para segmentar el espacio de los predictores en regiones simples con máxima diferencia en la variable respuesta. Es necesario tener cuidados particulares al momento de estimar el árbol que será usados como modelo predictivo ya que puede existir sobreajuste, i.e. construirse un modelo sólo útil para los datos disponibles cuyas predicciones pueden cambiar con pequeños cambios en el conjunto de datos observados.\nSi bien estos algoritmos se han utilizados para datos espaciales (Kanevski et al. 2009), es raro que se modele la estructura espacial. Una propuesta para incorporar la correlación espacial en los datos es utilizar las coordenadas geográficas o las matrices de distancias entre observaciones covariables en la construcción del modelo (Pejović et al. 2018). Otra propuesta, es modelar el residuo remanente del ajuste del algoritmo de aprendizaje automático con una función de autocorrelación espacial (Li et al. 2011) y finalmente combinar los resultados de la predicción determinística dada por el árbol y la predicción espacial obtenida mediante kriging de los residuos.\n\n4.3.1 Bosques aleatorios\nEl método de bosques aleatorios o Random Forest (RF) es una modificación del proceso de ensamblaje de varios árboles (Bagging) donde se ajustan múltiples árboles desde cada muestra obtenida por remuestreo formando un “bosque”. En cada nueva predicción, todos los árboles que forman el “bosque” participan aportando su predicción. Como valor final, se toma la media de todas las predicciones en el caso de variables respuesta continuas. El método RF a diferencia de Bagging realiza una selección aleatoria de \\(m\\) predictores antes de evaluar cada división. Si \\(m=p\\) los resultados de RF y Bagging son equivalentes. Este método trabaja bien con grandes bases de datos presentando mayor facilidad en la implementación y baja tendencia al sobreajuste. Para implementar RF es necesario optimizar el parámetro \\(m\\), no obstante, existe la recomendación de usar \\(m=\\frac{p}{3}\\) por defecto para regresión.\nPara contemplar la estructura espacial es posible combinar los resultados de RF con una interpolación geoestadística basada en kriging (Li et al. 2011). La predicción de los residuos se complementa con la predicción de RF de manera aditiva. Es recomendable que el ajuste del modelo espacial para realizar kriging se logre con un subconjunto de datos de entrenamiento, diferente al grupo de validación, para evitar sobreajustes.\n\n4.3.2 Árboles de regresión generalizados\nEl método Boosting es otro método de ensamblaje que consiste en ajustar secuencialmente modelos sencillos, de manera que cada modelo aprende de los errores del anterior. Los algoritmos de Boosting trabajan minimizando una función de pérdida (deviance) para maximizar la proporción de varianza que explica el modelo. Los árboles de regresión generalizados, conocido en inglés como Generalized Boosted Regression Trees (GBR) particularmente utiliza Boosting para ensamblar los árboles obtenidos de múltiples muestras obtenida mediante remuestreo de la muestra original. El algoritmo ajusta árboles de regresión a los datos de entrenamiento de manera iterativa. El primer árbol que se ajusta es aquel que, según la complejidad del árbol seleccionada, minimiza la deviance. El siguiente árbol se ajusta a los residuos del primer árbol. Luego, se vuelven a realizar predicciones para las observaciones que tienen en cuenta los dos árboles. En cada uno de los pasos siguientes se ajusta un nuevo árbol sobre los residuos de la combinación de los árboles anteriores. El procedimiento es parametrizado por varias constantes que es necesario identificar probando numerosas o todas las combinaciones posibles de parámetros ya que estos son dependientes entre sí.\nUna vez encontrada la combinación optima de parámetros se ajusta GBR en el grupo de entrenamiento. Luego, a partir de los residuos del modelo se ajusta un kriging y se guardan los parámetros de la función de semivarianza ajustados. Finalmente se utiliza el modelo GBR construido para predecir los datos en el grupo de validación adicionando a los resultados obtenidos desde el árbol la predicción de los residuos de cada sitio.\n\n\n\n\n\n\nBakka, Haakon, Håvard Rue, Geir Arne Fuglstad, Andrea Riebler, David Bolin, Janine Illian, Elias Krainski, Daniel Simpson, y Finn Lindgren. 2018. «Spatial modeling with R-INLA: A review». Wiley Interdisciplinary Reviews: Computational Statistics, n.º February: 1-24. https://doi.org/10.1002/wics.1443.\n\n\nBalzarini, Mónica, R Macchiavelli, y Fernando Casanoves. 2004. «Aplicaciones de modelos mixtos en agricultura y forestería». Curso de Capacitacion Centro Agronomico Tropical de Investigación y Enseñanza-CATIE.\n\n\nBlangiardo, Marta, y Michela Cameletti. 2015. Spatial and spatio-temporal Bayesian models with R-INLA. John Wiley & Sons.\n\n\nBreiman, Leo. 2001. «Random forests». Machine learning 45 (1): 5-32.\n\n\nBreiman, Leo, Jerome H. Friedman, Richard A. Olshen, y Charles J. Stone. 2017. Classification and regression trees. Classification and Regression Trees. https://doi.org/10.1201/9781315139470.\n\n\nBrenning, Alexander. 2012. «Spatial cross-validation and bootstrap for the assessment of prediction rules in remote sensing: The R package sperrorest». En Geoscience and Remote Sensing Symposium (IGARSS), 2012 IEEE International, 5372-75. IEEE.\n\n\nCameletti, Michela, Finn Lindgren, Daniel Simpson, y Håvard Rue. 2013. «Spatio-temporal modeling of particulate matter concentration through the SPDE approach». AStA Advances in Statistical Analysis 97 (2): 109-31. https://doi.org/10.1007/s10182-012-0196-3.\n\n\nCorrea Morales, Juan Carlos, Barrera Causil, y Carlos Javier. 2018. Introducción a la estadística bayesiana: notas de clase. Instituto Tecnológico Metropolitano.\n\n\nEfron, Bradley, y Trevor Hastie. 2016. Computer age statistical inference: Algorithms, evidence, and data science. Computer Age Statistical Inference: Algorithms, Evidence, and Data Science. https://doi.org/10.1017/CBO9781316576533.\n\n\nEfron, Bradley, y Robert Tibshirani. 1997. «Improvements on cross-validation: the 632+ bootstrap method». Journal of the American Statistical Association 92 (438): 548-60.\n\n\nHengl, Tomislav, Gerard B. M. Heuvelink, y David G. Rossiter. 2007. «About regression-kriging: From equations to case studies». Computers and Geosciences 33 (10): 1301-15. https://doi.org/10.1016/j.cageo.2007.05.001.\n\n\nKanevski, Mikhail, Vadim Timonin, Alexi Pozdnukhov, y Gordon Ritter. 2009. Machine learning for spatial environmental data: theory, applications, and software. Ssrn. EPFL press. https://doi.org/10.2139/ssrn.3015609.\n\n\nKuhn, Max, y Kjell Johnson. 2013. Applied predictive modeling. Vol. 26. Springer.\n\n\nLi, Jin, Andrew D Heap, Anna Potter, y James J Daniell. 2011. «Application of machine learning methods to spatial interpolation of environmental variables». Environmental Modelling & Software 26 (12): 1647-59.\n\n\nLindgren, Finn, y Håvard Rue. 2015. «Bayesian Spatial Modelling with R - INLA». Journal of Statistical Software 63 (19). https://doi.org/10.18637/jss.v063.i19.\n\n\nMorrell, Christopher H. 1998. «Likelihood ratio testing of variance components in the linear mixed-effects model using restricted maximum likelihood». Biometrics, 1560-68.\n\n\nPatterson, H Desmond, y Robin Thompson. 1971. «Recovery of inter-block information when block sizes are unequal». Biometrika 58 (3): 545-54.\n\n\nPejović, Milutin, Mladen Nikolić, Gerard B. M. Heuvelink, Tomislav Hengl, Milan Kilibarda, y Branislav Bajat. 2018. «Sparse regression interaction models for spatial prediction of soil properties in 3D». Computers and Geosciences 118: 1-13. https://doi.org/10.1016/j.cageo.2018.05.008.\n\n\nRue, Håvard, Sara Martino, y Nicolas Chopin. 2009. «Approximate Bayesian inference for latent Gaussian models by using integrated nested Laplace approximations». Journal of the royal statistical society: Series b (statistical methodology) 71 (2): 319-92."
  },
  {
    "objectID": "Parte2_cap_a_EscFinaImplR.html#conversión-de-coordenadas-espaciales",
    "href": "Parte2_cap_a_EscFinaImplR.html#conversión-de-coordenadas-espaciales",
    "title": "\n5  Implementación con R\n",
    "section": "\n5.1 Conversión de coordenadas espaciales",
    "text": "5.1 Conversión de coordenadas espaciales\nDado que la función utilizada para la generación del objeto datos no es específica para datos espaciales, es necesario transformar este objeto. Esta transformación permite ejecutar funciones estadísticas que solo trabajan sobre objetos de datos espaciales (clase sf). Para ello puede utilizarse funciones de los paquetes sf.\nLa función st_as_sf() convierte el data.frame en un objeto sf. Mediante el argumento coords, se le especifica que las columnas “x” e “y” del data.frame son los datos de las coordenadas espaciales. Se le asigna el sistema de coordenadas de referencia, utilizando el argumento crs = 4326, este argumento permiten hacer referencia a diferentes sistemas de proyecciones y asociar esta información al objeto con el que se está trabajando. Puede utilizarse tanto una cadena de caracteres aceptado por GDAL o Una alternativa para hacer referencia a un sistema de coordenadas particular es utilizar su código EPSG. La EPSG es una organización científica vinculada a la industria del petróleo europea (http://www.epsg-registry.org/) la cual desarrolló un repositorio que contiene información sobre sistemas de referencia, proyecciones cartográficas y elipsoides de todo el mundo. Para esta aplicación se utilizará la proyección longlat, la cual admite valores de longitud mayores a -180 y menores a 180 y valores de latitudes que se encuentren entre -90 y 90. El datum especificado es WGS84. Por esto, el texto utilizado puede ser \"+proj=longlat +datum=WGS84\", o bien el código 4326.\nA continuación, se genera un nuevo objeto espacial denominado datos_sf que contiene esta información.\n\ndatos_sf <- st_as_sf(datos, \n                     coords = c(\"x\", \"y\"), \n                     crs = 4326)\n\nLa función st_transform() permite transformar las coordenadas. Cuando se realiza la transformación del sistema de proyección geográfico a cartesiano, es necesario indicar a cuál zona o faja pertenecen los datos bajo análisis, para este caso la zona es 21. Al igual que en la sentencia anterior, se debe indicar el datum y elipsoide que en ambos casos corresponde a WGS84.\n\ndatos_sf <- \n  st_transform(datos_sf, \n               crs =\n                 \"+proj=utm\n                  +zone=21 \n                  +south \n                  +ellps=WGS84\n                  +datum=WGS84\")\n\nEsta transformación también puede realizarse en base al código EPSG en este caso para el sistema de coordenadas UTM faja 21 sur el código es 32721.\n\ndatos_sf <- st_transform(datos_sf, crs = 32721)\n\nSi se visualizan las primeras filas del objeto, se puede observar, entre otras cosas, la clase de objeto (sf), su geometría (puntos) y la información referida a su sistema de coordenadas.\n\nhead(datos_sf)\n\nSimple feature collection with 6 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 311983.1 ymin: 5800811 xmax: 313087.8 ymax: 5801277\nProjected CRS: WGS 84 / UTM zone 21S\n  Rinde                 geometry\n1 0.348 POINT (313087.8 5800921)\n2 0.360 POINT (311983.1 5800811)\n3 0.367 POINT (312932.5 5800910)\n4 0.001 POINT (312685.3 5801277)\n5 0.382 POINT (312856.4 5800906)\n6 0.409 POINT (312227.3 5801004)\n\n\nLa visualización de la información georreferenciada permite un rápido diagnóstico de la distribución de los datos y de los valores observados. Trabajando con datos de clase sf es posible realizar una rápida visualización de los datos espaciales. El paquete tmap permite realizar mapas temáticos. Para comenzar a realizar un mapa debe especificarse los datos con los que se desea trabajar mediante la función tm_shape() y luego se suman elementos creados con funciones de este paquete utilizando el símbolo +.\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\n\n\ntm_shape(datos_sf) + \n  tm_dots(\"Rinde\",title=\"Rendimiento (t/ha)\")\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting"
  },
  {
    "objectID": "Parte2_cap_a_EscFinaImplR.html#eliminación-de-outliers-e-inliers",
    "href": "Parte2_cap_a_EscFinaImplR.html#eliminación-de-outliers-e-inliers",
    "title": "\n5  Implementación con R\n",
    "section": "\n5.2 Eliminación de outliers e inliers\n",
    "text": "5.2 Eliminación de outliers e inliers\n\nEn un data.frame, una forma sencilla para obtener medidas resumen de una variable es con la función summary(). Se utiliza $ para hacer referencia a una columna particular dentro de un objeto.\n\nsummary(datos_sf$Rinde)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.001   3.706   4.679   4.617   5.476  24.000 \n\n\nLas funciones hist() y boxplot() realizan gráficos de histogramas y box-plots, respectivamente. Sus múltiples argumentos permiten la edición de cada gráfico. La función par() permite dividir la ventana gráfica de R, en el siguiente ejemplo se divide la ventana gráfica de R en dos columnas y una fila.\n\n#|layout-ncol: 2\n\nhist(\n  datos_sf$Rinde,\n  col = 'grey',\n  nclass = 20,\n  main = \"Histograma\",\n  ylab = 'Frecuencia Relativa',\n  xlab = 'Rendimiento (t/ha)'\n)\n\n\n\nboxplot(\n  datos$Rinde,\n  col = 'grey',\n  ylab = 'Rendimiento (t/ha)',\n  main = \"Box-Plot\",\n  ylim = c(0, 14)\n)\n\n\n\n\nLa función skewness() del paquete e1071 permite calcular el coeficiente de asimetría. Existen tres fórmulas para su cálculo (por defecto usa el tipo 3). Para más información, se puede utilizar help(skewness).\n\nskewness(datos_sf$Rinde)\n\n[1] 1.454937\n\n\nEn el histograma se observa asimetría derecha en la distribución de los datos. La asimetría también puede advertirse con los estadísticos con el coeficiente de asimetría el cual es de 1,45. En el gráfico box-plot se observan valores extremos de la variable que se encuentran principalmente por encima de la media \\(+\\) 3 SD.\nLas siguientes instrucciones calculan y crean objetos para la media, el DE y los límites superior (\\(media + 3DE\\)) e inferior (\\(media - 3DE\\)) con los que pueden detectarse los outliers y elimina estos valores.\n\nMedia <- mean(datos_sf$Rinde)\nDE <- sd(datos_sf$Rinde)\nLI <- Media-3*DE\nLS <- Media+3*DE\n\nLos símbolos | y & son operadores lógicos que significan or y and, respectivamente. Las siguientes instrucciones generan dos objetos. El objeto datos_1 es la base depurada, es decir sin outliers, mientras que el objeto outliers presenta los datos que han sido eliminados en esta etapa.\n\ndatos_1 <-\n  subset(datos_sf, \n         datos_sf$Rinde < LS & \n           datos_sf$Rinde > LI)\noutliers <-\n  subset(datos_sf, \n         datos_sf$Rinde > LS | \n           datos_sf$Rinde < LI)\n\nPara ver el impacto de la eliminación de outliers pueden obtenerse nuevamente las medidas resumen, coeficiente de asimetría, histograma y box-plot.\n\nsummary(datos_1$Rinde)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.348   3.691   4.659   4.523   5.445   8.734 \n\nskewness(datos_1$Rinde)\n\n[1] -0.5357052\n\n\n\nhist(\n  datos_1$Rinde,\n  col = 'grey',\n  nclass = 20,\n  main = \"Histograma\",\n  ylab = 'Frecuencia Relativa',\n  xlab = 'Rinde (t/ha)'\n)\nboxplot(\n  datos_1$Rinde,\n  col = 'grey',\n  ylab = 'Rinde (t/ha)',\n  main = \"Box-Plot\",\n  ylim = c(0, 14)\n)\n\n\n\n\n\n\n\n\n\n\n\nLas medidas resumen muestran un cambio principalmente a nivel de los valores máximos, pasando de 24 a 8,734 \\(t\\ ha^{-1}\\). El coeficiente de asimetría presenta un valor de -0,54 que se aproxima a valores recomendados para el análisis geoestadístico (-1 a 1). En la figura anterior se presenta el histograma y box-plot luego de la eliminación de los outliers. Para la variable en análisis, se eliminaron durante la depuración 120 casos que representan un 1,2% del total de sitios (n=9810) con mediciones. Puede observarse una mejora en la simetría de la distribución de la variable y una marcada disminución de valores extremos.\nLa identificación y eliminación de inliers requiere de la creación de una matriz de ponderación espacial. La función dnearneigh() se utiliza para identificar el vecindario de cada sitio. Para ello es necesario calcular la distancia espacial entre los puntos para lo cual se usa la sintaxis $geom que permite acceder a las coordenadas del objeto datos. En este ejemplo, se consideran datos vecinos a aquellos que se encuentran a una distancia Euclídea de 0 a 15 m. La función nb2listw() transforma el objeto vecindarios que contiene las distancias a una matriz de pesos estandarizados por filas (style = \"W\"). Este objeto es denominado pesos_sp. Para generar la matriz de pesos espaciales es necesario que todos los puntos tengan al menos un vecino, caso contario la función nb2listw() genera un error advirtiendo este hecho. Para superar el inconveniente es posible probar con distancias mayores hasta lograr que todos los puntos tengan al menos un vecino. Hay que tener la precaución de no generar un excesivo solapamiento entre los vecindarios. Otra opción es incorporar el argumento zero.policy=T dentro de la función nb2listw() que permite generar la matriz de pesos espaciales con observaciones que no presentan dato/s vecino/s. El mismo argumento debe agregarse luego cuando se calcula el índice de Moran local o gráfico de Moran (funciones localmoran() y moran.plot(), respectivamente). Las frecuencias del número de puntos vecinos para cada observación puede obtenerse mediante la función summary() del objeto vecindarios. En el ejemplo se observa que 3113 puntos tienen vecindarios conformados con 4 datos. Mientras que sólo un dato presenta 18 observaciones consideradas como vecinas.\n\nvecindarios <- dnearneigh(datos_1$geom,\n                          d1 = 0, d2 = 15)\nsummary(vecindarios)\n\nNeighbour list object:\nNumber of regions: 9690 \nNumber of nonzero links: 62578 \nPercentage nonzero weights: 0.06664601 \nAverage number of links: 6.457998 \nLink number distribution:\n\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n   5   83  302 3113 1292  999  893  870  393  457  547  473  171   43   31   13 \n  17   18 \n   4    1 \n5 least connected regions:\n3033 7885 9153 9598 9681 with 1 link\n1 most connected region:\n869 with 18 links\n\n\n\npesos_sp <- nb2listw(vecindarios,\n                     style = \"W\")\n\nLa función localmoran() calcula el índice local de Moran que permite identificar potenciales outliers espaciales. También permite el ajuste de los valores-p por el criterio de Bonferroni. La información referida al valor del índice local de Moran de cada punto se encuentra en la columna Ii mientras que su significancia estadística en la columna Pr(z < 0).\n\nmoranl <-\n  localmoran(datos_1$Rinde,\n             pesos_sp,\n             alternative = \"less\")\nhead(moranl)\n\n        Ii         E.Ii    Var.Ii     Z.Ii Pr(z < E(Ii))\n1 5.319474 -0.001140633 0.9189659 5.550244     1.0000000\n2 6.825936 -0.001134085 1.2186399 6.184387     1.0000000\n3 5.830844 -0.001130275 0.9106300 6.111455     1.0000000\n4 3.360586 -0.001122130 1.8092727 2.499241     0.9937770\n5 3.909178 -0.001107545 1.5305121 3.160750     0.9992132\n6 4.207286 -0.001107545 1.0710265 4.066458     0.9999761\n\n\nEl gráfico de Moran permite la identificación de puntos influyentes. La función moran.plot() construye el gráfico y devuelve los estadísticos de diagnóstico para cada punto. En el eje horizontal se expresan los valores de la variable rendimiento mientras que en el vertical se representa el retardo espacial de la variable. Adicionalmente, se ajusta y añade a este diagrama modelos de regresión lineal y estadísticos de influencia para identificar sitios con datos raros. Un punto se determina como influyente si al menos uno de los estadísticos así lo considera. En la figura siguiente los puntos negros con forma romboidal fueron identificados como influyentes y se los considera como inliers.\n\nmoranp <-\n  moran.plot(\n    datos_1$Rinde,\n    col = 3,\n    pesos_sp,\n    labels = F,\n    quiet = T,\n    xlab = \"Rinde\",\n    ylab = \"Rinde Spatially Lagged\"\n  )\n\n\n\n\nPara visualizar en una tabla los puntos potencialmente influyentes y sus estadísticos de diagnóstico puede imprimir el objeto moranp. Datos con * en la columna inf se los considera como influyente y por lo tanto posible outlier espacial.\n\nsummary(moranp)\n\nDesde el objeto moranp puede extraerse una matriz de valores lógicos (verdadero/falso) para los estadísticos diagnóstico que identifican un punto como influyente o no.\n\ninflu <- moranp$is_inf\nhead(influ)\n\n[1] TRUE TRUE TRUE TRUE TRUE TRUE\n\n\nEn la siguiente sentencia se adiciona al objeto datos_1 los valores de los objetos moranl e influ, que tienen información para detectar los outliers espaciales detectados con el índice de Moran local y gráfico de Moran, respectivamente.\n\ndatos_1 <- cbind(datos_1, moranl, influ)\n\nPosteriormente procedemos a eliminar los datos con Índice de Moran Local negativo y estadísticamente significativos (p<0,05). La función subset() selecciona datos que cumplen con cierta condición lógica. El operador lógico or que indica que extraiga los datos que cumplen con alguna de las dos condiciones. El nuevo objeto es denominado datos_2. Además, se crea un nuevo objeto que tendrá los datos que han sido eliminados en este proceso (inliers_ml).\n\ndatos_2 <-\n  subset(datos_1, \n         datos_1[[\"Ii\"]] >= 0 | \n           datos_1[[\"Pr.z...E.Ii..\"]] > 0.05)\ninliers_ml <-\n  subset(datos_1, \n         datos_1[[\"Ii\"]] < 0 &\n           datos_1[[\"Pr.z...E.Ii..\"]] < 0.05)\n\nExisten varias formas de eliminar las filas de la tabla que fueron identificadas como inliers con la función moran.plot(). Una alternativa es usando sentencias lógicas con los operadores == y & que significan igualdad lógica y and respectivamente. Como en el caso anterior se genera una nueva base (datos_3) la cual no tendrá los datos considerados como outliers y outliers espaciales. También se genera una nueva base que tendrá solo los datos considerados aquí como outliers espaciales (inliers_mp).\n\ndatos_3 <-\n  datos_2[!datos_2$influ, ]\n\nLa sentencia anterior instruye al software para que cree un objeto llamado datos_3 a partir de las filas del objeto datos_2 cuyas columna influ es igual a TRUE.\n\ninliers_mp <-\n  datos_2[datos_2$influ, ]\n\nLuego de identificar y eliminar los inliers detectados con el índice de Moran y posteriormente con el gráfico de Moran, la nueva base de datos presenta 9009 casos, es decir, se eliminaron 681 casos (7% de los datos) respecto a la base sin outliers.\nLos estadísticos descriptivos de los datos depurados muestran una mejora en el coeficiente de asimetría (-0,19) lo cual se refleja en el histograma y box-plot. Este último también muestra la ausencia de valores extremos.\n\nsummary(datos_3$Rinde)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.732   3.795   4.717   4.627   5.470   7.322 \n\nskewness(datos_3$Rinde)\n\n[1] -0.1952454\n\n\n\npar(mfrow = c(1, 2))\nhist(\n  datos_3$Rinde,\n  col = 'grey',\n  nclass = 20,\n  main = \"Histograma\",\n  ylab = 'Frecuencia Relativa',\n  xlab = 'Rendimiento (t/ha)'\n)\nboxplot(\n  datos_3$Rinde,\n  col = 'grey',\n  ylab = 'Rendimiento (t/ha)',\n  main = \"Box-Plot\",\n  ylim = c(0, 14)\n)\n\n\n\n\nLas siguientes líneas muestran la visualización conjunta de los datos originales y aquellos detectados como outliers y outliers espaciales. En este último se diferencian los detectados por el índice de Moran local (Inliers ML) respecto a los identificados por el gráfico de Moran (Inliers MP).\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(datos_3) +\n  tm_dots(\"Rinde\", title=\"Rendimiento (t/ha)\") +\n  tm_shape(outliers) +\n  tm_dots(col = \"red\", size = 0.1) +\n  tm_shape(inliers_ml) +\n  tm_dots(col = \"blue\", size = 0.1) +\n  tm_shape(inliers_mp) +\n  tm_dots(\"cyan\", size = 0.1) +\n  tm_add_legend(\"symbol\", \n                col = c(\"red\", \"blue\", \"cyan\"), \n                labels = c(\"Outliers\", \n                           \"Inliers ML\", \n                           \"Inliers MP\"))\n\n\n\n\nFinalmente las líneas siguientes permiten la exportación de los datos depurados en diferentes formatos (.csv, .shp, .gpkg). para ello se utiliza la función st_write(). Previo a ello se selecciona solo la variable Rinde del objeto datos_3.\n\ndatos_3 <- datos_3[, c(\"Rinde\")]\nst_write(datos_3,\n         \"base_depurada.csv\",\n         layer_options = \"GEOMETRY=AS_XY\",\n         delete_layer = T)\nst_write(datos_3, \n         \"base_depurada.shp\", \n         delete_layer = T)\nst_write(datos_3, \n         \"base_depurada.gpkg\", \n         delete_layer = T)"
  },
  {
    "objectID": "Parte2_cap_a_EscFinaImplR.html#detección-de-tendencias-espaciales",
    "href": "Parte2_cap_a_EscFinaImplR.html#detección-de-tendencias-espaciales",
    "title": "\n5  Implementación con R\n",
    "section": "\n5.3 Detección de tendencias espaciales",
    "text": "5.3 Detección de tendencias espaciales\nPara evaluar las tendencia de la media del rendimiento con las coordenadas geográficas primero se incorpora al data.frame del objeto sf las coordenadas. De esta forma el objeto datos_3 presenta la variable Rinde, su geometría y las coordenadas x e y.\n\ndatos_3$x <- st_coordinates(datos_3)[,1]\ndatos_3$y <- st_coordinates(datos_3)[,2]\nhead(datos_3)\n\nSimple feature collection with 6 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 311986.7 ymin: 5800721 xmax: 313167.5 ymax: 5800973\nProjected CRS: WGS 84 / UTM zone 21S\n    Rinde                 geometry        x       y\n256 1.732 POINT (311986.7 5800811) 311986.7 5800811\n257 1.734 POINT (313167.5 5800905) 313167.5 5800905\n263 1.784 POINT (312510.5 5800721) 312510.5 5800721\n270 1.804 POINT (312630.6 5800837) 312630.6 5800837\n [ reached 'max' / getOption(\"max.print\") -- omitted 2 rows ]\n\n\nPara visualizar tendencias espaciales graficamos la variable en estudio en función de las coordenadas. Si se desea desplegar los gráficos para la coordenada x e y en una misma ventana gráfica, se puede particionar la ventana en una fila y dos columnas utilizando la siguiente función:\n\npar(mfrow=c(1,2))\n\nLa función plot() permite realizar gráficos de dispersión. Además, puede adicionarse una línea de suavizado lowess con la función lines(). Esta última, realiza el ajuste sobre una ventana gráfica preexistente.\n\npar(mfrow = c(1, 2))\n\nwith(datos_3, {\n  plot(Rinde ~ x)\n  lines(lowess(Rinde ~ x), \n        col = \"red\", lwd = 3)\n\n})\nwith(datos_3, {\n  plot(Rinde ~ y)\n  lines(lowess(Rinde ~ y), \n        col = \"red\", lwd = 3) \n})\n\n\n\n\nMediante un modelo lineal de regresión, puede ajustarse la tendencia entre la variable en estudio y las coordenadas. Si la tendencia lineal resulta significativa, debería descontarse trabajando con los residuos del modelo, que se obtienen con la función residuals().\n\nregresion <- lm(formula = Rinde ~ 1 + x + y ,\n                data = datos_3,\n                na.action = na.omit)\n\nLa siguiente línea despliega una tabla resumen del modelo:\n\nsummary(regresion)\n\n\nCall:\nlm(formula = Rinde ~ 1 + x + y, data = datos_3, na.action = na.omit)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.1684 -0.8178  0.1033  0.8240  2.7396 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.856e+03  2.358e+02   7.872 3.89e-15 ***\nx           -5.632e-04  4.141e-05 -13.601  < 2e-16 ***\ny           -2.889e-04  4.130e-05  -6.995 2.85e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.069 on 8935 degrees of freedom\nMultiple R-squared:  0.03525,   Adjusted R-squared:  0.03503 \nF-statistic: 163.2 on 2 and 8935 DF,  p-value: < 2.2e-16\n\n\nEn este caso, los gráficos exploratorios no marcan una tendencia marcada con las coordenadas. Aún, cuando los valores-p del modelo de regresión son significativos (p<0,05), se decidió trabajar con la variable original debido a que el coeficiente de determinación del modelo acusa un ajuste pobre (0,035)."
  },
  {
    "objectID": "Parte2_cap_a_EscFinaImplR.html#cálculo-del-índice-de-moran",
    "href": "Parte2_cap_a_EscFinaImplR.html#cálculo-del-índice-de-moran",
    "title": "\n5  Implementación con R\n",
    "section": "\n5.4 Cálculo del índice de Moran",
    "text": "5.4 Cálculo del índice de Moran\nPara la conformación de la matriz de ponderadores espaciales se definieron los vecindarios de cada sitio mediante una red de conexión construida en base a la distancia euclídea. Se consideraron sitios vecinos a aquellos contiguos ubicados hasta 15 m de distancia. El procedimiento es similar al empleado para el cálculo de índice de Moran local. En este caso se agrega el argumento zero.policy=T dentro de la función nb2listw() y moran.mc(). Esto permite que se genera la matriz de pesos espaciales sin la restricción de que todos los puntos tengan al menos un dato vecino.\n\nvecindarios <- dnearneigh(datos_3$geom, \n                          0, 15)\npesos_sp <- nb2listw(vecindarios, \n                     style = \"W\", \n                     zero.policy = TRUE)\n\nPara realizar el cálculo del Índice de Moran y determinar su significancia estadística mediante simulación Monte Carlo, se utiliza moran.mc(). Se debe especificar la variable en estudio, la lista con los pesos de las ponderaciones espaciales y el número de simulaciones.\n\ni.moran <-\n  moran.mc(\n    datos_3$Rinde,\n    listw = pesos_sp,\n    nsim = 999,\n    zero.policy = T\n  )\ni.moran\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  datos_3$Rinde \nweights: pesos_sp  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.78762, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\nEstos resultados permiten concluir que existe autocorrelación espacial positiva (0,78196) y que esta es estadísticamente significativa (p=0,001)."
  },
  {
    "objectID": "Parte2_cap_a_EscFinaImplR.html#análisis-basado-en-semivariogramas",
    "href": "Parte2_cap_a_EscFinaImplR.html#análisis-basado-en-semivariogramas",
    "title": "\n5  Implementación con R\n",
    "section": "\n5.5 Análisis basado en semivariogramas",
    "text": "5.5 Análisis basado en semivariogramas\nLas semivariogramas empíricos pueden obtenerse usando la función variogram() del paquete gstat. Esta tiene múltiples argumentos, entre ellos una fórmula, un objeto de datos espaciales y la distancia hasta la cual los pares de puntos son incluidos en la estimación de semivarianza (cutoff). Dado que el objeto a tratar (datos_3) es de clase sf, no es necesario realizar su transformación a un objeto del tipo espacial. Utilizando la función plot() se visualiza el semivariograma empírico ajustado.\n\nsemiv_emp <- variogram(Rinde ~ 1, \n                       datos_3, \n                       cutoff = 400)\nhead(semiv_emp)\n\n      np     dist     gamma dir.hor dir.ver   id\n1  95967 17.97116 0.4070578       0       0 var1\n2 277818 41.40727 0.6439701       0       0 var1\n [ reached 'max' / getOption(\"max.print\") -- omitted 4 rows ]\n\n\n\nplot(semiv_emp,\n     main = \"Rendimiento (t/ha)\",\n     xlab = \"Distancia\",\n     ylab = \"Semivarianza\")\n\n\n\n\nEn el caso anterior la fórmula utilizada (Rinde~1) asume que el proceso es estacionario. Si se requiere adicionar una tendencia sea reemplaza el caracter 1 por el nombre de la covariable. Por ejemplo, si existe una tendencia dada por las coordenadas la fórmula se escribe como Rinde~x+y. Es posible solo colocar una de las coordenadas o incluir también términos polinómicos para las mismas. También se pueden usar otras covariables distintas a las coordenadas.\nPor defecto para el cálculo del semivariograma empírico la función variogram() utiliza el estimador de los momentos de Matheron. Para emplear el estimador robusto de Cressie-Hawkins se adiciona en la función el argumento cresice=TRUE. Por defecto la función emplea para el cálculo del semivariograma un tercio de la diagonal del “cuadro” que contiene las observaciones. Con el argumento cutoff se puede cambiar esta distancia. El argumento width permite cambiar el ancho de los intervalos de distancia en los que se agrupan los pares de puntos de datos para las estimaciones de semivarianza. Por defecto se calcula como cutoff/15. El argumento alpha permite el cálculo de los semivariogramas en distintas direcciones en el plano (x, y), tomando valores en grados positivos en sentido horario desde y (Norte). Para alpha = 0 la dirección es Norte y para alpha = 90 la dirección es Este. Esto es útil para evaluar la presencia de anisotropía. Usualmente se suelen calcular los semivariogramas direccionales para \\(45^\\circ\\), \\(90^\\circ\\), \\(135^\\circ\\) y \\(180^\\circ\\). Otras opciones pueden encontrarse mediante la función help().\nA continuación se ajusta un modelo de semivariograma teórico sobre el semivariograma empírico usando las funciones fit.variogram() yvgm(). Esta última ajusta el modelo teórico, sus argumentos indican el tipo de modelo a ajustar y los parámetros de ajuste (partial sill, rango y efecto nugget). Estos parámetros iniciales son de referencia y pueden obtenerse a partir del semivariograma empírico. Cambiar los parámetros modifica la suma de cuadrados del error (SCE).\nSe ajusta un modelo esférico, con valores 0,6, 200 y 0,2 como parámetros iniciales para estimar el sill parcial, rango y nugget, respectivamente. La salida del software R muestra los parámetros del semivariograma teórico ajustado: nugget (\\(C_0=0.31\\)), sill parcial (\\(C=0.72\\)) y rango (154 m). Nota: bajo la columna psill, para la fila Nug, se debe leer el valor \\(C_0\\).\n\nmod_esf <- fit.variogram(\n  semiv_emp,\n  vgm(0.6, \"Sph\", 200, 0.2))\nmod_esf\n\n  model     psill    range\n1   Nug 0.3021528   0.0000\n2   Sph 0.7317432 153.8709\n\n\nEl semivariograma empírico (puntos) y teórico ajustado(linea), para un modelo esférico, puede graficarse de la siguiente manera:\n\nplot(semiv_emp,\n     mod_esf,\n     main = \"Rendimiento (t/ha)\",\n     xlab = \"Distancia\",\n     ylab = \"Semivarianza\")\n\n\n\n\nEl modelo que mejor ajusta será el de menor SCE. La función attr() devuelve atributos de un objeto y puede usarse para consultar la SCE del modelo ajustado.\n\nattr(mod_esf, 'SSErr')\n\n[1] 1.745018\n\n\nRepitiendo el procedimiento para un modelo exponencial (especificado en la función vgm()) se obtiene un menor SCE indicando mejor ajuste. Abajo se presenta la salida del software R que contiene los parámetros del semivariograma teórico ajustado: nugget (\\(C_0=0.21\\)), sill parcial (\\(C=0.86\\)) y rango (64,88 m) o Rango Practico (\\(Rp=64.88 m \\times 3\\)). Nota: bajo la columna “psill”, para la fila Nugget, se debe leer el valor \\(C_0\\).\n\nmod_exp <- fit.variogram(\n  semiv_emp, \n  vgm(0.6, \"Exp\", 200, 0.2))\nmod_exp\n\n  model     psill    range\n1   Nug 0.2030498  0.00000\n2   Exp 0.8757545 64.67865\n\n\n\nattr(mod_exp, 'SSErr')\n\n[1] 0.3889793\n\n\n\nplot(semiv_emp,\n     mod_exp,\n     main = \"Rendimiento (t/ha)\",\n     xlab = \"Distancia\",\n     ylab = \"Semivarianza\")\n\n\n\n\nLas siguientes líneas permiten la visualización conjunta de los dos ajustes realizados.\n\nvgLine <- rbind(\n  cbind(\n    variogramLine(\n      mod_exp, maxdist = max(semiv_emp$dist)),\n    id = \"Exponencial\"),\n  cbind(\n    variogramLine(\n      mod_esf, maxdist = max(semiv_emp$dist)),\n    id = \"Esférico\")\n  )\n\n\nggplot(semiv_emp, aes(x = dist, y = gamma, \n                      color = id)) +\n  geom_line(data = vgLine) +\n  geom_point() +\n  labs(\n    title = \"Semivariograma experimental\n    y teórico ajustado\") +\n  xlab(\"Distancia\") +\n  ylab(\"Semivarianza\") +\n  scale_color_discrete(\n    name = \"Modelo\",\n    labels = c(\"Esférico\", \n               \"Exponencial\", \n               \"Experimental\"))\n\n\n\n\nAlgunas alternativas para aplicar las funciones de ajuste de los semivariogramas teóricos incluyen opciones de ajuste automático donde el usuario sólo especifica los modelos a ajustar sin tener que dar valores iniciales de los parámetros del semivariograma. La función estima valores iniciales razonables y selecciona aquel modelo de mejor bondad de ajuste en función a la SCE. A continuación, se presenta este ejemplo ajustando los modelos exponencial y esférico. Como era de esperar, el modelo de mejor ajuste fue el esférico.\n\nmodelos <- fit.variogram(semiv_emp, \n                         vgm(c(\"Exp\", \"Sph\")))\nmodelos\n\n  model     psill    range\n1   Nug 0.2030678  0.00000\n2   Exp 0.8757497 64.68324\n\n\n\nattr(modelos, 'SSErr')\n\n[1] 0.3889793\n\n\nSi bien, como se mostró en el análisis exploratorio de los datos no se evidencia una tendencia en la variable rendimiento con las coordenadas, en las siguientes líneas se ilustra cómo se realiza el ajuste del semivariograma empírico con tendencia dada por las coordenadas (x e y) y el posterior ajuste del modelo teórico. Los resultados muestran que no se observan diferencias importantes en los parámetros estimados del semivariograma teórico. En casos donde la tendencia es importante su efecto se puede reflejar en el ajuste del semivariograma empírico el cual suele mostrar un incremento de la semivarianza a medida que aumenta la distancia que no alcanza a estabilizarse dentro del dominio bajo estudio.\n\nsemiv_emp_t <- variogram(Rinde ~ x + y, \n                         datos_3, cutoff = 400)\nmodelos_t <- fit.variogram(semiv_emp_t, \n                           vgm(c(\"Exp\", \"Sph\")))\nmodelos_t\n\n  model     psill    range\n1   Nug 0.1997084  0.00000\n2   Exp 0.8741126 63.61738\n\nattr(modelos_t, 'SSErr')\n\n[1] 0.357681\n\nplot(semiv_emp_t , modelos_t)\n\n\n\n\n\n5.5.1 Mapeo de la variabilidad espacial\nPara el mapeo de la variabilidad espacial se confeccionará una grilla de predicción donde se realiza el kriging. Las dimensiones de la grilla se establecerá mediante un polígono de los límites del lote. El archivo limites.txt contiene los puntos georreferenciados de cada arista del polígono.\n\nlimites <- read.table(\"datos/limites.txt\", \n                      header = TRUE)\nhead(limites)\n\n         x       y\n1 311841.9 5800614\n2 311950.3 5800728\n3 311998.0 5800788\n4 312005.5 5800835\n5 312566.0 5801431\n6 312589.9 5801435\n\n\nLa función pred_grid() del paquete geoR genera una grilla regular de puntos de 10 metros de distancia entre estos. La función polygrid() recorta el polígono en la grilla siguiendo los límites del lote. Posteriormente se definen los nombres de las coordenadas y se transforma la clase del objeto a sf asignando el sistema de coordenadas.\n\ngrid <- pred_grid(limites, by = 10)\ngrid <- polygrid(grid, bor = limites)\n\nnames(grid) <- c(\"x\", \"y\")\ngrid <- st_as_sf(grid, coords = c(\"x\", \"y\"),\n                 crs = 32721)\nplot(grid)\n\n\n\n\nLa función krige() del paquete gstat realiza interpolación kriging y simulaciones condicionales mediante diferentes métodos de predicción. En este caso, se presenta la interpolación por kriging ordinario con el modelo de semivariograma exponencial estimado con geoestadística clásica. Los argumentos de esta función incluyen, la fórmula en la cual se especifica que el proceso es estacionario (Rinde~1), la base de datos (datos_3), el objeto sobre el cual se hará la predicción (grid) y la información del modelo del modelo de semivariograma teórico ajustado (model) La información de este último se encuentra dentro del objeto modelos. Los argumentos nmin y nmax permiten realizar el proceso de interpolación en un contexto local, con un número mínimo y máximo de vecinos de cada punto a predecir de 7 y 25, respectivamente. En caso de omitir estos últimos argumentos la interpolación se realiza en un contexto global.\n\nkriging_o <-\n  krige(\n    Rinde ~ 1,\n    datos_3,\n    st_as_sf(grid),\n    nmin = 7,\n    nmax = 25,\n    model = modelos\n  ) \n\n[using ordinary kriging]\n\n\nA continuación, se realiza la visualización de la predicción y su varianza.\n\npredK_otm <- tm_shape(kriging_o) +\n  tm_dots(\"var1.pred\", style = \"cont\",\n          title = \"Predicción\")\nvarK_otm <-  tm_shape(kriging_o) +\n  tm_dots(\"var1.var\", style = \"cont\",\n          title = \"Varianza\")\n\ntmap_arrange(predK_otm, varK_otm)\n\n\n\n\nLa siguiente línea de comando permite realizar la predicción kriging en bloques en un contexto local para lo cual solo se adiciona en la función krige() el argumento block. En este ejemplo se definió la dimensión del bloque de \\(40 \\times 40\\) m. Posteriormente se realiza la visualización de forma similar al caso anterior.\n\nkriging_b <-\n  krige(\n    Rinde ~ 1,\n    datos_3,\n    grid ,\n    nmin = 7,\n    nmax = 25,\n    model = modelos,\n    block = c(40, 40)\n  )\n\n[using ordinary kriging]\n\n\n\npredK_btm <- tm_shape(kriging_b) +\n  tm_dots(\"var1.pred\", style = \"cont\",\n          title = \"Predicción Bloque\")\nvarK_btm <-  tm_shape(kriging_b) +\n  tm_dots(\"var1.var\", style = \"cont\",\n          title = \"Varianza Bloque\")\n\ntmap_arrange(predK_btm, varK_btm)\n\n\n\n\nLas siguientes líneas de código muestran cómo obtener un data.frame conteniendo las predicciones realizadas sobre la grilla. En primer lugar, se extraen las coordenadas del objeto kriging_b y luego se transforma la clase del objeto a data.frame.\n\nkriging_b$x <- st_coordinates(kriging_b)[, 1]\nkriging_b$y <- st_coordinates(kriging_b)[, 2]\npredRinde <- \n  data.frame(kriging_b)[, c(\"x\", \"y\", \n                            \"var1.pred\")]\nnames(predRinde)[3] <- paste(\"Tg\")\n\nEn el caso que exista una tendencia dada por las coordenadas, se utiliza kriging universal como método de interpolación espacial. Los comandos son similares a los anteriores, sólo debe cambiarse la fórmula. Por ejemplo, para una tendencia de primer grado en las coordenadas espaciales se escribiría Rinde~x+y. El modelo de semivariograma que se utiliza debe haberse ajustando contemplado el mismo tipo de tendencia. En situaciones donde otras covariables explican la tendencia en la media, éstas deben especificarse en la fórmula. Para poder hacer la interpolación es necesario que los valores de las covariables también estén disponibles en la grilla de predicción. En este último caso la interpolación se denomina kriging con deriva externa.\nLa interpolación también puede hacerse utilizando los parámetros partial sill, rango y nugget del semivariograma estimado con REML. Para el ajuste de un MLM con errores correlacionados espacialmente vía REML, la base de datos no debe ser muy grande. Para poder realizar el ajuste de un MLM en el conjunto de datos de ilustración, se tomó una muestra aleatoria de n=500 sobre la base de datos original de n=9009, usando la función sample(). Las siguientes líneas de código muestran el ajuste del semivariograma y obtención de los parámetros del mismo usando la función gls() del paquete nlme. Se realiza el ajuste de dos modelos lineales, el primero sin correlación espacial (null_model) y el segundo con una estructura de correlación espacial del tipo exponencial (esp_model).\n\nset.seed(7)\ndatos_4 <- datos_3[\n  sample(1:nrow(datos_3), \n         500, replace = FALSE), ]\nnull_model <-\n  gls(Rinde ~ 1, datos_4, \n      method = \"REML\", \n      na.action = na.omit)\n\nesp_model <- gls(\n  Rinde ~ 1,\n  data = datos_4,\n  correlation = corExp(\n    form =  ~ x + y,\n    metric = \"euclidean\",\n    nugget = T\n  ),\n  method = \"REML\",\n  na.action = na.omit\n)\n\nA continuación, se realiza la comparación de ambos modelos mediante la prueba del cociente de verosimilitud. Los resultados muestran que hay diferencias significativas (p<0.005) entre ambos modelos, con lo cual se elige el modelo espacial. Los valores de AIC y BIC también mostraban un mejor ajuste de este último.\n\nanova(null_model, esp_model)\n\n           Model df      AIC      BIC    logLik   Test  L.Ratio p-value\nnull_model     1  2 1530.667 1539.092 -763.3336                        \nesp_model      2  4 1262.945 1279.796 -627.4727 1 vs 2 271.7218  <.0001\n\n\nCon el modelo ajustado se obtienen los parámetros del semivariograma y se construye el objeto m que contendrá estos para su posterior interpolación vía kriging.\n\nsummary(esp_model)\n\nGeneralized least squares fit by REML\n  Model: Rinde ~ 1 \n  Data: datos_4 \n       AIC      BIC    logLik\n  1262.945 1279.796 -627.4727\n\nCorrelation Structure: Exponential spatial correlation\n Formula: ~x + y \n Parameter estimate(s):\n      range      nugget \n137.5638148   0.1602904 \n\nCoefficients:\n              Value Std.Error  t-value p-value\n(Intercept) 4.14356 0.3088076 13.41793       0\n\nStandardized residuals:\n       Min         Q1        Med         Q3        Max \n-1.6544880 -0.2862945  0.4187832  1.0872181  2.4975704 \n\nResidual standard error: 1.269009 \nDegrees of freedom: 500 total; 499 residual\n\nesp_model$sigma\n\n[1] 1.269009\n\nnugget <- 0.1344285 * esp_model$sigma ^ 2\npsill <- esp_model$sigma ^ 2 - nugget\nrange <- 102.5470883\n\nm <- vgm(psill, \"Exp\", range, nugget)\nkriging_mlm <-\n  krige(\n    Rinde ~ 1,\n    datos_4,\n    grid ,\n    nmin = 7,\n    nmax = 25,\n    model = m\n  )\n\n[using ordinary kriging]\n\n\nEl mismo ajuste de semivariograma puede realizarse utilizando la función likfit() del paquete geoR. En este caso es necesario generar un objeto del tipo geodata. La función de ajuste solicita valores iniciales de sill parcial y rango y modelo teórico (para esta aplicación se usa el exponencial). En el caso del efecto nugget el mismo es estimado por defecto. Es posible también incorporar tendencias utilizando el argumento trend. Los resultados del modelo muestran valores estimados de los parámetros similares a los obtenidos con la función gls().\n\ndatos_geor <- as.data.frame(datos_4)\ndatos_geor <-\n  as.geodata(datos_geor,\n             coords.col = c(\"x\", \"y\"),\n             data.col = \"Rinde\")\n\nmlm_geor <-\n  likfit(\n    datos_geor,\n    ini = c(0.7, 90),\n    cov.model = \"exponential\",\n    lik.method = \"REML\",\n    messages = FALSE\n  )\nsummary(mlm_geor)\n\nSummary of the parameter estimation\n-----------------------------------\nEstimation method: restricted maximum likelihood \n\nParameters of the mean component (trend):\n  beta \n4.1436 \n\nParameters of the spatial component:\n   correlation function: exponential\n      (estimated) variance parameter sigmasq (partial sill) =  1.352\n      (estimated) cor. fct. parameter phi (range parameter)  =  137.6\n   anisotropy parameters:\n      (fixed) anisotropy angle = 0  ( 0 degrees )\n      (fixed) anisotropy ratio = 1\n\nParameter of the error component:\n      (estimated) nugget =  0.2581\n\nTransformation parameter:\n      (fixed) Box-Cox parameter = 1 (no transformation)\n\nPractical Range with cor=0.05 for asymptotic range: 412.0798\n\nMaximised Likelihood:\n   log.L n.params      AIC      BIC \n\"-624.4\"      \"4\"   \"1257\"   \"1274\" \n\nnon spatial model:\n   log.L n.params      AIC      BIC \n\"-760.2\"      \"2\"   \"1524\"   \"1533\" \n\nCall:\nlikfit(geodata = datos_geor, ini.cov.pars = c(0.7, 90), cov.model = \"exponential\", \n    lik.method = \"REML\", messages = FALSE)\n\n\n\n5.5.2 Validación cruzada\nA continuación, se ilustra el proceso de validación cruzada k-fold. En este caso la función utilizada es krige.cv() del paquete gstat. Aquí como en el ajuste del semivariogram empírico y la interpolación, se tienen los argumentos fórmula (Rinde~1, lo cual especifica que es un proceso estacionario), la base de datos (datos_3) y el modelo de semivariograma teórico ajustado (modelos). Aquí también se realiza la validación usando kriging en un contexto local por lo que se colocan los argumentos nmin y nmax. El argumento nfold determina el número de grupos (k) en los que se divide la base de datos para realizar la validación cruzada k-fold. Para obtener repetibilidad en los resultados se sugiere fijar la semilla mediante la función set.seed().\n\nset.seed(17)\nvalcru <-\n  krige.cv(\n    Rinde ~ 1,\n    datos_3,\n    modelos,\n    nfold = 10,\n    nmin = 7,\n    nmax = 25\n  )\n\nRealizada la validación es posible calcular estadísticos resumen como el error medio (ME), error cuadrático medio (MSE), media del cociente de la desviación cuadrática (mean squared deviation ratio, MSDR), raíz del error cuadrático medio (RMSE), la RMSE relativa a la media de los observados (RMSE_rel) y la correlación lineal entre los observados vs. Predichos. Un gráfico de estos últimos se muestra al final.\n\nME <- mean(valcru$residual)\nMSE <- mean(valcru$residual ^ 2)\nMSDR <- mean(valcru$zscore ^ 2)\nRMSE <- sqrt(mean(valcru$residual ^ 2))\nRMSE_rel <- \n  sqrt(mean(valcru$residual ^ 2)) / \n  mean(valcru$observed) * 100\nr <- cor(valcru$observed, \n         valcru$observed - valcru$residual)\n\ntabla <- data.frame(ME, MSE, RMSE, \n                    RMSE_rel, MSDR, r)\ntabla\n\n          ME       MSE      RMSE RMSE_rel      MSDR         r\n1 0.00034965 0.2261363 0.4755379  10.2783 0.6696371 0.9010834\n\nplot(\n  valcru$observed,\n  valcru$observed - valcru$residual,\n  xlab = \"Observados\",\n  ylab = \"Predichos\"\n)"
  },
  {
    "objectID": "Parte2_cap_a_EscFinaImplR.html#caracterización-de-variabilidad-espacial-con-múltiples-capas-de-datos",
    "href": "Parte2_cap_a_EscFinaImplR.html#caracterización-de-variabilidad-espacial-con-múltiples-capas-de-datos",
    "title": "\n5  Implementación con R\n",
    "section": "\n5.6 Caracterización de variabilidad espacial con múltiples capas de datos",
    "text": "5.6 Caracterización de variabilidad espacial con múltiples capas de datos\n\n5.6.1 Análisis de componentes principales\nPara implementar el análisis multivariado es necesario contar con información de cada variable en los mismos sitios georreferenciados. En esta sección se usa la base de datos Pred2.txt que contiene mediciones de conductividad eléctrica aparente en dos profundidades 0-30 cm (CE30) y 0-90 cm (CE90), elevación (Elev), profundidad de suelo (Pe) y rendimiento de trigo (Tg). Para generar esta base debido a las diferentes resoluciones espaciales de las variables medidas, se calculó una zona buffer de 15 m de radio para la variable Pe y sobre cada punto buffer se calculó la mediana de las restantes variables. La matriz de datos resultante quedó conformada por n=482 sitios (filas) y p=7 variables (columnas).\nPara realizar el Análisis de Componentes Principales espacial (MULTISPATI-PCA) se utiliza los paquetes ade4 y adespatial. Primero se necesita calcular la matriz de ponderación espacial en forma similar a la realizada para el cálculo del índice de Moran. Luego, se realiza un Análisis de Componentes Principales (PCA) clásico y posteriormente sobre las componentes generadas por PCA, se aplica el MULTISPATI-PCA.\nCarga de base de datos multivariada.\n\npred <- read.table(\"datos/Pred2.txt\", \n                   header = TRUE)\nhead(pred)\n\n         x       y Pe     Elev CE30 CE90     Tg\n1 312283.5 5800205 80 160.8836 21.8 30.6 4.1925\n2 312256.5 5800229 40 160.9239 30.3 17.9 4.0030\n [ reached 'max' / getOption(\"max.print\") -- omitted 4 rows ]\n\n\nLa función dudi.pca() del paquete ade4, permite realizar un PCA sobre objetos de clase data.frame. Sus argumentos indican, las variables con las que se realizará el PCA, un valor lógico (TRUE o FALSE) indicando si debe o no centrarse por la media (center) y normalizarse (scale), un valor lógico para la realización o no del gráfico (scannf) y la cantidad de ejes guardados (nf), que coincide con la cantidad de variables utilizadas en el análisis.\n\npca <-\n  dudi.pca(\n    pred[, 3:7],\n    center = TRUE,\n    scale = TRUE,\n    scannf = FALSE,\n    nf = 5\n  )\n\nPara transformar un PCA en un PCA espacial (MULTISPATI-PCA) se calcula la red de vecindarios y la matriz de ponderación espacial. La distancia máxima para definir los sitios vecinos de cada dato fue de 45 m. Además, se adiciona el argumento zero.policy=T para poder generar la matriz de pesos espaciales contemplando que algunos puntos no tengan datos vecinos. La función multispati() permite realizar el MUlTISPATI-PCA. Para ello es necesario colocar en la función el objeto que surge de realizar el ACP (pca) y la matriz de pesos espaciales (pesos_sp). El argumento nfposi hace referencia al número de ejes con autocorrelación positiva que es retenido en el análisis. También pueden guardarse ejes con autocorrelación negativa mediante el argumento nfnega. En general los ejes con autocorrelación negativa son aquellos de menor contribución a la variabilidad total.\n\ncord <- st_as_sf(pred[, 1:2], \n                 coords = c(\"x\", \"y\"),\n                 crs = 32721)\nvecindarios <- dnearneigh(cord, 0, 45)\npesos_sp <- nb2listw(vecindarios, \n                     style = \"W\", \n                     zero.policy = T)\n\npca_esp <-\n  adespatial::multispati(pca, pesos_sp, \n                         scannf = F, nfposi = 5)\n\nPara realizar un gráfico que muestre las correlaciones entre las variables se puede usar la función s.arrow(). En este gráfico de traza un vector para cada variable en el espacio definido por las componentes principales que se seleccionen. En este caso de estudio, la función utiliza la primera componente para graficar el eje horizontal y la segunda componente para el eje vertical. Para adicionar un gráfico de barras con los autovalores puede usarse el argumento add.scatter.eig().\n\ns.arrow(pca_esp$c1,\n        xax = 1,\n        yax = 2,\n        clabel = 1)\nadd.scatter.eig(\n  pca_esp$eig,\n  xax = 1,\n  yax = 2,\n  posi = \"bottomright\",\n  ratio = 0.2\n)\n\n\n\n\nEl gráfico obtenido del MULTISPATI-PCA muestra que las variables Elev y Pe son las más importantes en la explicación de la variabilidad espacial a nivel del primer eje (sPC1, eje horizontal). Mientras que la CE30 y Tg presentan mayor importancia en la SPC2. Además, se observa una correlación positiva entre CE30 y CE90, y negativa entre estas dos y la Pe. También la Elev y Tg se correlacionan en forma negativa. El gráfico de autovalores (barras) sugiere dos estructuras principales a nivel de sPC1 y sPC2, siempre la sPC1 explica la mayor parte de la variabilidad de los datos seguida por sPC2, sPC3, y así sucesivamente.\n\nsummary(pca_esp)\n\n\nMultivariate Spatial Analysis\nCall: adespatial::multispati(dudi = pca, listw = pesos_sp, scannf = F, \n    nfposi = 5)\n\nScores from the initial duality diagram:\n          var      cum     ratio     moran\nRS1 1.9361896 1.936190 0.3872379 0.6761627\nRS2 1.0780245 3.014214 0.6028428 0.4045002\nRS3 0.8781966 3.892411 0.7784821 0.2496834\nRS4 0.6276711 4.520082 0.9040163 0.5516981\n [ reached 'max' / getOption(\"max.print\") -- omitted 1 rows ]\n\nMultispati eigenvalues decomposition:\n           eig       var      moran\nCS1 1.42539027 1.8078454 0.78844701\nCS2 0.46844645 1.0952222 0.42771817\nCS3 0.37124309 0.7480682 0.49626907\nCS4 0.07901454 0.6887554 0.11472077\nCS5 0.04842263 0.6601089 0.07335553\n\n\nComo se establece en la literatura, MULTISPATI-PCA maximiza el producto entre la varianza espacial y la autocorrelación mientras que PCA maximiza la varianza total. Los resultados muestran que con MULTISPATIPCA se explica una menor proporción de la varianza acumulada en el primer eje, respecto de PCA (1,81 vs. 1,94). Las dos primeras CP del PCA explican 60% de la variabilidad total mientras que la CS1 y CS2 del MULTISPATI el 58%. No obstante, los valores del índice de Moran calculados para las tres primeras CPs sugieren que la estimación de autocorrelación aumentó cuando se usó MULTISPATIPCA respecto de la contenida en las CPs del PCA (0,79 vs. 0,68 para el eje 1, 0,43 vs. 0,40 para el eje 2, 0,50 vs. 0,25 para el eje 3). Este resultado permitiría una visualización mejor de la variabilidad espacial. Por el contrario, a nivel de las CPs 4 y 5 este comportamiento fue inverso.\n\n5.6.2 Análisis de conglomerados\nPara implementar este análisis también es necesario contar con información de cada variable en los mismos sitios georreferenciados. Otra forma de lograr esto es interpolar cada una de ellas con la misma grilla de predicción. Es decir, que cada punto de la grilla tendrá un dato para cada variable predicha. Para el siguiente caso de estudio se realizó el procedimiento de interpolación con mediciones de conductividad eléctrica aparente en dos profundidades 0-30 cm (CE30) y 0-90 cm (CE90), elevación (Elev), profundidad de suelo (Pe) y rendimiento de trigo (Tg) (archivo Pred.txt). Para cada variable se realizó un análisis exploratorio y la predicción espacial para el re-escalado usando una grilla común a todas ellas de \\(10 \\times 10\\) m. Una vez que se realiza el re-escalado de cada variable, se tiene un objeto para cada variable con igual número de filas y columnas que pueden unirse en un único objeto usando la función cbind(). Para predRinde se extraen las 3 primeras columnas correspondiente a las coordenadas y valores predichos, mientras que para las restantes sólo se extraen los valores predichos de cada variable (columna 3) considerando que, si se utilizó la misma grilla de predicción, las coordenadas de cada data.frame deberían ser las mismas. Se recomienda mantener clara la nomenclatura de cada variable, teniendo en cuenta que el software es case-sensitive (sensible a mayúsculas y minúsculas). A tal efecto, se renombraron las columnas. A continuación, se muestras los códigos de R para hacer el procedimiento de concatenación, pero para la ejemplificación se carga y utiliza una base de datos que previamente fue concatenada.\n\npred <- read.table(\"datos/Pred.txt\", \n                   header = T)\n\nPosteriormente, se implementará el análisis de clúster espacial KM-sPC (Córdoba et al. 2013). Para ello primero se realiza un análisis de componentes principales espaciales (MULTISPATI-PCA) sobre las variables originales. Luego las variables sintéticas (componentes principales espaciales, sPC) son utilizadas como input del análisis de cluster fuzzy k-means.\n\npca <-\n  dudi.pca(\n    pred[, 3:7],\n    center = TRUE,\n    scale = TRUE,\n    scannf = FALSE,\n    nf = 5\n  )\n\ncord <- st_as_sf(pred[, 1:2], \n                 coords = c(\"x\", \"y\"),\n                 crs = 32721)\nvecindarios <- dnearneigh(cord, 0, 10)\npesos_sp <- nb2listw(vecindarios, \n                     style = \"W\", \n                     zero.policy = T)\n\npca_esp <-\n  adespatial::multispati(pca, pesos_sp, \n                         scannf = F, nfposi = 5)\n\nLa función multispati() almacena las sPC en la posición li dentro de los objetos creados. La siguiente sentencia crea un nuevo objeto con la unión de las columnas con las coordenadas y las sPC.\n\ncs <- pca_esp$li[, 1:5]\npred_am <- cbind(pred[, c(\"x\", \"y\")], cs)\nhead(pred_am)\n\n         x       y      CS1       CS2        CS3        CS4        CS5\n1 312432.8 5800234 1.924408 0.5429137 -0.1181671 -0.3210506 0.02568016\n2 312422.8 5800244 1.926060 0.4479901 -0.2154732 -0.2557639 0.17323020\n [ reached 'max' / getOption(\"max.print\") -- omitted 4 rows ]\n\n\nPara realizar el análisis de cluster fuzzy k-means se utiliza la función cmeans() del paquete e1071 (Meyer et al. 2019). Para ello se necesita determinar las sPC que se utilizarán como input. En este caso se seleccionaron las columnas que corresponden a la sPC1, sPC2 y sPC3, de esta forma una gran cantidad de la variabilidad total es contemplada (\\(\\ge 70 \\%\\)) en el análisis. En este ejemplo se utilizaron 2, 3 y 4 clústers. Otras opciones de configuración son el número de iteraciones=100; método=cmeans (opción para usar el algoritmo fuzzy) y exponente difuso m=1.3.\n\nclases2 <- cmeans(pred_am[, 3:5], 2, \n                  100, method = \"cmeans\", \n                  m = 1.3)\nclases3 <- cmeans(pred_am[, 3:5], 3, \n                  100, method = \"cmeans\", \n                  m = 1.3)\nclases4 <- cmeans(pred_am[, 3:5], 4, \n                  100, method = \"cmeans\", \n                  m = 1.3)\n\nEn el ejemplo de ilustración se debe seleccionar entre dos, tres y cuatro clases. Para ello se utilizaron los siguientes índices: Xie-Beni, coeficiente de partición, entropía de clasificación y Fukuyama-Sugeno. Estos índices serán calculados para 2, 3 y 4 clases o clúster, utilizando la función fclustIndex(). En todos los índices, excepto el coeficiente de partición, el número de clases óptimo se obtiene cuando los índices tienen el menor valor. Para hacer que la interpretación del coeficiente de partición sea igual a los otros índices, se utiliza el valor inverso del índice. Luego se confeccionó una tabla con los índices obtenidos.\n\nindices <-  c(\n  \"xie.beni\",\n  \"fukuyama.sugeno\",\n  \"partition.coefficient\",\n  \"partition.entropy\"\n)\n\n\n\nind_2clases <-\n  sapply(indices, function(indx) {\n  fclustIndex(clases2,\n              pred_am[, 3:5],\n              index = indx)\n})\n\nind_3clases <-\n    sapply(indices, function(indx) {\n  fclustIndex(clases3,\n              pred_am[, 3:5],\n              index = indx)\n})\n\nind_4clases <-\n    sapply(indices, function(indx) {\n  fclustIndex(clases4,\n              pred_am[, 3:5],\n              index = indx)\n})\n\nindices <- cbind(ind_2clases, \n                 ind_3clases, \n                 ind_4clases)\nindices\n\n                           ind_2clases   ind_3clases   ind_4clases\nxie.beni.xb               4.832907e-05  9.371751e-05  1.088882e-04\nfukuyama.sugeno.fs       -1.201712e+04 -1.317561e+04 -1.440020e+04\npartition.coefficient.pc  9.255349e-01  8.631698e-01  8.343445e-01\npartition.entropy.pe      1.257311e-01  2.428793e-01  3.019072e-01\n\n\nEn este ejemplo la mayoría de los índices, excepto Fukuyama-Sugeno, muestran que el número de clases a seleccionar es dos. Puede suceder que ninguno de los índices coincida con otro en el número óptimo de clases. Para facilitar la toma de decisiones se recomienda calcular un índice resumen para cada clasificación. Este nuevo índice puede ser la distancia Euclídea de los valores de los índices previamente normalizados por su valor máximo a través de las diferentes clasificaciones.\n\nXieBeniN <- indices[1,] / max(indices[1,])\nFukSugN <- indices[2,] / max(indices[2,])\nCoefPartN <- indices[3,] / max(indices[3,])\nEntrPartN <- indices[4,] / max(indices[4,])\n\nindicesN <-\n  data.frame(rbind(XieBeniN, FukSugN, \n                   CoefPartN, EntrPartN))\nindicesN <- (indicesN) ^ 2\n\nindices2N <- sqrt(sum(indicesN[, 1]))\nindices3N <- sqrt(sum(indicesN[, 2]))\nindices4N <- sqrt(sum(indicesN[, 3]))\n\nindices2N\n\n[1] 1.53962\n\nindices3N\n\n[1] 1.860062\n\nindices4N\n\n[1] 2.061212\n\n\nEl índice resumen se optimiza para la estructura conformada por dos clúster. Para realizar mapas de la clasificación, primero se debe extraer los datos de las clases delimitadas con el algoritmo fuzzy k-means y combinar con la base de datos inicial en la cual sólo se dejan las coordenadas. Posteriormente se transforma la base (base_am) a un objeto de clase sf y luego se grafican con el paquete tmap.\n\nbase_am <-\n  cbind(\n    pred_am[, 1:2],\n    \"2clases\" = clases2$cluster,\n    \"3clases\" = clases3$cluster,\n    \"4clases\" = clases4$cluster\n  )\nhead(base_am)\n\n         x       y 2clases 3clases 4clases\n1 312432.8 5800234       2       1       3\n2 312422.8 5800244       2       1       3\n3 312432.8 5800244       2       1       3\n [ reached 'max' / getOption(\"max.print\") -- omitted 3 rows ]\n\n\n\nbase_am <- st_as_sf(base_am, \n                    coords = c(\"x\", \"y\"), \n                    crs = 32721)\n\n\ntm_shape(base_am) +\n  tm_dots(\"2clases\", title = \"Clasifición\")"
  },
  {
    "objectID": "Parte2_cap_a_EscFinaImplR.html#predicción-con-múltiples-capas-de-datos",
    "href": "Parte2_cap_a_EscFinaImplR.html#predicción-con-múltiples-capas-de-datos",
    "title": "\n5  Implementación con R\n",
    "section": "\n5.7 Predicción con múltiples capas de datos",
    "text": "5.7 Predicción con múltiples capas de datos\nEn esta sección también se usa la base de datos Pred2.txt que contiene mediciones de conductividad eléctrica aparente en dos profundidades 0-30 cm (CE30) y 0-90 cm (CE90), elevación (Elev), profundidad de suelo (Pe) y rendimiento de trigo (Tg). La matriz de datos está conformada por n=482 sitios (filas) y p=7 variables (columnas).\nAdicionalmente, para realizar la interpolación utilizando información de las covariables es necesario que la grilla de predicción contenga los valores de las coordenadas y de las covariables. El archivo grilla_am.txt contiene estos datos. A continuación, se cargan ambas bases de datos y transforma a objetos de clase sf.\n\npred <- read.table(\"datos/Pred2.txt\", \n                   header = TRUE)\npred <- st_as_sf(pred, coords = c(\"x\", \"y\"), \n                 crs = 32721)\nhead(pred)\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 312106.4 ymin: 5800205 xmax: 312283.5 ymax: 5800371\nProjected CRS: WGS 84 / UTM zone 21S\n  Pe     Elev CE30 CE90     Tg                 geometry\n1 80 160.8836 21.8 30.6 4.1925 POINT (312283.5 5800205)\n2 40 160.9239 30.3 17.9 4.0030 POINT (312256.5 5800229)\n [ reached 'max' / getOption(\"max.print\") -- omitted 4 rows ]\n\ngrilla <- read.table(\"datos/grilla_am.txt\", \n                     header = TRUE)\ngrilla <- st_as_sf(grilla, coords = c(\"x\", \"y\"), \n                   crs = 32721)\n\n\n5.7.1 Kriging con deriva externa\nEn las siguientes líneas de se realiza el ajuste del semivariograma empírico contemplando una tendencia dada por las covariables CE30, CE90, Elev y Pe y se realiza el ajuste de semivariogramas teóricos. Finalmente, se grafican ambos semivariogramas. Los resultados muestran que el modelo de mejor ajuste fue el exponencial. Los parámetros obtenidos fueron nugget (\\(C_0=0.16\\)), sill parcial (\\(C=0.55\\)) y rango (80 m). Nota: bajo la columna “psill”, para la fila Nugget, se debe leer el valor \\(C_0\\).\n\nsemiv_ked <- variogram(\n  Tg ~ CE30 + CE90 + Elev + Pe, pred\n  )\nv.fit_vut_ked <- \n  fit.variogram(semiv_ked , \n                vgm(c(\"Exp\", \"Sph\", \"Gau\")))\n                               \nv.fit_vut_ked\n\n  model     psill    range\n1   Nug 0.1629146  0.00000\n2   Exp 0.5489328 80.74262\n\nplot(semiv_ked, v.fit_vut_ked)\n\n\n\n\nPara realizar la interpolación espacial se utiliza también la función krige() y se especifica la tendencia con las covariables en la fórmula. En este caso la predicción se realiza también en un contexto local (argumentos nmin y nmax).\n\nkriging_ed <- krige(\n  Tg ~ CE30 + CE90 + Elev + Pe,\n  pred,\n  grilla,\n  model = v.fit_vut_ked,\n  nmin = 7,\n  nmax = 25\n)\n\n[using universal kriging]\n\n\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\n\n\nprediccionKED <-\n  tm_shape(kriging_ed) +\n  tm_dots(\"var1.pred\", style = \"cont\",\n          group = \"Predicción KED\",\n          title = \"Predicción KED\")\n\nvarianzaKED <-\n  tm_shape(kriging_ed) +\n  tm_dots(\"var1.var\", style = \"cont\",\n          group = \"Varianza KED\",\n          title = \"Varianza KED\") \n\n# tmap_mode(\"view\")\ntmap_arrange(prediccionKED, varianzaKED, sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.7.2 Kriging desde modelo de regresión\nPara realizar la interpolación por el método kriging regresión primero se ajusta un modelo lineal de regresión entre la variable Tg y las covariables CE30, CE90, Elev y Pe.\n\nmlr <- lm(Tg ~ CE30 + CE90 + Elev + Pe, pred)\n\nA partir del modelo ajustado se obtienen los residuos que son incorporados al objeto pred. Luego sobre estos residuos se modela el semivariograma empírico y teórico. Los resultados muestran que los valores estimados de los parámetros del semivariograma son similares a los obtenidos en el caso anterior. Esto es esperable ya que ambas aproximaciones son equivalentes.\n\npred$residuos <- mlr$residuals\nnames(pred)\n\n[1] \"Pe\"       \"Elev\"     \"CE30\"     \"CE90\"     \"Tg\"       \"geometry\" \"residuos\"\n\n\n\nsemiv_rk <- variogram(residuos ~ 1 , pred)\n\nv.fit_vut_rk <- \n  fit.variogram(semiv_rk , \n                vgm(c(\"Exp\", \"Sph\", \"Gau\")))\nv.fit_vut_rk\n\n  model     psill    range\n1   Nug 0.1629146  0.00000\n2   Exp 0.5489328 80.74262\n\n\n\nplot(semiv_rk , v.fit_vut_rk)\n\n\n\n\nFinalmente se realiza la predicción en la grilla de los residuos y esta es sumada a la predicción del modelo de regresión ajustado inicialmente para obtener los valores predichos finales.\n\nkgres <- krige(residuos ~ 1, pred, \n               grilla, model = v.fit_vut_rk)\n\n[using ordinary kriging]\n\ngrilla$RK_pred <- \n  predict(mlr, newdata = grilla) + \n  kgres$var1.pred\n\n\ntm_shape(grilla) +\n  tm_dots(\"RK_pred\", style = \"cont\",\n          title = \"Predicción RK\")\n\n\n\n\n\n\n\n5.7.3 Árboles aleatorios\nPara este método primero se ajusta el algoritmo random forest utilizando para ello el paquete caret. Para ello se optimizará el parámetro mtry mediante un proceso de validación cruzada. Los valores prbados de mtry se especifican en el objeto que lleva el mismo nombre. Con la función fitControl() se estable el tipo de validación cruzada, en este caso k-fold con k=10. Además, se permite el paralelizado del proceso en caso de ser necesario (allowParallel=T).\n\nmtry <- expand.grid(mtry = seq(1, 4, 1))\nfitControl <- trainControl(method = \"cv\",\n                           number = 10,\n                           allowParallel = T)\n\nLas siguientes son las opciones de paralelizado que involucra los paquetes parallel y doParallel\n\nlibrary(parallel)\nlibrary(doParallel)\ncluster <- makeCluster(max(1, detectCores() - 1))\nregisterDoParallel(cluster)\n\nEl ajuste del random forest se realiza con la función train() la cual requiere entre otros especifica la fórmula o modelo, la base de datos a utilizar, el algoritmo (rf, random forest), grilla de valores de hiperparámetros a evaluar (objeto mtry) y opciones de la validación.\n\nset.seed(7)\ntrain_rf <- train(\n  Tg ~ Elev + Pe + CE30 + CE90,\n  data = pred,\n  method = \"rf\",\n  tuneGrid = mtry,\n  trControl = fitControl\n)\n\nLuego de ajustar el modelo de RF se procede a obtener los residuos y el ajuste de los semivariogramas.\n\npred$residuosRF <- \n  pred$Tg - predict(train_rf, newdata = pred)\n\nsemiv_RFk <- variogram(residuosRF ~ 1 , pred)\nplot(semiv_RFk)\n\n\n\nv.fit_vut_RFk <- \n  fit.variogram(semiv_RFk , \n                vgm(c(\"Exp\", \"Sph\", \"Gau\")))\nplot(semiv_RFk , v.fit_vut_RFk)\n\n\n\n\nFinalmente se realiza la predicción de los residuos sobre la grilla y se suman a la predicción del modelo de random forest ajustado inicialmente.\n\nkgresRF <- krige(residuosRF ~ 1, pred, \n                 grilla, model = v.fit_vut_RFk)\n\n[using ordinary kriging]\n\n\n\ntm_shape(grilla) +\n  tm_dots(\"RK_pred\", style = \"cont\",\n          title = \"Predicción RFK\") \n\n\n\n\n\n\n\n\n\n\n\n\nCórdoba, Mariano, Cecilia Bruno, José Luis Costa, y Mónica Balzarini. 2013. «Subfield management class delineation using cluster analysis from spatial principal components of soil variables». Computers and Electronics in Agriculture 97 (septiembre): 6-14. https://doi.org/10.1016/j.compag.2013.05.009.\n\n\nMeyer, David, Evgenia Dimitriadou, Kurt Hornik, Andreas Weingessel, y Friedrich Leisch. 2019. e1071: Misc Functions of the Department of Statistics, Probability Theory Group (Formerly: E1071), TU Wien. https://CRAN.R-project.org/package=e1071."
  },
  {
    "objectID": "Parte2_cap_b_EscFinaImplInfo.html#conversión-de-coordenadas-espaciales",
    "href": "Parte2_cap_b_EscFinaImplInfo.html#conversión-de-coordenadas-espaciales",
    "title": "\n6  Implementación con InfoStat\n",
    "section": "\n6.1 Conversión de coordenadas espaciales",
    "text": "6.1 Conversión de coordenadas espaciales\nEste menú permite la conversión entre distintos sistemas de coordenadas de referencias basándose en el uso de los códigos EPSG (http://www.epsg.org/). El archivo de datos que se abre desde el menú Aplicaciones \\(\\to\\) Estadística Espacial \\(\\to\\) Transformación de Coordenadas \\(\\to\\) Datos de prueba. El archivo se denomina datosRinde.idb2 y contiene 9810 observaciones (registros o filas del archivo) sobre datos de rendimiento de trigo (Rinde, \\(t\\ ha^{-1}\\)) de un lote agrícola que fueron recolectados con un monitor de rendimiento. Cada valor registrado posee su ubicación geográfica (x e y) en unidades grado decimal que son expresadas en longitud (x) y latitud (y).\n\n\n\n\n\n\n\n\n\n\nEn el menú Aplicaciones \\(\\to\\) Estadística Espacial \\(\\to\\) Transformación de Coordenadas \\(\\to\\) Run, se abrirá una ventana de selector de variables donde debe indicarse la columna del archivo de datos que contiene la información correspondiente a la Coordenada X y la columna del archivo que contiene la información de la Coordenada Y. Luego, aparecerá una ventana para seleccionar el código EPSG original de los datos y código asignado. En este ejemplo el código original para las coordenadas geográficas WGS84 es 4326 y el asignado para el sistema UTM zona 21 es 32721. En caso de ser necesario previo a esta transformación permite también el pasaje de datos que estén expresados en grados, minutos y segundos a grados decimales. Para ello sólo se debe tildar esta opción en el cuadro de dialogo. Los valores de las coordenadas deben presentarse con un espacio entre subunidades. Por ejemplo “33 1 1” expresan los grados minutos y segundos respectivamente.\n\n\n\n\n\n\n\n\n\n\nComo resultado, en la tabla de datos original se agregarán dos nuevas columnas, al final del archivo, correspondiente a las coordenadas x e y transformadas, las cuales se denominarán Xt e Yt, haciendo referencia a la variable x transformada (Xt) y a la variable y transformada (Yt). Estas nuevas variables (coordenadas transformadas) permiten interpretar la distancia entre puntos de Rinde en metros."
  },
  {
    "objectID": "Parte2_cap_b_EscFinaImplInfo.html#eliminación-de-outliers-e-inliers",
    "href": "Parte2_cap_b_EscFinaImplInfo.html#eliminación-de-outliers-e-inliers",
    "title": "\n6  Implementación con InfoStat\n",
    "section": "\n6.2 Eliminación de outliers e inliers\n",
    "text": "6.2 Eliminación de outliers e inliers\n\nEn el menú Aplicaciones \\(\\to\\) Estadística Espacial \\(\\to\\) Depuración \\(\\to\\) Datos de prueba se abrirá un archivo denominado datosRinde_t.idb2 con 9810 registros y tres columnas correspondientes a las coordenadas planas UTM (x e y) de cada observación y los valores de la variable rendimiento de trigo (\\(t\\ ha^{-1}\\)). Luego, en el menú Aplicaciones \\(\\to\\) Estadística Espacial \\(\\to\\) Depuración \\(\\to\\) Run, se selecciona la variable respuesta, en este ejemplo Rinde y las columnas que indican la ubicación espacial de cada observación en el cuadro de Coordenadas.\n\n\n\n\n\nAl accionar el botón Aceptar, una nueva ventana permitirá seleccionar si desea visualizar los gráficos Histograma, Box-Plot, Red de Vecindarios, Visualización espacial y Moran Plot. También existe la opción de tildar que se genere una tabla nueva con los datos depurados, otra tabla con los estadísticos del Índice de Moran y una opción que permite que en la definición de los vecindarios existan algunos puntos que no presenten vecinos. Los valores inliers serán identificados a través del Índice de Moran local (IM local), Moran Plot o ambos (IM local+Moran Plot). En la misma ventana hay una opción de seleccionar el valor de desviación estándar (DE) de acuerdo al criterio del usuario. Considerando como valores outliers a los que se encuentren fuera del rango estimado como el valor medio ± el valor de DE seleccionado. Por defecto presenta un valor de 3 DE. También el usuario previamente puede limitar los valores de los datos por un valor mínimo y/o máximo. Por defecto este filtro no se aplica.\nEn este ejemplo se seleccionaron las Opciones Histograma, Box-Plot, Visualización espacial, Moran plot y Tabla datos depurados. Además, se seleccionó que la eliminación de inliers se haga mediante el cálculo del índice de local de Moran y el Moran Plot (IM local+Moran Plot). Se utilizó una distancia de 15 para definir los vecindarios y un criterio de 3 DE para la eliminación de outliers. Por defecto el código EPSG es 32721 que correspondiente a la base de datos de prueba. En esta rutina el sistema de referencia es importante sólo para realizar la visualización espacial.\n\n\n\n\n\nComo resultado se obtendrá el Box-Plot de la variable respuesta (Rinde) previo a la eliminación de los outliers e inliers (datos sin depurar), y luego de su eliminación (datos depurados). El gráfico de dispersión de Moran (Moran Plot) muestra en el eje horizontal los valores de la variable rendimiento mientras que en el vertical se representa el retardo espacial de la variable. Los puntos negros con forma romboidal son identificados como influyentes y se los considera como inliers.\n\n\n\n\n\nComo resultado de la depuración de datos en la base de datos original se crearán nuevas columnas al final del archivo con la clasificación de outliers y/o inliers para cada observación. Así, se adicionarán tres columnas denominadas Outliers, Inliers_ML e Inliers_MP según el método seleccionado. Estas variables son de tipo categóricas y contendrán la clasificación Outliers o Normal indicando si es considerado un valor extremo o no. En la columna de Inliers_ML, las categorías serán Inliers o Normal para indicar que no es un valor atípico respecto a sus vecinos cercanos y de la misma manera se indicarán los valores clasificados por el método de Moran Plot. En este ejemplo, 120 observaciones fueron clasificadas como outliers, 43 observaciones como inliers y 681 clasificadas como inliers según Moran Plot (en los que se encuentran incluidos los 43 casos considerados Inliers por el Índice de Moran Local). El procedimiento primero elimina los outliers por lo cual no serán considerados para su identificación como posibles inliers.\n\n\n\n\n\nTambién se generará una nueva tabla de datos que contendrá las observaciones seleccionadas después de la depuración por outliers e inliers, que para este ejemplo contiene 9009 registros. Los valores seleccionados se muestran en la tabla denominada Datos Depurados.idb2. En caso que el usuario seleccione Tabla estadísticos IM se genera otra tabla Estadísticos IM.idb2, en la cual sus columnas tendrán los valores de índice local observado (li), el valor esperado (E.li), la varianza (Var.li), el estadístico (Z.li) y la significancia estadística a través del valor-p (Pr.z…0). La información del Moran Plot de los puntos influyentes de la regresión proviene de diferentes estadísticos de diagnóstico como DFBETAS (dfb.1_ para la ordenada al origen y dfb.x para la pendiente), DFFITS (dffit), Covratio (cov.r), distancia de Cook (cook.d) y leverage (hat). Para cada uno de los estadísticos aparecerá una nueva columna del archivo con valores TRUE o FALSE según como ha sido clasificada la observación. En la ventana de Resultados de InfoStat se indica las tablas donde se encuentran estos resultados descriptos. Las nuevas tablas generadas son archivos temporarios, es decir, si desea guardar esta información, deberá ir al menú Archivo \\(\\to\\) Guardar tabla y seleccionar el directorio donde desea guardarlo. La opción de visualización espacial muestra un gráfico el cual se ejecuta como un archivo html en el navegador web. En este se podrá visualizar los datos detectados como outliers e inliers. Así mismo es posible cargar un mapa base como esri.WorldImagery, OpenStreetMap, OpenTopoMap."
  },
  {
    "objectID": "Parte2_cap_b_EscFinaImplInfo.html#detección-de-tendencias-espaciales",
    "href": "Parte2_cap_b_EscFinaImplInfo.html#detección-de-tendencias-espaciales",
    "title": "\n6  Implementación con InfoStat\n",
    "section": "\n6.3 Detección de tendencias espaciales",
    "text": "6.3 Detección de tendencias espaciales\nUn análisis gráfico para identificar de la presencia de tendencias se puede hacer con un gráfico de dispersión. Para ello ir al menú Gráficos \\(\\to\\) Diagrama de dispersión. En el casiller Eje Y colocar la variable Rinde y en Eje X la coordenada x. Al accionar Aceptar se generará el gráfico que muestra que no existe una tendencia entre ambas variables. El mismo procedimiento deber realizarse con la coordenada y.\n\n\n\n\n\n\n\n\n\n\nTambién es posible ajustar de un modelo de regresión lineal entre la variable Rendimiento y las coordenadas x e y, se puede realizar yendo al menú Estadísticas \\(\\to\\) Regresión Lineal. En el selector de variables se coloca al rendimiento en el casillero de Variable Dependiente y las coordenadas x e y en el casillero Regresoras."
  },
  {
    "objectID": "Parte2_cap_b_EscFinaImplInfo.html#cálculo-del-índice-de-moran",
    "href": "Parte2_cap_b_EscFinaImplInfo.html#cálculo-del-índice-de-moran",
    "title": "\n6  Implementación con InfoStat\n",
    "section": "\n6.4 Cálculo del índice de Moran",
    "text": "6.4 Cálculo del índice de Moran\nCon la base de datos ya depurada en el punto 2.2 se procederá a calcular índices de autocorrelación espacial. El archivo de ejemplo datosRinde_dep.idb2 también se encuentra disponible en el menú Aplicaciones \\(\\to\\) Estadística Espacial \\(\\to\\) Geoestadísticas \\(\\to\\) Semivariograma \\(\\to\\) WLS \\(\\to\\) Datos de prueba. Para cuantificar la magnitud de la estructuración espacial se estiman en este menú el índice de Moran y el índice de Geary. Para ello ir al menú Aplicaciones \\(\\to\\) Estadística Espacial \\(\\to\\) Geoestadísticas \\(\\to\\) Índices de Autocorrelación \\(\\to\\) Run. En el selector de variables colocar Rinde en el casillero Variables (aquí es posible colocar más de una) y en el casillero Coordenadas las columnas x e y.\n\n\n\n\n\nAl accionar Aceptar en el siguiente cuadro se puede modificar el número de permutaciones que son utilizadas para evaluar la significancia estadística de los índices a partir de simulación por Monte Carlo. Las ubicaciones son permutadas para obtener la distribución de los índices bajo hipótesis nula de distribución aleatoria. El cálculo también requiere la definición de una matriz de ponderación espacial, para este paso la red de vecindarios es definida usando la distancia Euclídea indicando el rango de distancia dentro de la cual dos observaciones serán consideradas colindantes o vecinas. El rango es estimado a partir de un valor de distancia máximo y uno mínimo. En este ejemplo se utiliza como distancia máxima 15 m. Además, se seleccionó la opción Datos con vecindario nulo la cual permite generar la matriz de pesos espaciales sin la restricción de que todos los puntos tengan al menos un dato vecino.\n\n\n\n\n\nPara este ejemplo, tanto el índice de Moran como el índice de Geary indican una autocorrelación estadísticamente significativa (valor p <0.05), es decir, no hay una distribución aleatoria de las observaciones en el espacio. La variable Rinde presentó una autocorrelación espacial positiva y alta (valor más cercano a 1 en el caso del índice de Moran y mas cercano a 0 en el de Geary sugieren autocorrelaciones positivas más fuerte)."
  },
  {
    "objectID": "Parte2_cap_b_EscFinaImplInfo.html#análisis-basado-en-semivariogramas",
    "href": "Parte2_cap_b_EscFinaImplInfo.html#análisis-basado-en-semivariogramas",
    "title": "\n6  Implementación con InfoStat\n",
    "section": "\n6.5 Análisis basado en semivariogramas",
    "text": "6.5 Análisis basado en semivariogramas\nPara acceder al archivo de ejemplo datosRinde_dep.idb2 se debe ir al menú Aplicaciones \\(\\to\\) Estadística Espacial \\(\\to\\) Geoestadísticas \\(\\to\\) Semivariograma \\(\\to\\) WLS \\(\\to\\) Datos de prueba. Luego se acciona la opción Run siguiendo la misma ruta de acceso. Esta opción realizará el ajuste de un semivariograma empírico y teórico usando el método de mínimos cuadrados ponderados (WLS). En el selector de variables se debe colocar la variable Rinde en el casillero Variables y en el casillero Coordenadas las columnas x e y.\n\n\n\n\n\nAl accionar el botón Aceptar, aparecerá una nueva ventana donde se podrá seleccionar la función de semivariograma: Exp (exponencial), Sph (esférico), Gau (gaussiano), Mat (Matern), Ste (parametrización de Matern Stein), Cir (circular), Lin (lineal), Pow (potencia o power), Wav (ondulado o wave), Pen (pentaesférico), Hol (holístico o hole), Log (logarítmico) y Spl (spline). Como opciones se puede seleccionar ajustar un semivariograma sin tendencia (Constante), con tendencia de primer orden, de segundo orden y con covariable. También es posible indicar los valores iniciales para los parámetros del semivariograma a ajustar (SillParcial, Rango y Nugget). En este ejemplo no se colocaron estos por lo cual la función estima valores iniciales razonable para realizar el ajuste. El parámetro kappa es opcional para los modelos Matern y Ste (parametrización de Matern Stein). También, puede ser fijado ingresando un valor en la opción Kappa. Si en lugar de colocar un valor numérico se coloca un carácter, se ajustará un kappa óptimo en un rango entre 0.05 y 5. La opción de parámetro width refiere a la amplitud del intervalo de distancia sobre la cual los pares de puntos agrupados para la estimación de la semivarianza. Por ejemplo, si la distancia máxima entre los pares de puntos es de 1000 y se selecciona un valor de width=100, se conformarán 10 grupos de una amplitud de 100 m. Para cada uno de esos 10 grupos se estimará la semivarianza. El cutoff es la máxima distancia de separación espacial hasta la cual los pares de puntos son tenidos en cuenta para la estimación de la semivarianza. Si la opción queda vacía, es decir no se coloca ningún valor, por defecto estima el cutoff como el tercio de la línea diagonal de una caja que contiene los datos. La función por defecto ajusta un semivariograma isotrópico. Para evaluar si el proceso es anisotrópico es posible el ajuste de semivariogramas direccionales. Esto puede hacer desde el menú Aplicaciones \\(\\to\\) Estadística Espacial \\(\\to\\) Geoestadísticas \\(\\to\\) Semivariograma \\(\\to\\) Direccionales. Para incorporar la anisotropía al modelo, en la opción anis, se debe colocar la dirección de mayor correlación espacial i.e mayor rango (valor entre 0° y 360°, medidos en el sentido de las agujas del reloj, donde el Norte es 0°.) y el cociente de anisotropía (cociente entre el mayor y el menor rango que se producen en las direcciones evaluadas, valor entre 0 y 1). La dirección y el cociente de anisotropía deben ir separados por coma. El separador de decimales es el punto. Por ejemplo anis=c(90, 0.2) indica que el mayor rango se produce a las 3 en punto (dirección este) con una diferencia de 5 veces en el rango.\nLa opción cressie permite el ajuste del semivariograma teórico mediante el estimador robusto de Cressie- Hawkins. Para seleccionar este estimador debe colocarse un si a esta opción, caso contrario utilizara el estimador de los momentos de Matheron. Esta función permite realizar una validación cruzada el tipo k-fold. Para ello se debe colocar si en la opción Validación. El número de grupos k se coloca en la opción n-fold. Además, es posible fijar la semilla para que la asignación de cada dato a los grupos k no cambie en caso de repetir el proceso. Las opciones nmin, nmax y Max.dist se utilizan para que la función kriging, usada en el proceso de validación, se realice en un contexto local. Por defecto se definen vecindarios con un número mínimo y máximo de vecinos de cada punto a predecir siendo estos de 7 y 25, respectivamente. En caso de omitir estos últimos argumentos la interpolación se realiza en un contexto global.\nEn este ejemplo, se ajustó un semivariograma experimental a partir de los datos observados y se probaron los modelos exponencial, esférico y gaussiano como modelos de semivariograma teóricos. Sus parámetros iniciales fueron estimados por defecto por la función. El valor del cutoff se fijó en 400 m. Además, se realizó una validación cruzada con k=10. Todas las otras opciones se dejaron por defecto. Al accionar Aceptar se generan los gráficos con los tres modelos ajustados. La ventana resultados muestra también la información del semivariograma empírico y cada uno de los modelos teóricos ajustados. Además, se proporcionan los valores de los parámetros estimados, criterios de bondad de ajuste (SCE y AIC), y error de predicción de los modelos ajustados. En este ejemplo siguiendo los criterios de SCE y AIC el modelo de mejor ajuste (valores más bajos de estos indicadores) fue el exponencial. Los valores de los parámetros fueron: Sill Parcial=0,86, rango=64,88, efecto Nugget=0,21. Los resultados de la validación cruzada muestran pequeñas diferencias entre los errores de predicción de los modelos ajustados. Para el caso del modelo exponencial la RMSE relativa a la media de los valores observados (nRMSE) fue del 11,82% mientras cociente de la desviación cuadrática media (MSDR) fue de 0,60 (más cercano a 1 mejor modelo).\n\n\n\n\n\n\n\n\n\n\n\n6.5.1 Mapeo de variabilidad espacial\nPara realizar la interpolación espacial mediante kriging se utilizará la base de datos datosRinde_limites.idb2 que se encuentra en el menú Aplicaciones \\(\\to\\) Estadística Espacial \\(\\to\\) Geoestadísticas \\(\\to\\) Interpolación \\(\\to\\) Kriging \\(\\to\\) Datos de prueba. El archivo de datos contiene además de las coordenadas (x e y), la variable respuesta (Rinde) y dos nuevas columnas (x1 e y1) que contienen los vértices del polígono donde se desea realizar la interpolación. Estas últimas no son obligatorias para realizar la interpolación. En caso de omitirse la función tomara los puntos más externos de la base de datos y a partir de este formara un polígono que definirán los límites del área a interpolar.\n\n\n\n\n\nSi se vuelve a la misma ruta de accesos y se acciona Run se abrirá el selector de variables donde se colocará la variable Rinde en el casillero Variable Respuesta, x e y en Coordenadas y las columnas x1 e y1 en los casilleros Coord. Vértices X y Coord. Vértices Y, respectivamente.\n\n\n\n\n\nLuego de accionar Aceptar, se genera una ventana con diferentes opciones para seleccionar el tipo de modelo ajustado (Exp, Sph, Gau, Ste, Cir, Lin, Pow, Wav, Pen, Hol, Log, y Spl), el tipo de kriging, ordinario sin tendencia o universal (incorporando la tendencia en primer o segundo orden) y los valores para realizar la predicción espacial. En este caso se utilizará el modelo seleccionado en el punto anterior. Así, se selecciona el modelo exponencial y se fijan los valores de los parámetros Sill Parcial=0.86, rango=64,88, efecto Nugget=0.21. En este caso la predicción se realiza en un contexto local (opción por defecto). Para ello se fija un número mínimo (nmin) y máximo (nmax) de puntos que son utilizados para realizar la predicción en cada uno de los sitios de la grilla de predicción. Otra opción para determinar cuáles son los puntos que aportan información para la predicción de un sitio determinado es usando una medida de distancia (Max.dist). Puntos que se ubican más allá de la distancia máxima determinada por el usuario respecto a la posición sobre la que se quiere predecir el valor de la respuesta, no serán utilizados para la predicción. En caso de requerir que la predicción se realice utilizando toda la información disponible (kriging global), las opciones nmin, nmax y Max.dist se dejan vacías. La predicción también puede realizarse de manera puntual o en bloque. En este ejemplo donde bloque (Block) es cero, la predicción será local.\nPara realizar la predicción se requiere definir los puntos donde se realiza la interpolación en Opciones de Predicción. Con Dim. Grilla Predicción se fija la dimensión de la grilla de predicción, en este caso, 10 indica que tiene una dimensión de 10×10 m. También puede fijarse el sistema de coordenadas que tendrán los mapas generados usando el código EPSG. Para estos datos el código es 32721 que corresponde al sistema de coordenadas UTM, zona 21, hemisferio sur. Esto es importante para proyectar correctamente los mapas generados. Es posible visualizar estos mapas en el navegador web colocando si en la opción Generar HTML o exportarlos como geotiff (opción Generar GeoTIF). En caso de colocar si en esta última opción el software abrirá una ventana para elegir el directorio en el cual se guardará el archivo generado. Los valores de la predicción también pueden ser colocados en forma de tabla mediante la opción Guardar Predicción. Finalmente, también es posible cambiar los valores mínimos y máximos de las escalas de valores de los mapas que se generan (Predicción y Varianza de Predicción).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.5.2 Validación cruzada\nLa evaluación de la capacidad predictiva de los modelos ajustados que se realizó en el punto 2.5, también puede realizarse desde el menú Aplicaciones \\(\\to\\) Estadística Espacial \\(\\to\\) Geoestadísticas \\(\\to\\) Interpolación \\(\\to\\) Validación Cruzada. En el selector de variables se coloca la variable Rinde en el casillero Variable Respuesta y las coordenadas en el cuadro Coordenadas.\n\n\n\n\n\nEn la siguiente ventana se coloca la información del modelo a evaluar, en este caso exponencial con parámetros Sill Parcial= 0.86, rango=64.88 y efecto Nugget=0.21. La predicción kriging se realiza en un contexto local (opción por defecto) con nmin=7 y nmax=25. El número de grupos de la validación cruzada es de k=10. La función permite también calcular el error de predicción para kriging universal (tendencias de primer y segundo orden) y kriging con deriva externa.\n\n\n\n\n\nPara este ejemplo la RMSE relativa a la media de los valores observados (nRMSE) fue del 10,27% mientras cociente de la desviación cuadrática media (MSDR) fue de 0.66 (más cercano a 1 mejor modelo)."
  },
  {
    "objectID": "Parte2_cap_b_EscFinaImplInfo.html#caracterización-de-variabilidad-espacial-con-múltiples-capas-de-datos",
    "href": "Parte2_cap_b_EscFinaImplInfo.html#caracterización-de-variabilidad-espacial-con-múltiples-capas-de-datos",
    "title": "\n6  Implementación con InfoStat\n",
    "section": "\n6.6 Caracterización de variabilidad espacial con múltiples capas de datos",
    "text": "6.6 Caracterización de variabilidad espacial con múltiples capas de datos\n\n6.6.1 Análisis de componentes principales\nLa base de datos Pred2.idb2 se encuentra disponible en el menú Aplicaciones \\(\\to\\) Estadística Espacial \\(\\to\\) Geoestadísticas \\(\\to\\) Multivariado \\(\\to\\) a. MULTISPATI-PCA \\(\\to\\) Datos de prueba. En el selector de variables se colocan las variables Pe, Elev, CE30, CE90 y Tg en el cuadro Variables y las coordenadas x e y en Coordenadas.\n\n\n\n\n\nEn la siguiente ventana se presentan las opciones del MULTISPATI-PCA. Aquí se puede seleccionar estandarizar las variables (opción por defecto), generara gráficos (biplot del PCA, gráfico de la red de vecindarios, gráficos del MULTISPATI-PCA y gráfico de los autovalores) y solicitar que la conformación de la matriz de pesos espaciales admita datos con vecindarios nulo. Las otras opciones del análisis implican fijar el número de ejes retenidos por el PCA y por el MULTISPATI-PCA. Para este último caso pueden ser aquellos que presenten una autocorrelación positiva (MULTISPATI-PCA (+)) o negativa (MULTISPATI-PCA (-)). La parametrización para este ejemplo se muestra en la siguiente figura:\n\n\n\n\n\nEl gráfico obtenido del MULTISPATI-PCA muestra que las variables Elev y Pe son las más importantes en la explicación de la variabilidad espacial a nivel del primer eje (sPC1, eje horizontal). Mientras que la CE30 y Tg presentan mayor importancia en la SPC2. Además, se observa una correlación positiva entre CE30 y CE90, y negativa entre estas dos y la Pe. También la Elev y Tg se correlacionan en forma negativa.\n\n\n\n\n\nLos resultados muestran que con MULTISPATIPCA se explica una menor proporción de la varianza acumulada en el primer eje, respecto de PCA (1,81 vs. 1,94). Las tres primeras CP del PCA explican 78% de la variabilidad total mientras que la CS1, CS2 CS3 del MULTISPATI el 73%. No obstante, los valores del índice de Moran calculados para las tres primeras CPs sugieren que la estimación de autocorrelación aumentó cuando se usó MULTISPATIPCA respecto de la contenida en las CPs del PCA (0,79 vs. 0,68 para el eje 1, 0,43 vs. 0,40 para el eje 2, 0,50 vs. 0,25 para el eje 3).\n\n\n\n\n\n\n6.6.2 Análisis de conglomerados\nPara realizar la ilustración se utilizará la base de datos Pred.idb2 que se encuentra en el menú Aplicaciones \\(\\to\\) Estadística Espacial \\(\\to\\) Geoestadísticas \\(\\to\\) Multivariado \\(\\to\\) c. Clasificación KMsPC \\(\\to\\) Datos de prueba. El archivo de datos contiene además de las coordenadas (x e y), valores de mediciones de conductividad eléctrica aparente en dos profundidades 0-30 cm (CE30) y 0-90 cm (CE90), elevación (Elev), profundidad de suelo (Pe) y rendimiento de trigo (Tg). Las variables CE30, CE90, Elev Pe y Tg se colocan en el casillero Variables y las coordenadas x e y en el cuadro Coordenadas.\n\n\n\n\n\nAl accionar Aceptar, aparecerá la ventana para seleccionar opciones del método de análisis. El método KM-sPC primero realiza un análisis de componentes principales espaciales (MULTISPATI-PCA) sobre las variables originales. Luego las variables sintéticas (componentes principales espaciales, sPC) son utilizadas como input del análisis de clúster fuzzy k-means. Por ello, es posible estandarizar las variables para realizar el MULTISPATI-PCA y elegir la opción que permite conformar una matriz de pesos espaciales en presencia de datos con vecindarios nulo. Otras opciones del análisis incluyen la distancia (euclídea o manhattan) utilizada en el método de cluster, el número mínimo y máximo de clúster a generar, el número de iteraciones y el exponente difuso. En este ejemplo se usaron las siguientes opciones: 2 hasta 6 cluster, 100 iteraciones y un valor de 1.3 para el exponente difuso. Para el cálculo de la red de vecindarios, necesario para realizar el MULTISPATI-PCA, la distancia mínima y máxima fue de 0 y 10 m, respectivamente. La opción de varianza explicada (%) fue del 70, lo que indica que seleccione la cantidad de ejes (componentes principales espaciales) necesarios tal que la varianza total explicada sea mayor o igual a 70%.\n\n\n\n\n\nEn la ventana resultados se muestra la suma de cuadrados de distancias dentro (SCDD) la cual puede usarse para determinar el número de clúster óptimo. Para esto otros índices son calculados como Xie-Beni, Fukuyama Sugeno, Coeficiente de Partición, Entropía de Partición. Para todos ellos un menor valor del índice implica mejor clasificación. Dado que muchas veces los índices no coinciden se adiciona el cálculo de un índice resumen. Los resultados muestran que, para la mayoría de los índices, incluyendo el resumen, el número de clúster optimo es 2. Cuando se ejecuta el análisis los índices también son graficados en forma conjunta usando una escala normalizada. En este caso también un menor valor del índice en la escala normalizada implica una mejor clasificación."
  },
  {
    "objectID": "Parte2_cap_b_EscFinaImplInfo.html#predicción-con-múltiples-capas-de-datos",
    "href": "Parte2_cap_b_EscFinaImplInfo.html#predicción-con-múltiples-capas-de-datos",
    "title": "\n6  Implementación con InfoStat\n",
    "section": "\n6.7 Predicción con múltiples capas de datos",
    "text": "6.7 Predicción con múltiples capas de datos\nLos datos de ilustración Pred2.idb2 serán utilizados para cada de las alternativas de predicción disponibles en el menú. El archivo de datos contiene además de las coordenadas (x e y), valores de mediciones de conductividad eléctrica aparente en dos profundidades 0-30 cm (CE30) y 0-90 cm (CE90), elevación (Elev), profundidad de suelo (Pe) y rendimiento de trigo (Tg).\n\n6.7.1 Kriging con deriva externa\nEl acceso al menú es a partir de las opciones: Aplicaciones \\(\\to\\) Estadística Espacial \\(\\to\\) Geoestadísticas \\(\\to\\) Interpolación \\(\\to\\) c. Kriging (KED) \\(\\to\\) Datos de prueba. Para realizar la interpolación se necesita de una grilla de predicción previamente generada en formato de archivo .txt que cuente con la información de las coordenadas (x e y) en las dos primeras columnas y de cada una de las covariables que se usaron en el ajuste del modelo. Se requiere que los nombres de las covariables sean los mismos tanto en la grilla de predicción como en la tabla de observaciones. El archivo grilla_am.txt contiene esta información.\nEn primer lugar se procede al ajuste de los semivariograma experimental teórico. Para ello se accede al menú Aplicaciones \\(\\to\\) Estadística Espacial \\(\\to\\) Geoestadísticas \\(\\to\\) b. Semivariograma \\(\\to\\) WLS \\(\\to\\) Run. Las variables CE30, CE90, Elev Pe y Tg se colocan en el casillero Variables, las coordenadas x e y en el cuadro Coordenadas y Pe, Elev, CE30 y CE90 en el cuadro Covariables.\n\n\n\n\n\nEn la siguiente ventana se seleccionan los modelos teóricos a ajustar, en este ejemplo exponencial esférico y gaussiano. En la opción Tendencia se selecciona Covariables. Las otras opciones de ajuste de los semivariogramas se dejan por defecto. Los resultados muestran que el modelo de mejor ajuste según los valores de SCE y AIC es el exponencial. Los semivariogramas teóricos ajustados se despliegan en forma automática.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCon los valores de los parámetros estimados del semivariograma exponencial se procede a realizar la interpolación espacial. Para ello se accede al menú Aplicaciones \\(\\to\\) Estadística Espacial \\(\\to\\) Geoestadísticas \\(\\to\\) Interpolación \\(\\to\\) c. Kriging (KED) \\(\\to\\) Run. Las variables CE30, CE90, Elev Pe y Tg se colocan en el casillero Variables, las coordenadas x e y en el cuadro Coordenadas y Pe, Elev, CE30 y CE90 en el cuadro Covariables.\n\n\n\n\n\nEn la ventana siguiente se selecciona el modelo de semivariograma exponencial y se colocan los valores de los parámetros estimados en el paso anterior. En este ejemplo se selecciona la opción para mostrar el mapa de predicción en una ventana del navegador web (opción Generar HTML). Para ello es importante fijar el sistema de coordenada mediante el código EPSG.\n\n\n\n\n\nAl Aceptar se abrirá una ventana que permite seleccionar la grilla de predicción que en este ejemplo se denomina grilla_am.txt. Posteriormente se generarán los mapas de predicción y varianza de predicción.\n\n\n\n\n\n\n\n\n\n\n\n6.7.2 Kriging desde modelo de regresión\nEl acceso al menu es a partir de las opciones: Aplicaciones \\(\\to\\) Estadística Espacial \\(\\to\\) Geoestadísticas \\(\\to\\) Interpolación \\(\\to\\) c. Regression Kriging \\(\\to\\) Datos de prueba. Las variables CE30, CE90, Elev Pe y Tg se colocan en el casillero Variables y las coordenadas x e y en el cuadro Coordenadas.\n\n\n\n\n\nEn la ventana siguiente se puede seleccionar que el método sólo realice la predicción en base al ajuste de un modelo de regresión lineal o sumando a la predicción los valores interpolados usando kriging ordinario de los residuos del modelo de regresión (Regresión Lineal + kriging Ordinario). La función ajusta en forma automática el semivariograma experimental y los modelos teóricos exponencial, esférico y gaussiano. Luego, selecciona el de mejor ajuste según el valor de SCE, y los parámetros de este son usados en la interpolación de los residuos. El método también permite realizar una selección de variables paso a paso (stepwise) en ambas direcciones usando el criterio de información de Akaike para evaluar el ajuste de los modelos. Otras opciones incluyen la posibilidad de realizar una validación cruzada k-fold, donde se debe especificar el valor de k y un valor para la semilla. Para ello es necesario colocar si en la opción Evaluar. También es posible guardar los valores predichos (opción Predichos) y obtener la predicción (opción Predecir) sobre una nueva base de datos. Para esto se necesita tener un archivo .txt con la información de las coordenadas (x e y) en las dos primeras columnas y de cada una de las covariables que se usaron en el ajuste del modelo. Se requiere que los nombres de las covariables sean los mismos tanto en la grilla como en la tabla de datos. La opción Importancia permite elaborar un ranking de la importancia relativa de cada covariable en el modelo ajustado. Esta se calcula en función a la influencia que tiene cada predictor en Error Cuadrático Medio (MSE) del modelo mediante un proceso de permutación de los valores de cada covariable.\n\n\n\n\n\nAl ejecutar el análisis, dado que se solicitó realizar la predicción, el software mostrará una ventana para seleccionar el archivo .txt que corresponde a la grilla de predicción. En este ejemplo se denomina grilla_am.txt. Al accionar Abrir se procederá con el análisis. En la ventana Resultados se menciona que los valores Predichos se adicionan a la tabla de datos Pred2.idb2. Además, los valores de la predicción se adicionan a la grilla y se despliegan en una nueva tabla de datos denominada Predicción.\n\n\n\n\n\n\n\n\n\n\nLos resultados muestran las medidas para cuantificar el error de predicción, entre estas el error medio absoluto (MAE), la raíz del error cuadrático medio (RMSE), la RMSE relativa a la media de los observados (nRMSE) y un valor de \\(R^2\\). Los valores muestran un mejor desempeño del kriging regresión vs. el modelo de regresión lineal múltiple. En el ranking de importancia de las variables explicativas se observa que la elevación fue la variable que contribuyo en mayor medida a explicar la variabilidad del rendimiento de trigo. El semivariograma ajustado sobre los residuos del modelo de regresión lineal muestra la existencia de una estructura de correlación espacial (bajo valor del cociente nugget/sill).\n\n\n\n\n6.7.3 Árboles aleatorios\nPara realizar el análisis las variables CE30, CE90, Elev Pe y Tg se colocan en el casillero Variables y las coordenadas x e y en el cuadro Coordenadas.\n\n\n\n\n\nEn la ventana siguiente se puede seleccionar que el método sólo realice la predicción en base al algoritmo Random Forest o sumando a la predicción de este los valores interpolados usando kriging ordinario de los residuos del Random Forest (Random Forest + kriging Ordinario). Luego, selecciona el de mejor ajuste según el valor de SCE, y los parámetros de este son usados en la interpolación de los residuos. La función ajusta en forma automática el semivariograma experimental y los modelos teóricos exponencial, esférico y gaussiano. El método también permite fijar un valor del hiperparámetro mtry del random forest o también realizar una selección del mtry optimo mediante un proceso de validación cruzada del tipo k-fold. Para la validación se puede especificar el valor de k y un valor de semilla. En caso de no fijar el valor de mtry utiliza el recomendado de \\(p/3\\) para modelos de regresión o \\(\\sqrt{p}\\) para modelos de clasificación. Si se quiere probar más de un valor se puede especificar un vector de valores que inician con el valor colocado en la opción Desde hasta el valor especificado en Hasta con un salto dado por la opción Paso. La opción Evaluar permite realizar una validación cruzada de la misma forma que la especificada inicialmente. Para ello es necesario colocar si en dicha opción.\nTambién, es posible guardar los valores predichos (opción Predichos) y obtener la predicción (opción Predecir) sobre una nueva base de datos. Para tal fin se necesita tener un archivo .txt de la grilla de predicción con la información de las coordenadas (x e y) en las dos primeras columnas y de cada una de las covariables que se usaron en el ajuste del modelo. Se requiere que los nombres de las covariables sean los mismos tanto en la grilla como en la tabla de datos. La opción Importancia devuelve un ranking de la importancia relativa de cada covariable en el modelo ajustado, que se calcula en función a la influencia que tiene cada predictor en Error Cuadrático Medio (MSE) del modelo mediante un proceso de permutación de los valores de cada covariable. La opción Relaciones Parciales generar un gráfico que muestra el efecto marginal de cada una de las covariables sobre la variable respuesta. Para obtener este gráfico debe colocarse si. Esto genera un panel con los gráficos de cada una de las covaraiables del modelo. Si en lugar de si se coloca el carácter m el método genera un gráfico independiente para cada una de las regresoras. También, es posible editar los valores mínimos y máximos de la variable respuesta (eje y) en todos los gráficos mediante las opciones ymin e ymax.\n\n\n\n\n\nAl ejecutar el análisis, dado que se solicitó realizar la predicción, el software mostrará una ventana para seleccionar el archivo .txt que corresponde a la grilla de predicción. En este ejemplo se denomina grilla_am.txt. Al accionar Abrir se procederá con el análisis. En la ventana Resultados se menciona que los valores Predichos se adicionan a la tabla de datos Pred2.idb2. Además, los valores de la predicción se adicionan a la grilla y se despliegan en una nueva tabla de datos denominada Predicción.\n\n\n\n\n\n\n\n\n\n\nLos resultados muestran las medidas para cuantificar el error de predicción, entre estas el error medio absoluto (MAE), la raíz del error cuadrático medio (RMSE), la RMSE relativa a la media de los observados (nRMSE) y un valor de \\(R^2\\). Los valores muestran un mejor desempeño del kriging regresión vs. el modelo de regresión lineal múltiple. En el ranking de importancia de las variables explicativas se observa que la elevación fue la mayor contribución en la determinación del rendimiento de trigo. El gráfico de las relaciones parciales muestra que cuando aumenta la elevación disminuye el rendimiento de trigo. Con la variable Pe la correlación es negativa mientras que con la conductividad eléctrica presenta una relación no lineal. El semivariograma ajustado sobre los residuos del modelo de regresión lineal muestran la existencia de una estructura de correlación espacial (bajo valor del cociente nugget/sill)."
  },
  {
    "objectID": "Parte3_cap_a_EscRegionImplR.html",
    "href": "Parte3_cap_a_EscRegionImplR.html",
    "title": "\n7  Bases de datos regionales\n",
    "section": "",
    "text": "8 Predicción con múltiples capas de datos\nUna vez que se ha confeccionado la grilla de predicción y se ha unificado el sistema de referencia espacial entre las distintas capas de información, se comienza con el ajuste de modelos que luego serán usados para la predicción espacial en sitios sin datos. El objeto suelos, se utilizará para el ajuste de los modelos predictivos, mientras que cetroide_pred se usará para obtener predicciones para cada celda de la grilla. La distribución espacial de la variable de interés (COS) puede visualizarse con funciones del paquete tmap. A través del argumento palette se modifica la paleta de colores, las opciones disponibles pueden buscarse ejecutando el comando tmaptools::palette_explorer(). También se pueden adicionar otras herramientas de estadística descriptiva, como por ejemplo un histograma de frecuencia mediante el argumento legend.hist = TRUE. Los estilos de los ejes y leyendas se pueden modificar con la función tm_layout()."
  },
  {
    "objectID": "Parte3_cap_a_EscRegionImplR.html#manejo-de-datos-espaciales",
    "href": "Parte3_cap_a_EscRegionImplR.html#manejo-de-datos-espaciales",
    "title": "\n7  Bases de datos regionales\n",
    "section": "\n7.1 Manejo de datos espaciales",
    "text": "7.1 Manejo de datos espaciales\nMediante la función read.table() se lee un archivo de texto que se guarda como un objeto denominado suelos, en el cual las columnas están separadas por tabuladores y la primera fila contiene los nombres de columnas. Mediante la función head() se visualizan las primeras filas del objeto suelos donde se observa que la primera columna corresponde a una identificación, las siguientes dos son las coordenadas X e Y las cuales corresponden al sistema de proyección UTM faja 20. Las columnas siguientes contienen las variables en estudio.\n\nsuelos <- read.table(\"datos/suelos_cba.txt\",\n                     sep = \"\\t\", header = TRUE)\nhead(suelos)\n\n  ID_2        X       Y elevacion twi arcilla  pH   COS\n1    2 603163.6 6576899       100 135    30.8 6.6 25.98\n2    3 596537.1 6390518        87 133    24.0 7.4 17.29\n3    4 595665.5 6380484        93 126    28.5 6.1 17.43\n4    5 601138.5 6353446       105 119    28.8 6.9 15.08\n5    6 601798.1 6344096       111 127    25.2 7.4 17.30\n6    7 587501.2 6615272        94 135    33.6 6.7 16.13\n\n\nPara transformar este objeto en uno de clase espacial, se utilizará la función st_as_sf(), especificando que las coordenadas X e Y, se encuentra en las columnas “X” e “Y”, respectivamente. Todos los sistemas de coordenadas tienen asociados un código que los identifica y que a través del cual, se pueden conocer los parámetros asociados al mismo, este código se llama EPSG por su acrónimo en inglés. El código EPSG del sistema de referencia y proyección de la base de datos es 32720. El objeto suelos_sf, ahora es un objeto espacial de clase sf, donde cada observación corresponde a cada sitio de muestreo. Se muestra el sistema de referencia y proyección de las coordenadas y el tipo de geometría.\n\nsuelos_sf <- st_as_sf(suelos, \n                      coords = c(\"X\", \"Y\"), \n                      crs = 32720)\nsuelos_sf\n\nSimple feature collection with 350 features and 6 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 235888.3 ymin: 6133188 xmax: 603163.6 ymax: 6722199\nProjected CRS: WGS 84 / UTM zone 20S\nFirst 10 features:\n   ID_2 elevacion twi arcilla  pH   COS                 geometry\n1     2       100 135    30.8 6.6 25.98 POINT (603163.6 6576899)\n2     3        87 133    24.0 7.4 17.29 POINT (596537.1 6390518)\n3     4        93 126    28.5 6.1 17.43 POINT (595665.5 6380484)\n4     5       105 119    28.8 6.9 15.08 POINT (601138.5 6353446)\n5     6       111 127    25.2 7.4 17.30 POINT (601798.1 6344096)\n6     7        94 135    33.6 6.7 16.13 POINT (587501.2 6615272)\n7     8        97 135    35.7 6.9 15.24 POINT (589808.2 6593001)\n8     9       100 134    30.1 6.0 12.90 POINT (585406.5 6575712)\n9    10       100 134    32.7 6.6 15.74 POINT (584319.9 6552571)\n10   11       108 135    33.2 6.0 27.03 POINT (582795.8 6536823)\n\n\nPara explorar los datos se usa el paquete tmap que permite realizar gráficos estáticos o dinámicos. Con la opción dinámica, se puede interactuar con el gráfico de manera análoga a un SIG. Para cada gráfico, se comienza utilizando la función tm_shape() especificando el objeto a graficar. Cada observación se grafica un punto mediante la función tm_dots(), cada nivel se agrega mediante el símbolo +.\n\ntm_shape(suelos_sf) + \n  tm_dots()\n\n\n\n\nPara agregar latitud y longitud a esta figura se realiza una reproyección. En la función tm_shape() se especifica el nuevo sistema de coordenadas con el que se desea graficar (argumento projection). Se agregar el nivel tm_grid() para visualizar una grilla que contiene las coordenadas latitud y longitud.\n\ntm_shape(suelos_sf, projection = 4326) +\n  tm_grid(col = \"grey90\") +\n  tm_dots()\n\n\n\n\nCualquiera de estos gráficos se puede convertir en un gráfico dinámico mediante la función tmap_mode() especificando como argumento \"view\". Para continuar con gráficos estáticos se debe especificar \"plot\" como argumento de esta función. Mediante la función tm_basemap(), se pueden incorporar distintas capas base. Las opciones disponibles para las capas base se pueden ver mediante el comando names(leaflet::providers).\nCualquiera de estos gráficos se puede convertir en un gráfico dinámico utilizando la función tmap_mode() especificando como argumento \"view\". Para continuar con gráficos estáticos se debe especificar \"plot\" como argumento de esta función. Mediante la función tm_basemap(), se pueden incorporar distintas capas base (capas de fondo que ayudan a visualizar). Las opciones disponibles para las capas base se pueden ver mediante el comando names(leaflet::providers).\n\ntmap_mode(\"view\")\n\ntm_shape(suelos_sf) +\n  tm_dots() +\n  tm_basemap(\"Esri.WorldImagery\", \"OpenTopoMap\")"
  },
  {
    "objectID": "Parte3_cap_a_EscRegionImplR.html#confección-de-grillas-de-predicción",
    "href": "Parte3_cap_a_EscRegionImplR.html#confección-de-grillas-de-predicción",
    "title": "\n7  Bases de datos regionales\n",
    "section": "\n7.2 Confección de grillas de predicción",
    "text": "7.2 Confección de grillas de predicción\nPara generar esta grilla es necesario definir una resolución espacial en el área de interés. Para este ejemplo, se utiliza un archivo vectorial, limites_cba.shp, el cual define el límite del territorio sobre el que se desea predecir.\n\nlimites_cba <- st_read(\"datos/limites_cba.shp\",\n                       quiet = TRUE)\nlimites_cba <- st_transform(limites_cba, \n                            crs = 32720)\n\nLa función st_make_grid() genera una grilla rectangular conteniendo el área del objeto limites_cba. Para definir la resolución espacial de la grilla se utiliza el argumento cellsize definiendo un tamaño de grilla en relación con la unidad de medida del sistema de coordenadas, en este caso 10000 metros, dado que está en UTM.\n\ngrilla_base <- st_make_grid(limites_cba, \n                            cellsize = 10000)\n\ntm_shape(grilla_base) +\n  tm_borders() +\n  tm_shape(limites_cba) +\n  tm_borders(col = \"red\")\n\n\n\n\nDado que la grilla es rectangular, es necesario cortarla según los límites. Para esto se realiza una intersección entre los límites y la grilla utilizando la función st_intersection().\n\ngrilla_pred <- st_intersection(limites_cba,\n                               grilla_base)\n\ntm_shape(grilla_pred) +\n  tm_borders()\n\n\n\n\nLos algoritmos de predicción implementados trabajan prediciendo sitios puntuales, por lo cual, a partir de la grilla, es necesario generar una grilla de puntos. Una alternativa es utilizar la función st_centroid() para obtener el centroide de cada celda.\n\ncentroide_pred <- st_centroid(grilla_pred)\n\nWarning in st_centroid.sf(grilla_pred): st_centroid assumes attributes are\nconstant over geometries of x\n\ntm_shape(centroide_pred) +\n  tm_dots()"
  },
  {
    "objectID": "Parte3_cap_a_EscRegionImplR.html#agregado-de-capas-de-información",
    "href": "Parte3_cap_a_EscRegionImplR.html#agregado-de-capas-de-información",
    "title": "\n7  Bases de datos regionales\n",
    "section": "\n7.3 Agregado de capas de información",
    "text": "7.3 Agregado de capas de información\nSe presenta los comandos necesarios para combinar múltiples capas de información en un mismo objeto. Las variables elevación y twi son extraídas desde modelos digitales de elevación, que se encuentran en formato raster. El paquete terra de R es específico para lectura y manipulación de este tipo de archivos. Para leer un archivo de este formato, se puede utilizar la función rast() mientras que para reproyectar se utiliza la función project(). El archivo elevacion.tif contiene datos de elevación para la provincia de Córdoba. Cuando se imprime el objeto, se muestra la cantidad de pixeles por fila, columna, pixeles totales, la resolución espacial, las coordenadas extremas en latitud y longitud, el sistema de coordenadas de referencia, los valores mínimo y máximo de la variable observada.\n\nelevacion <- rast(\"datos/elevacion.tif\")\nelevacion <-\n  project(elevacion, \n          y = \"+proj=utm +zone=20 +south \n                +datum=WGS84 +units=m +no_defs\")\nelevacion\n\nclass       : SpatRaster \ndimensions  : 3028, 2123, 1  (nrow, ncol, nlyr)\nresolution  : 217.938, 217.938  (x, y)\nextent      : 196380.6, 659063, 6100616, 6760532  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=utm +zone=20 +south +datum=WGS84 +units=m +no_defs \nsource(s)   : memory\nname        :  elevacion \nmin value   :   38.37358 \nmax value   : 2745.15332 \n\n\nPara obtener en los sitios de predicción el valor de la variable del objeto raster, se utiliza la función extract() definiendo como argumento el nombre del objeto raster y el nombre del objeto vectorial que contiene los sitios. Estos valores extraídos se adicionan en una columna llamada elevacion dentro del objeto centroide_pred utilizando la funcion cbind.\n\nelevacion_val <-\n  extract(elevacion, centroide_pred, ID = FALSE)\n\ncentroide_pred <- \n  cbind(centroide_pred,\n        elevacion_val)\n\nEl archivo twi.tif contiene valores de un índice topográfico de humedad también generado a partir de datos provenientes de un modelo digital de elevación..\n\ntwi <- rast(\"datos/twi.tif\")\ntwi <-\n  project(twi, \n         y = \"+proj=utm +zone=20 +south\n                +datum=WGS84 +units=m +no_defs\")\ntwi\n\nclass       : SpatRaster \ndimensions  : 1514, 1062, 1  (nrow, ncol, nlyr)\nresolution  : 435.8761, 435.8761  (x, y)\nextent      : 196380.6, 659280.9, 6100616, 6760532  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=utm +zone=20 +south +datum=WGS84 +units=m +no_defs \nsource(s)   : memory\nname        :      twi \nmin value   :  53.5437 \nmax value   : 137.8588 \n\n\nUtilizando la función extract() se extrae los valores de TWI para cada sitio de la grilla de predicción.\n\ntwi_val <- extract(twi, centroide_pred, ID = FALSE)\n\ncentroide_pred <-\n  cbind(centroide_pred,\n        twi_val)\n\nSe adiciona a la grilla variables procedentes de otras fuentes (SIG de muestreo de suelo). Estos datos se encuentran en los archivos raster llamados arcilla.tif y pH.tif, respectivamente. Estos raster tienen la misma resolución espacial y extensión, por lo que es posible superponerlos en un mismo objeto mediante la función c().\n\narcilla <- rast(\"datos/arcilla.tif\")\npH <- rast(\"datos/pH.tif\")\nedaf <- c(pH, arcilla)\ncrs(edaf) <- \"+proj=utm +zone=20 +south +datum=WGS84 +units=m +no_defs\"\n\ntm_shape(edaf) +\n  tm_raster() +\n  tm_facets(free.scales = TRUE) +\n  tm_legend(position = c('left','bottom'))\n\n\n\n\nSe seleccionan los valores de los sitios utilizando la función extract(). Como los valores extraídos mediante la funcion desde un stack de rasters genera un objeto de tipo data.frame con tantas columnas como capas contenga ese raster, se adicionan al objeto centroide_pred mediante la función cbind(), la cual une columnas de igual número de filas. Ahora el objeto cetroide_pred contiene todos los sitios de predicción con las variables auxiliares adicionadas.\n\nedaf_pred <- extract(edaf, \n                     centroide_pred, \n                     ID = FALSE)\ncentroide_pred <- cbind(centroide_pred, \n                        edaf_pred)\ncentroide_pred\n\nSimple feature collection with 1668 features and 8 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 239427.6 ymin: 6129968 xmax: 614506.7 ymax: 6729403\nProjected CRS: WGS 84 / UTM zone 20S\nFirst 10 features:\n          UNION JURISDICCI CAPITAL FUENTE elevacion      twi       pH   arcilla\n1   -2147483648    CORDOBA CORDOBA    IGN  274.8449 118.4764 6.391590  8.209237\n1.1 -2147483648    CORDOBA CORDOBA    IGN  257.2227 113.7088 6.262683  8.882079\n1.2 -2147483648    CORDOBA CORDOBA    IGN  266.5454 112.7255 6.142890  9.475647\n1.3 -2147483648    CORDOBA CORDOBA    IGN  255.0000 118.6159 6.136549  9.945969\n1.4 -2147483648    CORDOBA CORDOBA    IGN  230.9800 119.1276 5.946674 10.268174\n1.5 -2147483648    CORDOBA CORDOBA    IGN  201.1941 118.7030 5.782518 10.445782\n1.6 -2147483648    CORDOBA CORDOBA    IGN  185.8633 123.6503 5.805237 10.512389\n1.7 -2147483648    CORDOBA CORDOBA    IGN  176.7831 122.9565 5.846396 10.524518\n1.8 -2147483648    CORDOBA CORDOBA    IGN  162.5928 126.4006 5.893137 10.547595\n1.9 -2147483648    CORDOBA CORDOBA    IGN  156.4618 124.1537 5.920496 10.639622\n                    geometry\n1   POINT (310498.8 6129968)\n1.1 POINT (319332.6 6130045)\n1.2 POINT (329333.4 6130153)\n1.3 POINT (339337.7 6130238)\n1.4 POINT (349337.5 6130314)\n1.5 POINT (359337.5 6130390)\n1.6 POINT (369342.5 6130448)\n1.7 POINT (379342.4 6130496)\n1.8 POINT (389342.3 6130543)\n1.9 POINT (399342.2 6130590)\n\n\nPara identificar la variación de la variable de interés en un plano, se puede usar una escala de colores. La elección de la escala cambia según los colores y los puntos de corte (valores de la variable de interés en los cuales cambia el color). Para definirla algunas opciones automáticamente identifican los valores para categorizar y asignar un color. Por defecto, tmap categoriza los valores en intervalos fijos. Utilizando el argumento style, se puede modificar el método a utilizar. Las opciones style = \"order\" y style = \"cont\" permiten representar variables numéricas en un gradiente de color. La opción “order” realiza una escala en función del ranking de los valores, permitiendo una mejor visualización de variables asimétricas. Para la visualización de más de un mapa generado con tmap en un mismo gráfico, se puede utilizar la función tmap_arrange(), utilizando como argumento los mapas que se quieren visualizar. El argumento sync = TRUE permite la visualización interactiva con la navegación (zoom y movimiento) sincronizada en ambos mapas.\n\nelevTm <- tm_shape(centroide_pred) +\n  tm_dots(\"elevacion\", style = \"order\")\n\ntwiTm <- tm_shape(centroide_pred) +\n  tm_dots(\"twi\", style = \"cont\")\n\npHTM <- tm_shape(centroide_pred) +\n  tm_dots(\"pH\", style = \"cont\")\n\narcillaTm <- tm_shape(centroide_pred) +\n  tm_dots(\"arcilla\", style = \"cont\")\n\ntmap_arrange(elevTm, twiTm, pHTM, arcillaTm, \n             ncol = 2)"
  },
  {
    "objectID": "Parte3_cap_a_EscRegionImplR.html#regresión-con-errores-correlacionados-espacialmente-vía-reml",
    "href": "Parte3_cap_a_EscRegionImplR.html#regresión-con-errores-correlacionados-espacialmente-vía-reml",
    "title": "\n7  Bases de datos regionales\n",
    "section": "\n8.1 Regresión con errores correlacionados espacialmente vía REML",
    "text": "8.1 Regresión con errores correlacionados espacialmente vía REML\nSe ajusta un modelo de regresión lineal con la función gls(), usando COS como variable dependiente y elevación, twi, arcilla y pH como variables predictoras. Primero, se ajusta suponiendo errores independientes (sin correlación espacial). Los resultados se guardan en el objeto denominado ajuste_ML. Seguidamente, se ajusta otro modelo de regresión con igual estructura para la componente sistemática, pero suponiendo que los términos de error aleatorio no son independientes sino que se correlacionan a través de un modelo de covarianza espacial. En particular, se ajusta el modelo de correlación espacial esférico y se suponen varianza residual única (modelo homocedástico). El método de estimación del modelo es REML. Los resultados se guardan en el objeto ajuste_err_corr.\n\najuste_ML <- gls(\n  COS ~ 1 + elevacion + twi + arcilla + pH,\n  data = suelos,\n  method = \"REML\")\n\n\najuste_err_corr <- gls(\n  COS ~ 1 + elevacion + twi + arcilla + pH,\n  data = suelos,\n  correlation = corSpher(form =  ~ X + Y),\n  method = \"REML\"\n)\n\nUtilizando la función summary() se muestra a continuación el resultado del modelo sin correlación espacial (objeto ajuste_ML). Todos los términos del modelo, a excepción de elevacion resultaron significativos para un nivel de significación \\(\\alpha=0.05\\). Se observó una correlación alta entre elevacion y twi (0,859), por esta colinealidad entre ambas variables, el término elevacion pudo no haber resultado significativo y podría sacarse del modelo. Se muestra también las características de la distribución de los residuos (mínimo, máximo valor y principales cuartiles). Es de esperar que los residuos estandarizados se encuentren en el intervalo [-3, 3], los valores fuera de este rango se consideran valores atípicos y podrían ser eliminados para reajustar el modelo. La varianza residual es el cuadrado de 4.58, indicando que desviaciones de 4,58 g/kg pueden existir por azar y que no se relacionan a las fuentes de variación reconocidas a priori.\n\nsummary(ajuste_ML)\n\nGeneralized least squares fit by REML\n  Model: COS ~ 1 + elevacion + twi + arcilla + pH \n  Data: suelos \n      AIC      BIC   logLik\n  2088.02 2111.081 -1038.01\n\nCoefficients:\n               Value Std.Error   t-value p-value\n(Intercept) 37.17201  6.654919  5.585644  0.0000\nelevacion    0.00349  0.002126  1.641544  0.1016\ntwi         -0.21008  0.046813 -4.487724  0.0000\narcilla      0.33407  0.030836 10.833750  0.0000\npH          -0.76640  0.381759 -2.007542  0.0455\n\n Correlation: \n          (Intr) elevcn twi    arcill\nelevacion -0.767                     \ntwi       -0.919  0.859              \narcilla   -0.047 -0.001 -0.064       \npH        -0.347 -0.163 -0.035  0.040\n\nStandardized residuals:\n       Min         Q1        Med         Q3        Max \n-3.4274421 -0.5958947 -0.1180292  0.4724770  4.7869984 \n\nResidual standard error: 4.58134 \nDegrees of freedom: 350 total; 345 residual\n\n\nPara el modelo ajustado suponiendo errores correlacionados, los criterios de información de AIC y BIC fueron menores que los obtenidos bajo el supuesto de errores independientes, indicando la conveniencia de considerar la correlación espacial. Los parámetros del modelo asociado a la componente aleatoria son rango = 17791,35 m y varianza residual igual al cuadrado de 4,56. Estos caracterizan la matriz de varianza y covarianza de los errores y proveen una estimación del semivariograma esférico que describe el proceso espacial subyacente, i.e. observaciones separadas por más de 17791,35 m no se encuentran correlacionadas y la varianza residual de las observaciones independientes o con distancias mayor al rango, expresada como desvío estándar, es 4,56.\n\nsummary(ajuste_err_corr)\n\nGeneralized least squares fit by REML\n  Model: COS ~ 1 + elevacion + twi + arcilla + pH \n  Data: suelos \n       AIC      BIC    logLik\n  2077.613 2104.518 -1031.806\n\nCorrelation Structure: Spherical spatial correlation\n Formula: ~X + Y \n Parameter estimate(s):\n   range \n17791.35 \n\nCoefficients:\n               Value Std.Error   t-value p-value\n(Intercept) 36.97464  6.444078  5.737771  0.0000\nelevacion    0.00363  0.002117  1.712655  0.0877\ntwi         -0.20331  0.046119 -4.408389  0.0000\narcilla      0.33384  0.026734 12.487196  0.0000\npH          -0.87070  0.361113 -2.411156  0.0164\n\n Correlation: \n          (Intr) elevcn twi    arcill\nelevacion -0.763                     \ntwi       -0.916  0.853              \narcilla   -0.218  0.083  0.038       \npH        -0.285 -0.224 -0.108  0.260\n\nStandardized residuals:\n       Min         Q1        Med         Q3        Max \n-3.4154164 -0.5776356 -0.1173952  0.4755831  4.8429592 \n\nResidual standard error: 4.565597 \nDegrees of freedom: 350 total; 345 residual\n\n\nLas predicciones se realizaron utilizando la función predict() sobre los centroides de la grilla de predicción utilizando el mejor modelo entre los ajustados. Se convierte el objeto centroide_pred en un data.frame, eliminando del objeto centroide_pred la columna que contiene las características espaciales mediante la función st_drop_geometry() y extrayendo mediante la función st_coordinates(), las coordenadas sin los atributos espaciales. Estas partes se guardan en el objeto suelos_pred de clase data.frame.\n\nsuelos_pred <- data.frame(\n  st_drop_geometry(centroide_pred),\n  st_coordinates(centroide_pred))\n\n\npred_ajuste_err_corr <- predict(\n  ajuste_err_corr,\n  newdata = suelos_pred,\n  na.action = na.pass)\n\nLos predichos se adicionan al objeto centroide_pred utilizando la función cbind(). Para que la visualización de estos valores, se pueda realizar utilizando los polígonos de la grilla de predicción en vez de los centroides, se deben adicionar los predichos mediante la función st_join().\n\npred_err_corr <- cbind(\n  centroide_pred,\n  \"COS_pred\" = pred_ajuste_err_corr)\n\npred_err_corr <- st_join(\n  grilla_pred,\n  pred_err_corr)\n\ntm_shape(pred_err_corr) +\n  tm_fill(\"COS_pred\", style = \"cont\",\n          title = \"Predichos COS (g/kg)\")"
  },
  {
    "objectID": "Parte3_cap_a_EscRegionImplR.html#regresión-con-efectos-aleatorios-de-sitio-vía-inla",
    "href": "Parte3_cap_a_EscRegionImplR.html#regresión-con-efectos-aleatorios-de-sitio-vía-inla",
    "title": "\n7  Bases de datos regionales\n",
    "section": "\n8.2 Regresión con efectos aleatorios de sitio vía INLA",
    "text": "8.2 Regresión con efectos aleatorios de sitio vía INLA\nPara abordar la regresión bayesiana de datos espaciales, primero se define el predictor lineal ajustando un modelo de regresión lineal con la función inla(). INLA representa una combinación de aproximaciones analíticas y esquemas de integración numérica eficiente para obtener una aproximación confiable de la distribución a posteriori de interés. En el ejemplo de ilustración, se usa COS como variable dependiente y elevación, twi, arcilla y pH como variables predictoras y no se ha contemplado la estructura de correlación espacial. Se especifica la distribución que se asume para la variable respuesta a través del argumento family. El cómputo de las medidas para evaluación y comparación de modelos se realiza con el argumento control.compute especificando la medida que se pretende. Para explorar las opciones disponibles para la evaluación y comparación de modelos se ejecuta el comando ?control.compute (en el ejemplo, se solicita el criterio DIC).\n\najuste_INLA <- inla(\n  COS ~ 1 + elevacion + twi + arcilla + pH,\n  family = 'gaussian',\n  data = suelos,\n  control.compute = list(dic = TRUE))\n\nEl modelo ajustado es retornado como un objeto INLA. Este provee información sobre el tiempo de procesado y algunos estadísticos sobre las distribuciones a posteriori de los coeficientes de regresión (efectos fijos) y de los hiperparámetros. Para el modelo ajustado se observan los intervalos de credibilidad del 95% para los coeficientes de regresión asociados a cada una de las variables predictoras (predictor lineal) y como hiperparámetro la precisión de las observaciones de COS. En este ajuste, no hubo efectos aleatorios ni especificaciones relacionadas a la espacialidad de los datos. El intervalo de credibilidad contiene al verdadero parámetro con un 95% de probabilidad. Luego, el ajusta indica que todas las variables impactan a la respuesta, excepto la variable elevación para la cual el desvío estándar (sd) es alto relativo a la media de la distribución del coeficiente de regresión y el intervalo de credibilidad contiene al 0. Podría ser oportuno realizar un nuevo ajuste sin esta variable, que como se ha especificado anteriormente está altamente correlacionada con twi. La media a posteriori para el coeficiente de regresión que acompaña el pH es -0,766 con un intervalo de credibilidad del 95% entre -1,515 y -0,018, por lo que se interpreta que a mayores valores de pH se tendrán menores valores de COS. Se muestra también el intervalo de credibilidad [0,041; 0,055] para la precisión (inversa de la varianza \\(1/\\sigma_e^2\\) ), la estimación es 0,048 y por tanto la varianza residual es próxima a 20 o el error estándar residual cercano a 4,56. El valor de DIC, el cual es una función de la deviance del modelo y de una medida del número efectivo de parámetros del modelo, es 2065,79. El numero efectivo de parámetros es una cantidad que caracteriza la complejidad del modelo y que no solo depende de la cantidad de parámetros sino también de la dependencia entre ellos. Esta medida puede ser usada para comparar modelos, menores valores indican mejor ajuste del modelo a los datos. El mejor de los modelos ajustados, también tendrá menor diferencia entre el valor de DIC para ese modelo y el valor de DIC para el modelo saturado. La verosimilitud marginal es otro criterio usado en selección de modelos en estadística bayesiana, al reportarse en escala log menor valor indica mejor ajuste. R-INLA obtiene las distribuciones marginales a posteriori para todos los parámetros del modelo.\n\nsummary(ajuste_INLA)\n\n\nCall:\n   c(\"inla.core(formula = formula, family = family, contrasts = contrasts, \n   \", \" data = data, quantiles = quantiles, E = E, offset = offset, \", \" \n   scale = scale, weights = weights, Ntrials = Ntrials, strata = strata, \n   \", \" lp.scale = lp.scale, link.covariates = link.covariates, verbose = \n   verbose, \", \" lincomb = lincomb, selection = selection, control.compute \n   = control.compute, \", \" control.predictor = control.predictor, \n   control.family = control.family, \", \" control.inla = control.inla, \n   control.fixed = control.fixed, \", \" control.mode = control.mode, \n   control.expert = control.expert, \", \" control.hazard = control.hazard, \n   control.lincomb = control.lincomb, \", \" control.update = \n   control.update, control.lp.scale = control.lp.scale, \", \" \n   control.pardiso = control.pardiso, only.hyperparam = only.hyperparam, \n   \", \" inla.call = inla.call, inla.arg = inla.arg, num.threads = \n   num.threads, \", \" blas.num.threads = blas.num.threads, keep = keep, \n   working.directory = working.directory, \", \" silent = silent, inla.mode \n   = inla.mode, safe = FALSE, debug = debug, \", \" .parent.frame = \n   .parent.frame)\") \nTime used:\n    Pre = 0.491, Running = 0.668, Post = 0.115, Total = 1.27 \nFixed effects:\n              mean    sd 0.025quant 0.5quant 0.975quant   mode kld\n(Intercept) 37.171 6.650     24.122   37.171     50.221 37.171   0\nelevacion    0.003 0.002     -0.001    0.003      0.008  0.003   0\ntwi         -0.210 0.047     -0.302   -0.210     -0.118 -0.210   0\narcilla      0.334 0.031      0.274    0.334      0.395  0.334   0\npH          -0.766 0.381     -1.515   -0.766     -0.018 -0.766   0\n\nModel hyperparameters:\n                                         mean    sd 0.025quant 0.5quant\nPrecision for the Gaussian observations 0.048 0.004      0.041    0.048\n                                        0.975quant  mode\nPrecision for the Gaussian observations      0.055 0.048\n\nDeviance Information Criterion (DIC) ...............: 2065.62\nDeviance Information Criterion (DIC, saturated) ....: 358.50\nEffective number of parameters .....................: 6.00\n\nMarginal log-Likelihood:  -1070.11 \n is computed \nPosterior summaries for the linear predictor and the fitted values are computed\n(Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n\n\nLos efectos aleatorios en INLA se incluyen en la formula del predictor lineal usando la función f(). Para el ejemplo de ilustración, más abajo se ajusta el modelo de regresión donde se adiciona un efecto aleatorio de sitio para caracterizar el proceso espacial subyacente a los datos. Dado que la función f() se valúa sobre un red de nodos conformada a partir de las observaciones, es primero necesario construir una malla que cubra el dominio espacial y definir un objeto que contiene la identificación de los nodos con observaciones. La malla se arma con la función inla.mesh.2d() cuyos argumentos o parámetros de la malla son: cutoff define la distancia mínima entre vértices de los triángulos que conforman la malla y max.edge que refiere a la longitud máxima del lado de cada triángulo. Por defecto, la malla se construye con el método de triangulación de Delauny.\n\nsitios <- suelos[, c(\"X\", \"Y\")]\n\nmalla <- inla.mesh.2d(sitios, cutoff = 200,\n                      max.edge = 200000)\n\nPara estimar la matriz de varianzas y covarianzas de los efectos de sitio por el método SPDE se utiliza la función inla.spde2.matern(). Un argumento a especificar es el parámetro \\(\\alpha\\) (que varía entre 0 y 2). Por defecto es 2 para aproxima una función de correlación espacial del tipo exponencial como modelo de correlación espacial entre los efectos de sitio.\n\nspde <- inla.spde2.matern(mesh = malla,\n                          alpha = 2)\n\nOtra función posible es inla.spde2.pcmatern, en la cual hay que especificar el rango (argumetno prior.range) y desvío estándar marginal (argumetno prior.sigma) a priori. Para ambos argumentos hay que especificarle un vector de largo dos. En el caso de prior.range los valores contienen el range0 y Prange, especificando que \\(P(\\rho < \\rho_0) = p_\\rho\\), donde \\(\\rho\\) es el rando espacial del campo aleatorio. El argumento prior.sigma se especifica de tal manera que \\(P(\\sigma > \\sigma_0) = p_\\rho\\), donde \\(\\sigma\\) es la desviación estándar residual del campo.\n\n\nspde <- inla.spde2.pcmatern(\n  mesh = malla,\n  prior.range = c(20000, 0.05),\n  prior.sigma = c(0.2, 0.05)\n)\n\nLuego de realizar la malla y crear el objeto del modelo Matern, se ajusta el modelo de regresión con efecto aleatorio de sitio. En esta ilustración se ajustará utilizando tanto la función inla() del paquete INLA, como así también la función bru() del paquete inlabru. Con ambas funciones se obtendrá el mismo modelo, por lo que se debería seleccionar una única alternativa.\nEl paquete inlabru fue desarrollado para facilitar la modelación espacial utilizando funciones del paquete INLA, por lo que crearemos un objeto espacial llamado suelos_sf para la función que pueda identificar los sitios y el sistema de coordenadas de manera automática. Los resultados son un objeto INLA que incluyen las distribuciones a posteriori de los efectos latentes y de los hiperparámetros, así como estadísticos de resumen. Como se ejemplifica adelante, pueden obtenerse estimaciones a posteriori de parámetros del campo espacial latente. La fórmula es similar a la especificada anteriormente, pero se adiciona el efecto de sitio llamado site.\n\nsuelos_sf <- st_as_sf(suelos, \n                      coords = c(\"X\", \"Y\"), \n                      crs = 32720)\n\najuste_INLAspde <-\n  bru(\n    COS ~ \n      Intercept(1) + elevacion + twi + \n      arcilla + pH + \n      site(main = coordinates, model = spde),\n    family = \"gaussian\",\n    data = as_Spatial(suelos_sf)\n  )\n\nEn el caso de utilizar la función inla(), es necesario especificar los sitios donde se encuentras las observaciones, para esto se generará el objeto site a partir de los sitios de la malla.\n\nsite <- malla$idx$loc\n\najuste_INLAspde_inla <- inla(\n  COS ~ 1 + elevacion + twi + arcilla + pH +\n    f(site, model = spde),\n  family = 'gaussian',\n  data = suelos,\n  control.compute = list(dic = TRUE),\n  control.predictor = list(compute = TRUE))\n\n\nsummary(ajuste_INLAspde)\n\ninlabru version: 2.7.0\nINLA version: 22.12.16\nComponents:\nIntercept: main = linear(1)\nelevacion: main = linear(elevacion)\ntwi: main = linear(twi)\narcilla: main = linear(arcilla)\npH: main = linear(pH)\nsite: main = spde(coordinates)\nLikelihoods:\n  Family: 'gaussian'\n    Data class: 'SpatialPointsDataFrame'\n    Predictor: COS ~ .\nTime used:\n    Pre = 1.04, Running = 0.452, Post = 0.167, Total = 1.66 \nFixed effects:\n            mean    sd 0.025quant 0.5quant 0.975quant   mode kld\nIntercept 41.445 6.525     28.633   41.448     54.239 41.454   0\nelevacion  0.005 0.002      0.000    0.005      0.009  0.005   0\ntwi       -0.219 0.045     -0.307   -0.219     -0.131 -0.219   0\narcilla    0.233 0.039      0.157    0.233      0.308  0.233   0\npH        -1.038 0.377     -1.776   -1.039     -0.299 -1.039   0\n\nRandom effects:\n  Name    Model\n    site SPDE2 model\n\nModel hyperparameters:\n                                            mean       sd 0.025quant 0.5quant\nPrecision for the Gaussian observations 5.70e-02 5.00e-03   4.80e-02 5.70e-02\nRange for site                          3.42e+05 1.06e+05   1.77e+05 3.28e+05\nStdev for site                          9.60e-01 2.19e-01   5.83e-01 9.43e-01\n                                        0.975quant     mode\nPrecision for the Gaussian observations   6.60e-02 5.60e-02\nRange for site                            5.91e+05 3.01e+05\nStdev for site                            1.44e+00 9.13e-01\n\nDeviance Information Criterion (DIC) ...............: 2015.13\nDeviance Information Criterion (DIC, saturated) ....: 366.79\nEffective number of parameters .....................: 13.66\n\nWatanabe-Akaike information criterion (WAIC) ...: 2020.41\nEffective number of parameters .................: 17.93\n\nMarginal log-Likelihood:  -1071.64 \n is computed \nPosterior summaries for the linear predictor and the fitted values are computed\n(Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n\n\nEl objeto resultante provee información sobre los intervalos de credibilidad del 95% de los coeficientes de regresión y de los hiperparámetros. Estos son además de la precisión Theta1 y Theta2 que definen la función de correlación espacial subyacente. Los parámetros Theta1 y Theta2 no son de interpretación directa, pero dependen de los parámetros que caracterizan el proceso espacial (rango y varianza estructural). Utilizando la función inla.spde2.result() se puede obtener la distribución a posteriori de los parámetros expresadas en términos de rango y varianza estructural.\n\n\n\n\nresultados_spde <-\n  inla.spde2.result(inla = ajuste_INLAspde,\n                    name = \"site\", spde = spde)\n\ninla.emarginal(function(x) {x},\n  resultados_spde$marginals.range.nominal[[1]])\n\n[1] 342057.7\n\ninla.emarginal(function(x) {x},\n  resultados_spde$marginals.variance.nominal[[1]])\n\n[1] 0.9689066\n\n\n\n\n\nPara comparar los modelos de regresión ajustados con errores independientes y con correlación espacial se visualizan medidas de bondad de ajuste como DIC para ambos modelos.\n\nc(ajuste_INLA$dic$dic, ajuste_INLAspde$dic$dic)\n\n[1] 2065.624 2015.131\n\n\nComparando los valores de DIC se observa la conveniencia de usar un modelo con correlación espacial respecto a uno que supone los valores de COS independientes, dado que el primero tiene un valor menor.\n\n8.2.1 Obtención de predicciones\nPara obtener predicciones se puede utilizar tanto funciones del paquete inlabru o bien específicas del paquete INLA. Aquí, a modo ilustrativo, se presentan ambas formas de obtener los mismos predichos, aunque el usuario deberá optar por la de su conveniencia. Para ambas alternativas es necesario contar con un objeto que contenga los sitios en los que queremos realizar las predicciones. En el caso de no utilizar funciones de inlabru es necesario que este objeto contenga tanto información de los sitios a predecir como los datos de los sitios observados. La función bind_rows() del paquete dplyr permite juntar dos objetos de clase data.frame que contengan el mismo nombre de columnas colocando NA cuando no hay valor para un campo.\n\nsuelos_pred_INLA <- dplyr::bind_rows(suelos_pred,\n                                     suelos)\nhead(suelos_pred_INLA)\n\n          UNION JURISDICCI CAPITAL FUENTE elevacion      twi       pH   arcilla\n1   -2147483648    CORDOBA CORDOBA    IGN  274.8449 118.4764 6.391590  8.209237\n1.1 -2147483648    CORDOBA CORDOBA    IGN  257.2227 113.7088 6.262683  8.882079\n1.2 -2147483648    CORDOBA CORDOBA    IGN  266.5454 112.7255 6.142890  9.475647\n1.3 -2147483648    CORDOBA CORDOBA    IGN  255.0000 118.6159 6.136549  9.945969\n1.4 -2147483648    CORDOBA CORDOBA    IGN  230.9800 119.1276 5.946674 10.268174\n1.5 -2147483648    CORDOBA CORDOBA    IGN  201.1941 118.7030 5.782518 10.445782\n           X       Y ID_2 COS\n1   310498.8 6129968   NA  NA\n1.1 319332.6 6130045   NA  NA\n1.2 329333.4 6130153   NA  NA\n1.3 339337.7 6130238   NA  NA\n1.4 349337.5 6130314   NA  NA\n1.5 359337.5 6130390   NA  NA"
  },
  {
    "objectID": "Parte3_cap_a_EscRegionImplR.html#predicciones-utilizando-el-paquete-inlabru",
    "href": "Parte3_cap_a_EscRegionImplR.html#predicciones-utilizando-el-paquete-inlabru",
    "title": "\n7  Bases de datos regionales\n",
    "section": "\n8.3 Predicciones utilizando el paquete inlabru\n",
    "text": "8.3 Predicciones utilizando el paquete inlabru\n\nEl paquete inlabru contiene una función predict la cual puede ser utilizada para obtener las predicciones. Transformaremos el objeto suelos_pred_INLA en uno de clase espacial. Para utilizar esta función, el modelo debe ajustarse mediante la función bru(). Mediante una fórmula, es necesario especificar los efectos que queremos considerar para la predicción, en este caso consideraremos todos los efectos del ajuste.\n\nsuelos_pred_INLA_sf <-\n  st_as_sf(suelos_pred_INLA, \n           coords = c(\"X\", \"Y\"), \n           crs = 32720)\n\npred_INLAspde_bru <-\n  predict(\n    ajuste_INLAspde,\n    as_Spatial(suelos_pred_INLA_sf),\n    ~ Intercept + elevacion + \n      twi + arcilla + pH + site\n  )\n\nLos predichos pueden ser graficados utilizando el paquete ggplot2 y la función gg() del paquete inlabru.\n\nggplot() +\n  gg(pred_INLAspde_bru, aes(color = mean), size = 3) +\n  coord_equal() +\n  labs(x = \"X\",\n       y = \"Y\",\n       color = \"COS predicho\")"
  },
  {
    "objectID": "Parte3_cap_a_EscRegionImplR.html#utilizando-inla",
    "href": "Parte3_cap_a_EscRegionImplR.html#utilizando-inla",
    "title": "\n7  Bases de datos regionales\n",
    "section": "\n8.4 Utilizando INLA\n",
    "text": "8.4 Utilizando INLA\n\nEn R-INLA no existe una funcion predict() como en gls. Las predicciones deben ser obtenidas como parte del modelo ajustado. Dado que las predicciones puedes ser entendidas como el ajuste de un modelo con datos faltantes simplemente se especificará, antes del ajuste, y[i] = NA para aquellos sitios donde se desea predecir. Las distribuciones de los valores predichos no son devueltas directamente, pero se pueden explorar. INLA retorna las a posteriori marginales para los efectos aleatorios y para el predictor linear en el sitio faltante. Adicionando el ruido de las observaciones a los valores ajustados se obtienen los valores predichos para el sitio.\nLuego de identificar el predictor lineal, debe definirse la malla y el modelo espacial para la grilla de predicción asociada a los efectos aleatorios de sitios. Mediante el argumento control.predictor en la función inla() se indica que debe computarse el valor de la variable respuesta en el lugar del dato faltante.\n\nsitios_pred <- suelos_pred_INLA[, c(\"X\", \"Y\")]\n\n\nmalla_pred <- inla.mesh.2d(loc = sitios_pred, \n                           cutoff = 200,\n                           max.edge = 200000)\n\n\nnodos_pred <- malla_pred$idx$loc\n\nspde_pred <- inla.spde2.pcmatern(\n  mesh = malla_pred,\n  prior.range = c(20000, 0.05),\n  prior.sigma = c(0.2, 0.05)\n)\n\npred_INLAspde <-\n  inla(\n    COS ~ 1 + elevacion + twi + \n      arcilla + pH + \n      f(nodos_pred, \n        model = spde_pred, \n        diagonal = 1e-6),\n    family = 'gaussian',\n    data = suelos_pred_INLA,\n    control.predictor = \n      list(link = 1, compute = TRUE)\n  )\n\nSe puede obtener la media de la distribución a posteriori de los valores predichos para cada sitio en la grilla de predicción, para mapear la distribución espacial de la variable respuesta.\n\nCOS_pred_INLA <-\n  pred_INLAspde$summary.fitted.values$mean\n\nCOS_pred_inlabru <-\n  pred_INLAspde_bru$mean\n\npred_err_corr <-\n  cbind(\n    suelos_pred_INLA_sf,\n    \"COS_pred_INLA\" = COS_pred_INLA,\n    \"COS_pred_inlabru\" = COS_pred_inlabru\n    )\n\nmap_pred_inla <- \n  tm_shape(pred_err_corr) +\n  tm_dots(\"COS_pred_INLA\", style = \"cont\") \n\nmap_pred_inlabru <-  \n  tm_shape(pred_err_corr) +\n  tm_dots(\"COS_pred_inlabru\", style = \"cont\")\n\ntmap_arrange(map_pred_inla, \n             map_pred_inlabru)"
  },
  {
    "objectID": "Parte3_cap_a_EscRegionImplR.html#regresión-vía-modelos-basados-en-árbol",
    "href": "Parte3_cap_a_EscRegionImplR.html#regresión-vía-modelos-basados-en-árbol",
    "title": "\n7  Bases de datos regionales\n",
    "section": "\n8.5 Regresión vía modelos basados en árbol",
    "text": "8.5 Regresión vía modelos basados en árbol\nSe ajusta un modelo GBR o gradient boosting model con errores correlacionados espacialmente en dos pasos, primero se optimiza la parametrización del predictor GBR usando datos de los sitios observados y se obtienen los residuos de este modelo. En segunda instancia, se ajusta un modelo de semivariaograma a los residuos que se usará para realizar predicción kriging de residuos sobre toda la grilla de predicción. Finalmente, los residuos predichos se adicional a la componente sistemática predicha con el modelo GBR sobre la misma grilla de predicción.\nPara implementar GBR se utiliza el paquete caret. Para optimizar el modelo GBM. se genera una grilla de valores posibles para sus parámetros con la función expand.grid(). Esta función genera un data.frame que contiene en las filas cada una de las combinaciones posibles generadas a partir de los rangos de valores propuestos para cada parámetro del modelo GBM. Éstos son: n.trees que definen el número total de árboles ajustados, shrinkage que regula la extensión de cada árbol, n.minobsinnode que representa el mínimo de observaciones en cada nodo terminal y bag.fraction la proporción de observaciones del grupo de entrenamiento seleccionadas aleatoriamente para la expansión sucesiva del árbol. El tipo de validación cruzada para la optimización de los parámetros del modelo se realiza a través de la función train.control. Luego utilizando la función train() se especifica el modelo con el argumento method, en este caso gbm. La misma función train() genera un objeto con el modelo parametrizado con la configuración valores que arrojan el menor error predictivo, es decir con un modelo del tipo árbol optimizado.\n\nparam_gbm <-  expand.grid(\n  interaction.depth = c(2:4),\n  n.trees = (1:30) * 100,\n  shrinkage = c(0.001, 0.01),\n  n.minobsinnode = c(7, 5)\n)\n\ncontrol <- trainControl(method = \"repeatedcv\",\n                        number = 5,\n                        repeats = 5)\n\najuste_gbm <- train(\n  COS ~ elevacion + twi + arcilla + pH,\n  data = suelos,\n  method = \"gbm\",\n  trControl = control,\n  verbose = FALSE,\n  metric = \"RMSE\",\n  tuneGrid = param_gbm\n)\n\nPidiendo un gráfico del objeto ajuste_gbm se puede acceder al resumen del proceso de optimización de los parámetros. El rendimiento del modelo depende de estos parámetros, pero es posible identificar las combinaciones que generan el mejor desempeño predictivo. A su vez, a través del comando ajuste_gbm$bestTune podemos acceder a los parámetros que definen el modelo óptimo.\n\nggplot(ajuste_gbm) + \n  theme_bw()\n\n\n\n\n\najuste_gbm$bestTune\n\n    n.trees interaction.depth shrinkage n.minobsinnode\n173    2300                 4     0.001              7\n\n\nEl objeto resultante del ajuste GBM, provee un gráfico de la influencia relativa de cada variable predictora para explicar COS, y el árbol con el que se realizará la predicción. A partir de la función predict() sobre los datos observados, se obtienen predichos y consecuentemente los residuos del modelo GBR.\n\nsummary(ajuste_gbm)\n\n\n\n\n                var   rel.inf\ntwi             twi 39.342667\narcilla     arcilla 38.700249\nelevacion elevacion 13.873191\npH               pH  8.083893\n\n\nEn la segunda etapa, se ajusta una función de semivarianza a los residuos del modelo GBM utilizando las funciones variogram y fit.variogram del paquete gstat.\n\nsuelos$residuosgbm <-\n  suelos$COS - predict(ajuste_gbm, \n                       newdata = suelos)\nsuelos <- \n  st_as_sf(suelos, \n           coords = c(\"X\", \"Y\"), \n           crs = 32720)\n\nsemiv_gbmk <- variogram(residuosgbm ~ 1, suelos)\n\nplot(semiv_gbmk)\n\n\n\n\n\nsemiv_aj_gbmk <-\n  fit.variogram(semiv_gbmk ,\n                vgm(c(\"Exp\", \"Sph\", \"Gau\")))\nplot(semiv_gbmk, semiv_aj_gbmk)\n\n\n\n\nPara obtener un predicción de COS en los sitios no muestreados, se realiza la predicción kriging de los residuos sobre los sitios de la grilla de predicción utilizando el modelo ajustado en el paso anterior a partir de la función krige(). Luego se utiliza el modelo GBM (árbol optimo) para predecir COS sobre la grilla de predicción sin considerar la espacialidad. Finalmente, la predicción de COS en cada sitio se compone sumando la predicción del modelo GBM y la predicción kriging de los residuos para cada sitio de la grilla de predicción.\n\nkrig_res_gbm <-\n  krige(\n    residuosgbm ~ 1,\n    location = suelos,\n    newdata = pred_err_corr,\n    model = semiv_aj_gbmk\n  )\n\n[using ordinary kriging]\n\ngbmk_pred <-\n  predict(ajuste_gbm,\n          newdata = pred_err_corr, \n          na.action = na.pass) +\n  krig_res_gbm$var1.pred\n\npred_err_corr <- \n  cbind(pred_err_corr,\n        \"COS_pred_GBM\" = gbmk_pred)\n\ntm_shape(pred_err_corr) +\n  tm_dots(\"COS_pred_GBM\", style = \"cont\") +\n  tm_layout(legend.outside = TRUE)\n\n\n\n\n\n\n\n\n\n\nHang, Susana, Gustavo Negro, Alejandro Becerra, y Ariel Edgar Rampoldi. 2015. Suelos de Córdoba: Variabilidad de las propiedades del horizonte superficial. Córdoba: Jorge Omar Editorial."
  },
  {
    "objectID": "referencias.html",
    "href": "referencias.html",
    "title": "Referencias",
    "section": "",
    "text": "Anselin, Luc. 1995. “Local Indicators of Spatial\nAssociation—LISA.” Geographical Analysis 27 (2): 93–115.\n\n\nBabai, László. 1979. “Monte-Carlo Algorithms in Graph Isomorphism\nTesting.” Université Tde Montréal Technical Report, DMS,\nno. 79–10.\n\n\nBakka, Haakon, Håvard Rue, Geir Arne Fuglstad, Andrea Riebler, David\nBolin, Janine Illian, Elias Krainski, Daniel Simpson, and Finn Lindgren.\n2018. “Spatial Modeling with r-INLA: A Review.” Wiley\nInterdisciplinary Reviews: Computational Statistics, no. February:\n1–24. https://doi.org/10.1002/wics.1443.\n\n\nBalzarini, Mónica, R Macchiavelli, and Fernando Casanoves. 2004.\n“Aplicaciones de Modelos Mixtos En Agricultura y\nForestería.” Curso de Capacitacion Centro Agronomico Tropical\nde Investigación y Enseñanza-CATIE.\n\n\nBezdek, James C, Chris Coray, Robert Gunderson, and James Watson. 1981.\n“Detection and Characterization of Cluster Substructure i. Linear\nStructure: Fuzzy c-Lines.” SIAM Journal on Applied\nMathematics 40 (2): 339–57.\n\n\nBlangiardo, Marta, and Michela Cameletti. 2015. Spatial and\nSpatio-Temporal Bayesian Models with r-INLA. John Wiley & Sons.\n\n\nBreiman, Leo. 2001. “Random Forests.” Machine\nLearning 45 (1): 5–32.\n\n\nBreiman, Leo, Jerome H. Friedman, Richard A. Olshen, and Charles J.\nStone. 2017. Classification and Regression Trees.\nClassification and Regression Trees. https://doi.org/10.1201/9781315139470.\n\n\nBrenning, Alexander. 2012. “Spatial Cross-Validation and Bootstrap\nfor the Assessment of Prediction Rules in Remote Sensing: The r Package\nSperrorest.” In Geoscience and Remote Sensing Symposium\n(IGARSS), 2012 IEEE International, 5372–75. IEEE.\n\n\nCameletti, Michela, Finn Lindgren, Daniel Simpson, and Håvard Rue. 2013.\n“Spatio-Temporal Modeling of Particulate Matter Concentration\nThrough the SPDE Approach.” AStA Advances in Statistical\nAnalysis 97 (2): 109–31. https://doi.org/10.1007/s10182-012-0196-3.\n\n\nClifford, Peter, Sylvia Richardson, and Denis Hemon. 1989.\n“Assessing the Significance of the Correlation Between Two Spatial\nProcesses.” Biometrics 45 (1): 123–34. https://doi.org/10.2307/2532039.\n\n\nCórdoba, Mariano, Cecilia Bruno, José Luis J. L. Costa, and Mónica\nBalzarini. 2012. “Principal Component Analysis with Georeferenced\nData. An Application in Precision Agriculture.” Rev. FCA\nUNCUYO 44 (1): 27–39.\n\n\nCórdoba, Mariano, Cecilia Bruno, José Luis Costa, and Mónica Balzarini.\n2013. “Subfield management class delineation\nusing cluster analysis from spatial principal components of soil\nvariables.” Computers and Electronics in\nAgriculture 97 (September): 6–14. https://doi.org/10.1016/j.compag.2013.05.009.\n\n\nCorrea Morales, Juan Carlos, Barrera Causil, and Carlos Javier. 2018.\nIntroducción a La Estadística Bayesiana: Notas de Clase.\nInstituto Tecnológico Metropolitano.\n\n\nCover, Thomas, and Peter Hart. 1967. “Nearest Neighbor Pattern\nClassification.” IEEE Transactions on Information Theory\n13 (1): 21–27.\n\n\nCressie, Noel, and Christopher K Wikle. 2015. Statistics for\nSpatio-Temporal Data. John Wiley & Sons.\n\n\nDi Rienzo, J A, F Casanoves, M G Balzarini, L Gonzalez, M Tablada, and C\nW Robledo. 2019. “InfoStat.”\n\n\nDray, Stéphane, Daniel Chessel, and Jean Thioulouse. 2003.\n“Co‐inertia Analysis and the Linking of Ecological Data\nTables.” Ecology 84 (11): 3078–89.\n\n\nDray, Stéphane, Sonia Saïd, and Françis Débias. 2008. “Spatial\nOrdination of Vegetation Data Using a Generalization of Wartenberg’s\nMultivariate Spatial Correlation.” Journal of Vegetation\nScience 19 (1): 45���56.\n\n\nDutilleul, Pierre, Peter Clifford, Sylvia Richardson, and Denis Hemon.\n1993. “Modifying the t Test for Assessing the\nCorrelation Between Two Spatial Processes.”\nBiometrics 49 (1): 305. https://doi.org/10.2307/2532625.\n\n\nEfron, Bradley, and Trevor Hastie. 2016. Computer Age Statistical\nInference: Algorithms, Evidence, and Data Science. Computer Age\nStatistical Inference: Algorithms, Evidence, and Data Science. https://doi.org/10.1017/CBO9781316576533.\n\n\nEfron, Bradley, and Robert Tibshirani. 1997. “Improvements on\nCross-Validation: The 632+ Bootstrap Method.” Journal of the\nAmerican Statistical Association 92 (438): 548–60.\n\n\nFrogbrook, Z L, and M A Oliver. 2007. “Identifying Management\nZones in Agricultural Fields Using Spatially Constrained Classification\nof Soil and Ancillary Data.” Soil Use and Management 23\n(1): 40–51. https://doi.org/10.1111/j.1475-2743.2006.00065.x.\n\n\nFukuyama, Yoshiki, and M. Sugeno. 1989. “A New Method of Choosing\nthe Number of Clusters for the Fuzzy c-Mean Method.” In Proc.\n5th Fuzzy Syst. Symp., 1989, 247–50.\n\n\nGabriel, K Ruben, and Robert R Sokal. 1969. “A New Statistical\nApproach to Geographic Variation Analysis.” Systematic\nZoology 18 (3): 259–78.\n\n\nGabriel, Karl Ruben. 1971. “The Biplot Graphic Display of Matrices\nwith Application to Principal Component Analysis.”\nBiometrika 58 (3): 453–67.\n\n\nGalarza, Romina, M Nicolás Mastaglia, Enrique M Albornoz, and César\nMartınez. 2013. “Identificación Automática de Zonas de Manejo En\nLotes Productivos Agrıcolas.” In V Congreso Argentino de\nAgroinformática (CAI) e 42da. JAIIO.\n\n\nHang, Susana, Gustavo Negro, Alejandro Becerra, and Ariel Edgar\nRampoldi. 2015. Suelos de Córdoba: Variabilidad de las propiedades del\nhorizonte superficial. Córdoba: Jorge Omar\nEditorial.\n\n\nHengl, Tomislav, Gerard B. M. Heuvelink, and David G. Rossiter. 2007.\n“About Regression-Kriging: From Equations to Case Studies.”\nComputers and Geosciences 33 (10): 1301–15. https://doi.org/10.1016/j.cageo.2007.05.001.\n\n\nIhaka, Ross, and Robert Gentleman. 1996. “R: A Language for Data\nAnalysis and Graphics.” Journal of Computational and\nGraphical Statistics 5 (3): 299–314.\n\n\nKanevski, Mikhail, Vadim Timonin, Alexi Pozdnukhov, and Gordon Ritter.\n2009. Machine Learning for Spatial Environmental Data: Theory,\nApplications, and Software. Ssrn. EPFL press. https://doi.org/10.2139/ssrn.3015609.\n\n\nKuhn, Max, and Kjell Johnson. 2013. Applied Predictive\nModeling. Vol. 26. Springer.\n\n\nLee, Der-Tsai, and Bruce J Schachter. 1980. “Two Algorithms for\nConstructing a Delaunay Triangulation.” International Journal\nof Computer & Information Sciences 9 (3): 219–42.\n\n\nLi, Jin, Andrew D Heap, Anna Potter, and James J Daniell. 2011.\n“Application of Machine Learning Methods to Spatial Interpolation\nof Environmental Variables.” Environmental Modelling &\nSoftware 26 (12): 1647–59.\n\n\nLindgren, Finn, and Håvard Rue. 2015. “Bayesian Spatial Modelling\nwith r - INLA.” Journal of Statistical Software 63 (19).\nhttps://doi.org/10.18637/jss.v063.i19.\n\n\nMeyer, David, Evgenia Dimitriadou, Kurt Hornik, Andreas Weingessel, and\nFriedrich Leisch. 2019. E1071: Misc Functions of the Department of\nStatistics, Probability Theory Group (Formerly: E1071), TU Wien. https://CRAN.R-project.org/package=e1071.\n\n\nMilne, A E, R Webster, D Ginsburg, and D Kindred. 2012. “Spatial\nMultivariate Classification of an Arable Field into Compact Management\nZones Based on Past Crop Yields.” Computers and Electronics\nin Agriculture 80: 17–30. https://doi.org/10.1016/j.compag.2011.10.007.\n\n\nMorrell, Christopher H. 1998. “Likelihood Ratio Testing of\nVariance Components in the Linear Mixed-Effects Model Using Restricted\nMaximum Likelihood.” Biometrics, 1560–68.\n\n\nPatterson, H Desmond, and Robin Thompson. 1971. “Recovery of\nInter-Block Information When Block Sizes Are Unequal.”\nBiometrika 58 (3): 545–54.\n\n\nPearson, Karl. 1901. “LIII. On Lines and Planes of Closest Fit to\nSystems of Points in Space.” The London, Edinburgh, and\nDublin Philosophical Magazine and Journal of Science 2 (11):\n559–72. https://doi.org/10.1080/14786440109462720.\n\n\nPejović, Milutin, Mladen Nikolić, Gerard B. M. Heuvelink, Tomislav\nHengl, Milan Kilibarda, and Branislav Bajat. 2018. “Sparse\nRegression Interaction Models for Spatial Prediction of Soil Properties\nin 3D.” Computers and Geosciences 118: 1–13. https://doi.org/10.1016/j.cageo.2018.05.008.\n\n\nPing, J L, and A Dobermann. 2003. “Creating Spatially Contiguous\nYield Classes for Site-Specific Management.” Agronomy\nJournal 95 (5): 1121. https://doi.org/10.2134/agronj2003.1121.\n\n\nRue, Håvard, Sara Martino, and Nicolas Chopin. 2009. “Approximate\nBayesian Inference for Latent Gaussian Models by Using Integrated Nested\nLaplace Approximations.” Journal of the Royal Statistical\nSociety: Series b (Statistical Methodology) 71 (2): 319–92.\n\n\nSchabenberger, Oliver, and Carol A Gotway. 2005. Statistical Methods\nfor Spatial Data Analysis. CRC press.\n\n\nTeam, R Core. 2019. “R: A Language and Environment for Statistical\nComputing.”\n\n\nTeam, RStudio. 2019. “RStudio: Integrated Development Environment\nfor r.”\n\n\nVallejos, Ronny, Adriana Mallea, Myriam Herrera, and Silvia Ojeda. 2015.\n“A Multivariate Geostatistical Approach for Landscape\nClassification from Remotely Sensed Image Data.” Stochastic\nEnvironmental Research and Risk Assessment 29 (2): 369–78. https://doi.org/10.1007/s00477-014-0884-5.\n\n\nWebster, Richard, and Margaret A Oliver. 2007. Geostatistics for\nEnvironmental Scientists. Vadose Zone Journal. Vol. 1. 2.\nJohn Wiley & Sons. https://doi.org/10.2136/vzj2002.0321.\n\n\nXie, Xuanli Lisa, and Gerardo Beni. 1991. “A Validity Measure for\nFuzzy Clustering.” IEEE Transactions on Pattern Analysis\n& Machine Intelligence 13 (8): 841–47. https://doi.org/10.1109/34.85677.\n\n\nZimback, C R L. 2001. “Análise Espacial de Atributos Químicos de\nSolos Para Fins de Mapeamento Da Fertilidade Do Solo. 2001. 114\nf.”"
  },
  {
    "objectID": "anexoR.html",
    "href": "anexoR.html",
    "title": "Introducción a R",
    "section": "",
    "text": "Herramientas de software\nR (R. C. Team 2019) es un lenguaje de programación orientado a objetos (Ihaka y Gentleman 1996). Es un software libre y de código abierto, lo que significa que puede ser usado, compartido y modificado libremente. Cualquier persona puede participar en el desarrollo de nuevas funciones y disponibilizarlas para la comunidad de los usuarios de R en forma de paquetes (packages), por lo que R se transformó en uno de los lenguajes de programación más utilizados en Estadística. Presenta potentes capacidades para el procesamiento y visualización, no solo de datos espaciales, sino también de otros tipos de dato. R puede ser instalado en plataformas Windows, Mac OS y en sistemas basados en Linux. Existen múltiples entornos de desarrollo integrado (Integrated Development Environment IDE) los cuales facilitan la programación. Ejemplos de este tipo de software es el intérprete de R que contiene InfoStat (Di Rienzo et al. 2019) y RStudio (Rs. Team 2019). Para poder utilizar el intérprete de R en InfoStat es necesario tener instalados ambos programas. El instalador de InfoStat está disponible en http://infostat.com.ar/, mientras que R puede descargarse en https://cran.r-project.org/. El instalador de RStudio puede descargarse desde la página https://rstudio.com/products/rstudio/download/#download."
  },
  {
    "objectID": "anexoR.html#introducción-al-manejo-de-datos-espaciales-con-r",
    "href": "anexoR.html#introducción-al-manejo-de-datos-espaciales-con-r",
    "title": "Introducción a R",
    "section": "Introducción al manejo de datos espaciales con R",
    "text": "Introducción al manejo de datos espaciales con R\nNumerosos paquetes para el manejo de datos espaciales se encuentran en repositorios digitales de R, ejemplos de ellos son geoR, gstat, rgdal, spdep, sf, stars, terra y raster. En los últimos años se han desarrollado paquetes especializados como ggplot2, leaflet y tmap que incrementaron considerablemente las capacidades para elaboración de gráficos y mapas, tanto estáticos como interactivos. La sintaxis de estos paquetes usa distintos niveles de información, i.e. individualmente se especifica cada nivel del gráfico y luego éstos se combinan para obtener el gráfico completo. Ejemplos de estos niveles son los datos, la estética, los objetos geométricos, las escalas, las particiones, entre otros. Es posible el análisis de datos espaciales utilizando R, sin necesidad de usar un software GIS."
  },
  {
    "objectID": "anexoR.html#intérprete-de-r-en-infostat",
    "href": "anexoR.html#intérprete-de-r-en-infostat",
    "title": "Introducción a R",
    "section": "Intérprete de R en InfoStat",
    "text": "Intérprete de R en InfoStat\nLa interfaz del intérprete de R en InfoStat se divide en cinco paneles. El panel superior izquierdo permite al usuario visualizar scripts previamente escritos o escribir nuevos. En el panel Resultados se muestran las salidas y resultados en forma de texto, los resultados gráficos se mostrarán en una nueva ventana. Debajo de este panel, se muestran información ya sea detección de un error durante la ejecución de un comando, como la correcta finalización de ciertos procedimientos. En los paneles derechos se muestran los objetos cargados en el ambiente de trabajo, mientras que en el panel inferior derecho se muestran los paquetes instalados y en rojo los paquetes cargados."
  },
  {
    "objectID": "anexoR.html#rstudio",
    "href": "anexoR.html#rstudio",
    "title": "Introducción a R",
    "section": "RStudio",
    "text": "RStudio\nLa interfaz de RStudio se divide en cuatro paneles, a su vez, cada panel puede contener más de una pestaña. El panel superior izquierdo permite al usuario cargar scripts previamente escritos o escribir nuevos. En el panel consola se muestran las sentencias de código ejecutadas y los resultados. En los paneles derechos se muestran los objetos cargados en el ambiente de trabajo, mientras que en el panel inferior derecho se muestran archivos en el directorio de trabajo, gráficos generados, ayudas.\n\n\n\n\n\n\n\n\n\n\n\nDi Rienzo, J A, F Casanoves, M G Balzarini, L Gonzalez, M Tablada, y C W Robledo. 2019. «InfoStat».\n\n\nIhaka, Ross, y Robert Gentleman. 1996. «R: a language for data analysis and graphics». Journal of computational and graphical statistics 5 (3): 299-314.\n\n\nTeam, R Core. 2019. «R: A Language and Environment for Statistical Computing».\n\n\nTeam, RStudio. 2019. «RStudio: Integrated Development Environment for R»."
  }
]